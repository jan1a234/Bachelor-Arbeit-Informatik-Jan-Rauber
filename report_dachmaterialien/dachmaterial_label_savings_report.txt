
================================================================================
LABEL-EINSPARUNGS-BERICHT - Dachmaterial
================================================================================

ZIEL: 90% der Baseline-Performance
------------------------------------------------------------

Logistic Regression:
  Baseline (Random 100%): 0.5396
  Ziel-Accuracy: 0.4857
  Labels benötigt:
    - Random Sampling     :    400 ±  244 ( 93.9% gespart)
    - Margin Sampling     :    400 ±  244 ( 93.9% gespart)
    - Least Confidence    :    898 ± 1281 ( 86.3% gespart)
    - Entropy Sampling    :  1,655 ± 2087 ( 74.8% gespart)

Naive Bayes:
  Baseline (Random 100%): 0.0262
  Ziel-Accuracy: 0.0236
  Labels benötigt:
    - Random Sampling     :    100 ±    0 ( 98.5% gespart)
    - Entropy Sampling    :    100 ±    0 ( 98.5% gespart)
    - Margin Sampling     :    100 ±    0 ( 98.5% gespart)
    - Least Confidence    :    100 ±    0 ( 98.5% gespart)

Neural Network:
  Baseline (Random 100%): 0.5454
  Ziel-Accuracy: 0.4908
  Labels benötigt:
    - Random Sampling     :  1,100 ±    0 ( 83.2% gespart)
    - Margin Sampling     :  2,373 ± 1851 ( 63.8% gespart)
    - Least Confidence    :  2,373 ± 1851 ( 63.8% gespart)
    - Entropy Sampling    :  3,210 ± 1944 ( 51.1% gespart)

Random Forest:
  Baseline (Random 100%): 0.4713
  Ziel-Accuracy: 0.4242
  Labels benötigt:
    - Random Sampling     :    200 ±  200 ( 97.0% gespart)
    - Entropy Sampling    :    200 ±  200 ( 97.0% gespart)
    - Margin Sampling     :    200 ±  200 ( 97.0% gespart)
    - Least Confidence    :    200 ±  200 ( 97.0% gespart)

SVM:
  Baseline (Random 100%): 0.5470
  Ziel-Accuracy: 0.4923
  Labels benötigt:
    - Random Sampling     :    300 ±  244 ( 95.4% gespart)
    - Margin Sampling     :    300 ±  244 ( 95.4% gespart)
    - Least Confidence    :    300 ±  244 ( 95.4% gespart)
    - Entropy Sampling    :    400 ±  400 ( 93.9% gespart)

ZIEL: 95% der Baseline-Performance
------------------------------------------------------------

Logistic Regression:
  Baseline (Random 100%): 0.5396
  Ziel-Accuracy: 0.5127
  Labels benötigt:
    - Random Sampling     :    600 ±    0 ( 90.9% gespart)
    - Margin Sampling     :  1,178 ± 1223 ( 82.0% gespart)
    - Least Confidence    :  1,875 ± 1896 ( 71.4% gespart)
    - Entropy Sampling    :  2,513 ± 1954 ( 61.7% gespart)

Naive Bayes:
  Baseline (Random 100%): 0.0262
  Ziel-Accuracy: 0.0249
  Labels benötigt:
    - Random Sampling     :    100 ±    0 ( 98.5% gespart)
    - Entropy Sampling    :    100 ±    0 ( 98.5% gespart)
    - Margin Sampling     :    100 ±    0 ( 98.5% gespart)
    - Least Confidence    :    100 ±    0 ( 98.5% gespart)

Neural Network:
  Baseline (Random 100%): 0.5454
  Ziel-Accuracy: 0.5181
  Labels benötigt:
    - Random Sampling     :  2,293 ± 1871 ( 65.0% gespart)
    - Margin Sampling     :  3,071 ± 1775 ( 53.2% gespart)
    - Least Confidence    :  3,311 ± 1633 ( 49.5% gespart)
    - Entropy Sampling    :  4,325 ± 1695 ( 34.1% gespart)

Random Forest:
  Baseline (Random 100%): 0.4713
  Ziel-Accuracy: 0.4478
  Labels benötigt:
    - Entropy Sampling    :    300 ±  244 ( 95.4% gespart)
      -> 25.0% weniger Labels als Random Sampling
    - Margin Sampling     :    300 ±  244 ( 95.4% gespart)
      -> 25.0% weniger Labels als Random Sampling
    - Least Confidence    :    300 ±  244 ( 95.4% gespart)
      -> 25.0% weniger Labels als Random Sampling
    - Random Sampling     :    400 ±  400 ( 93.9% gespart)

SVM:
  Baseline (Random 100%): 0.5470
  Ziel-Accuracy: 0.5196
  Labels benötigt:
    - Random Sampling     :    600 ±    0 ( 90.9% gespart)
    - Margin Sampling     :    800 ±  244 ( 87.8% gespart)
    - Entropy Sampling    :  1,496 ± 1551 ( 77.2% gespart)
    - Least Confidence    :  1,496 ± 1551 ( 77.2% gespart)

ZIEL: 98% der Baseline-Performance
------------------------------------------------------------

Logistic Regression:
  Baseline (Random 100%): 0.5396
  Ziel-Accuracy: 0.5288
  Labels benötigt:
    - Random Sampling     :    800 ±  244 ( 87.8% gespart)
    - Margin Sampling     :  2,453 ± 1848 ( 62.6% gespart)
    - Least Confidence    :  3,408 ± 2089 ( 48.0% gespart)
    - Entropy Sampling    :  3,946 ± 1892 ( 39.8% gespart)

Naive Bayes:
  Baseline (Random 100%): 0.0262
  Ziel-Accuracy: 0.0257
  Labels benötigt:
    - Random Sampling     :    100 ±    0 ( 98.5% gespart)
    - Entropy Sampling    :    100 ±    0 ( 98.5% gespart)
    - Margin Sampling     :    100 ±    0 ( 98.5% gespart)
    - Least Confidence    :    100 ±    0 ( 98.5% gespart)

Neural Network:
  Baseline (Random 100%): 0.5454
  Ziel-Accuracy: 0.5345
  Labels benötigt:
    - Margin Sampling     :  3,668 ± 1840 ( 44.1% gespart)
      -> 8.9% weniger Labels als Random Sampling
    - Random Sampling     :  4,026 ± 1821 ( 38.6% gespart)
    - Least Confidence    :  4,046 ± 1735 ( 38.3% gespart)
    - Entropy Sampling    :  4,543 ± 1688 ( 30.7% gespart)

Random Forest:
  Baseline (Random 100%): 0.4713
  Ziel-Accuracy: 0.4619
  Labels benötigt:
    - Margin Sampling     :    500 ±  200 ( 92.4% gespart)
      -> 28.6% weniger Labels als Random Sampling
    - Least Confidence    :    600 ±  316 ( 90.9% gespart)
      -> 14.3% weniger Labels als Random Sampling
    - Random Sampling     :    700 ±  374 ( 89.3% gespart)
    - Entropy Sampling    :    998 ± 1233 ( 84.8% gespart)

SVM:
  Baseline (Random 100%): 0.5470
  Ziel-Accuracy: 0.5360
  Labels benötigt:
    - Random Sampling     :  2,671 ± 1951 ( 59.3% gespart)
    - Margin Sampling     :  2,911 ± 1854 ( 55.6% gespart)
    - Entropy Sampling    :  3,050 ± 2027 ( 53.5% gespart)
    - Least Confidence    :  3,290 ± 1885 ( 49.8% gespart)


BESTE STRATEGIEN (bei 95% Performance):
------------------------------------------------------------
Logistic Regression: Random Sampling (nur 600 Labels = 90.9% Einsparung)
Naive Bayes: Random Sampling (nur 100 Labels = 98.5% Einsparung)
Neural Network: Random Sampling (nur 2,293 Labels = 65.0% Einsparung)
Random Forest: Entropy Sampling (nur 300 Labels = 95.4% Einsparung)
SVM: Random Sampling (nur 600 Labels = 90.9% Einsparung)


DURCHSCHNITTLICHE EINSPARUNGEN ÜBER ALLE KLASSIFIKATOREN:
------------------------------------------------------------
Entropy Sampling: -106.4% weniger Labels als Random Sampling
Least Confidence: -76.3% weniger Labels als Random Sampling
Margin Sampling: -27.7% weniger Labels als Random Sampling

================================================================================