{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ea4d377-71b2-4b76-91c9-2dc5bc967995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:13:52 [INFO] Jupyter/IPython Umgebung erkannt - UTF-8 Handling bereits aktiv\n",
      "================================================================================\n",
      "ACTIVE LEARNING AUF DACHMATERIAL - ROBUSTE VERSION MIT STATISTISCHER ANALYSE\n",
      "================================================================================\n",
      "Python Version: 3.13.4\n",
      "PyTorch Version: 2.7.0+cpu\n",
      "NumPy Version: 2.2.6\n",
      "Pandas Version: 2.2.3\n",
      "Scikit-learn Version: 1.6.1\n",
      "SciPy Version: 1.15.3\n",
      "Verwende CPU (keine GPU gefunden)\n",
      "\n",
      "Experiment-Konfiguration:\n",
      "- Anzahl Runs: 5\n",
      "- Budget-Stufen: ['20%', '40%', '60%', '80%', '100%']\n",
      "- Batch-Größe: 500\n",
      "- Signifikanzniveau: 0.05\n",
      "- Mindest-Samples pro Klasse: 20\n",
      "================================================================================\n",
      "17:13:52 [INFO] Lade vollständigen Dachmaterial-Datensatz...\n",
      "17:13:52 [INFO] [ok] Datensatz geladen: 8,213 Zeilen, 10 Spalten\n",
      "17:13:52 [INFO] Ursprüngliche Klassen-Verteilung:\n",
      "mat_qgis\n",
      "Ziegel                4112\n",
      "Metallbahn            1212\n",
      "Asbest|Faserzement     892\n",
      "Beton                  838\n",
      "Bitumen                734\n",
      "PVC|Polycarbonat       140\n",
      "Schiefer               128\n",
      "Glas                   119\n",
      "Dachbegrünung           24\n",
      "Kunststoffbahn          12\n",
      "Kupfer                   2\n",
      "Name: count, dtype: int64\n",
      "17:13:52 [WARNING] Entferne Klassen mit weniger als 20 Samples:\n",
      "17:13:52 [WARNING]   - Kunststoffbahn: 12 Samples\n",
      "17:13:52 [WARNING]   - Kupfer: 2 Samples\n",
      "17:13:52 [INFO] Gefilterte Klassen-Verteilung:\n",
      "mat_qgis\n",
      "Ziegel                4112\n",
      "Metallbahn            1212\n",
      "Asbest|Faserzement     892\n",
      "Beton                  838\n",
      "Bitumen                734\n",
      "PVC|Polycarbonat       140\n",
      "Schiefer               128\n",
      "Glas                   119\n",
      "Dachbegrünung           24\n",
      "Name: count, dtype: int64\n",
      "17:13:52 [INFO] [ok] Dachmaterial-Datensatz vorbereitet: 8,199 Samples\n",
      "17:13:52 [INFO]   Klassen: 9 - Asbest|Faserzement, Beton, Bitumen, Dachbegrünung, Glas, Metallbahn, PVC|Polycarbonat, Schiefer, Ziegel\n",
      "17:13:52 [INFO] Klassen im Trainingsset: 9\n",
      "17:13:52 [INFO] Klassen im Testset: 9\n",
      "17:13:52 [INFO] [ok] Daten vorbereitet: 6,559 Trainingssamples, 1,640 Testsamples\n",
      "17:13:52 [INFO]   Feature-Dimension nach Preprocessing: 28\n",
      "17:13:52 [INFO]   Klassen: 9 im Training, 9 im Test\n",
      "17:13:52 [INFO]   Speicherbedarf: 0.9 MB\n",
      "\n",
      "============================================================\n",
      "Experiment 1/20: Neural Network + Random Sampling\n",
      "============================================================\n",
      "17:13:52 [INFO] \n",
      "Neural Network + Random Sampling - Budget: 20% (1,311 Samples)\n",
      "17:13:52 [INFO]   Run 1/5\n",
      "17:13:59 [INFO]     1,311 labeled -> Accuracy: 0.4927 (Train: 0.6s, Query: 0.00s)\n",
      "17:14:01 [INFO]     Final: 1,311 labeled -> Accuracy: 0.5287, F1: 0.1820\n",
      "17:14:01 [INFO]   Run 2/5\n",
      "17:14:03 [INFO]     1,311 labeled -> Accuracy: 0.5037 (Train: 0.6s, Query: 0.00s)\n",
      "17:14:05 [INFO]     Final: 1,311 labeled -> Accuracy: 0.5439, F1: 0.1862\n",
      "17:14:05 [INFO]   Run 3/5\n",
      "17:14:07 [INFO]     1,311 labeled -> Accuracy: 0.5195 (Train: 0.7s, Query: 0.00s)\n",
      "17:14:09 [INFO]     Final: 1,311 labeled -> Accuracy: 0.5323, F1: 0.1711\n",
      "17:14:09 [INFO]   Run 4/5\n",
      "17:14:11 [INFO]     1,311 labeled -> Accuracy: 0.5061 (Train: 0.6s, Query: 0.00s)\n",
      "17:14:13 [INFO]     Final: 1,311 labeled -> Accuracy: 0.5311, F1: 0.1652\n",
      "17:14:13 [INFO]   Run 5/5\n",
      "17:14:15 [INFO]     1,311 labeled -> Accuracy: 0.5171 (Train: 0.6s, Query: 0.00s)\n",
      "17:14:16 [INFO]     Final: 1,311 labeled -> Accuracy: 0.5238, F1: 0.1757\n",
      "17:14:16 [INFO] \n",
      "Neural Network + Random Sampling - Budget: 40% (2,623 Samples)\n",
      "17:14:16 [INFO]   Run 1/5\n",
      "17:14:22 [INFO]     2,623 labeled -> Accuracy: 0.5354 (Train: 1.4s, Query: 0.00s)\n",
      "17:14:25 [INFO]     Final: 2,623 labeled -> Accuracy: 0.5329, F1: 0.1812\n",
      "17:14:25 [INFO]   Run 2/5\n",
      "17:14:31 [INFO]     2,623 labeled -> Accuracy: 0.5402 (Train: 1.5s, Query: 0.00s)\n",
      "17:14:34 [INFO]     Final: 2,623 labeled -> Accuracy: 0.5396, F1: 0.1781\n",
      "17:14:34 [INFO]   Run 3/5\n",
      "17:14:40 [INFO]     2,623 labeled -> Accuracy: 0.5262 (Train: 1.3s, Query: 0.00s)\n",
      "17:14:43 [INFO]     Final: 2,623 labeled -> Accuracy: 0.5402, F1: 0.1892\n",
      "17:14:43 [INFO]   Run 4/5\n",
      "17:14:49 [INFO]     2,623 labeled -> Accuracy: 0.5390 (Train: 1.4s, Query: 0.00s)\n",
      "17:14:52 [INFO]     Final: 2,623 labeled -> Accuracy: 0.5378, F1: 0.1786\n",
      "17:14:52 [INFO]   Run 5/5\n",
      "17:14:58 [INFO]     2,623 labeled -> Accuracy: 0.5183 (Train: 1.4s, Query: 0.00s)\n",
      "17:15:01 [INFO]     Final: 2,623 labeled -> Accuracy: 0.5341, F1: 0.1665\n",
      "17:15:01 [INFO] \n",
      "Neural Network + Random Sampling - Budget: 60% (3,935 Samples)\n",
      "17:15:01 [INFO]   Run 1/5\n",
      "17:15:10 [INFO]     3,935 labeled -> Accuracy: 0.5415 (Train: 1.8s, Query: 0.00s)\n",
      "17:15:15 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5390, F1: 0.1758\n",
      "17:15:15 [INFO]   Run 2/5\n",
      "17:15:24 [INFO]     3,935 labeled -> Accuracy: 0.5348 (Train: 1.8s, Query: 0.00s)\n",
      "17:15:28 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5378, F1: 0.1726\n",
      "17:15:28 [INFO]   Run 3/5\n",
      "17:15:38 [INFO]     3,935 labeled -> Accuracy: 0.5232 (Train: 1.8s, Query: 0.00s)\n",
      "17:15:42 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5384, F1: 0.1704\n",
      "17:15:42 [INFO]   Run 4/5\n",
      "17:15:51 [INFO]     3,935 labeled -> Accuracy: 0.5409 (Train: 2.0s, Query: 0.00s)\n",
      "17:15:55 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5402, F1: 0.1826\n",
      "17:15:55 [INFO]   Run 5/5\n",
      "17:16:05 [INFO]     3,935 labeled -> Accuracy: 0.5366 (Train: 1.9s, Query: 0.00s)\n",
      "17:16:09 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5427, F1: 0.1842\n",
      "17:16:09 [INFO] \n",
      "Neural Network + Random Sampling - Budget: 80% (5,247 Samples)\n",
      "17:16:09 [INFO]   Run 1/5\n",
      "17:16:26 [INFO]     5,247 labeled -> Accuracy: 0.5415 (Train: 2.6s, Query: 0.00s)\n",
      "17:16:31 [INFO]     Final: 5,247 labeled -> Accuracy: 0.5512, F1: 0.1834\n",
      "17:16:31 [INFO]   Run 2/5\n",
      "17:16:48 [INFO]     5,247 labeled -> Accuracy: 0.5476 (Train: 2.8s, Query: 0.00s)\n",
      "17:16:54 [INFO]     Final: 5,247 labeled -> Accuracy: 0.5482, F1: 0.1863\n",
      "17:16:54 [INFO]   Run 3/5\n",
      "17:17:11 [INFO]     5,247 labeled -> Accuracy: 0.5409 (Train: 2.4s, Query: 0.00s)\n",
      "17:17:16 [INFO]     Final: 5,247 labeled -> Accuracy: 0.5366, F1: 0.1604\n",
      "17:17:16 [INFO]   Run 4/5\n",
      "17:17:32 [INFO]     5,247 labeled -> Accuracy: 0.5402 (Train: 2.5s, Query: 0.00s)\n",
      "17:17:37 [INFO]     Final: 5,247 labeled -> Accuracy: 0.5451, F1: 0.1874\n",
      "17:17:37 [INFO]   Run 5/5\n",
      "17:17:53 [INFO]     5,247 labeled -> Accuracy: 0.5439 (Train: 2.5s, Query: 0.00s)\n",
      "17:17:58 [INFO]     Final: 5,247 labeled -> Accuracy: 0.5348, F1: 0.1657\n",
      "17:17:58 [INFO] \n",
      "Neural Network + Random Sampling - Budget: 100% (6,559 Samples)\n",
      "17:17:58 [INFO]   Run 1/5\n",
      "17:18:20 [INFO]     6,559 labeled -> Accuracy: 0.5421 (Train: 3.1s, Query: 0.00s)\n",
      "17:18:26 [INFO]     Final: 6,559 labeled -> Accuracy: 0.5390, F1: 0.1739\n",
      "17:18:26 [INFO]   Run 2/5\n",
      "17:18:48 [INFO]     6,559 labeled -> Accuracy: 0.5445 (Train: 3.0s, Query: 0.00s)\n",
      "17:18:55 [INFO]     Final: 6,559 labeled -> Accuracy: 0.5494, F1: 0.1827\n",
      "17:18:55 [INFO]   Run 3/5\n",
      "17:19:17 [INFO]     6,559 labeled -> Accuracy: 0.5445 (Train: 3.0s, Query: 0.00s)\n",
      "17:19:24 [INFO]     Final: 6,559 labeled -> Accuracy: 0.5445, F1: 0.1860\n",
      "17:19:24 [INFO]   Run 4/5\n",
      "17:19:46 [INFO]     6,559 labeled -> Accuracy: 0.5409 (Train: 3.2s, Query: 0.00s)\n",
      "17:19:53 [INFO]     Final: 6,559 labeled -> Accuracy: 0.5463, F1: 0.1831\n",
      "17:19:53 [INFO]   Run 5/5\n",
      "17:20:16 [INFO]     6,559 labeled -> Accuracy: 0.5445 (Train: 3.3s, Query: 0.00s)\n",
      "17:20:24 [INFO]     Final: 6,559 labeled -> Accuracy: 0.5476, F1: 0.1815\n",
      "\n",
      "[ok] Neural Network + Random Sampling abgeschlossen in 6.5 Minuten\n",
      "\n",
      "============================================================\n",
      "Experiment 2/20: Neural Network + Entropy Sampling\n",
      "============================================================\n",
      "17:20:24 [INFO] \n",
      "Neural Network + Entropy Sampling - Budget: 20% (1,311 Samples)\n",
      "17:20:24 [INFO]   Run 1/5\n",
      "17:20:27 [INFO]     1,311 labeled -> Accuracy: 0.3366 (Train: 0.6s, Query: 0.02s)\n",
      "17:20:28 [INFO]     Final: 1,311 labeled -> Accuracy: 0.4860, F1: 0.1844\n",
      "17:20:28 [INFO]   Run 2/5\n",
      "17:20:31 [INFO]     1,311 labeled -> Accuracy: 0.4567 (Train: 0.6s, Query: 0.02s)\n",
      "17:20:32 [INFO]     Final: 1,311 labeled -> Accuracy: 0.5043, F1: 0.1736\n",
      "17:20:32 [INFO]   Run 3/5\n",
      "17:20:34 [INFO]     1,311 labeled -> Accuracy: 0.4293 (Train: 0.6s, Query: 0.01s)\n",
      "17:20:36 [INFO]     Final: 1,311 labeled -> Accuracy: 0.5152, F1: 0.1643\n",
      "17:20:36 [INFO]   Run 4/5\n",
      "17:20:38 [INFO]     1,311 labeled -> Accuracy: 0.3884 (Train: 0.6s, Query: 0.02s)\n",
      "17:20:40 [INFO]     Final: 1,311 labeled -> Accuracy: 0.4524, F1: 0.1651\n",
      "17:20:40 [INFO]   Run 5/5\n",
      "17:20:42 [INFO]     1,311 labeled -> Accuracy: 0.3841 (Train: 0.6s, Query: 0.02s)\n",
      "17:20:44 [INFO]     Final: 1,311 labeled -> Accuracy: 0.4902, F1: 0.1810\n",
      "17:20:44 [INFO] \n",
      "Neural Network + Entropy Sampling - Budget: 40% (2,623 Samples)\n",
      "17:20:44 [INFO]   Run 1/5\n",
      "17:20:50 [INFO]     2,623 labeled -> Accuracy: 0.4884 (Train: 1.6s, Query: 0.01s)\n",
      "17:20:53 [INFO]     Final: 2,623 labeled -> Accuracy: 0.5335, F1: 0.1701\n",
      "17:20:53 [INFO]   Run 2/5\n",
      "17:20:59 [INFO]     2,623 labeled -> Accuracy: 0.5165 (Train: 1.4s, Query: 0.01s)\n",
      "17:21:02 [INFO]     Final: 2,623 labeled -> Accuracy: 0.5159, F1: 0.1710\n",
      "17:21:02 [INFO]   Run 3/5\n",
      "17:21:08 [INFO]     2,623 labeled -> Accuracy: 0.5427 (Train: 1.7s, Query: 0.01s)\n",
      "17:21:11 [INFO]     Final: 2,623 labeled -> Accuracy: 0.5488, F1: 0.1921\n",
      "17:21:11 [INFO]   Run 4/5\n",
      "17:21:17 [INFO]     2,623 labeled -> Accuracy: 0.5073 (Train: 1.4s, Query: 0.01s)\n",
      "17:21:19 [INFO]     Final: 2,623 labeled -> Accuracy: 0.5134, F1: 0.1475\n",
      "17:21:19 [INFO]   Run 5/5\n",
      "17:21:25 [INFO]     2,623 labeled -> Accuracy: 0.4988 (Train: 1.5s, Query: 0.01s)\n",
      "17:21:28 [INFO]     Final: 2,623 labeled -> Accuracy: 0.5366, F1: 0.1925\n",
      "17:21:28 [INFO] \n",
      "Neural Network + Entropy Sampling - Budget: 60% (3,935 Samples)\n",
      "17:21:28 [INFO]   Run 1/5\n",
      "17:21:38 [INFO]     3,935 labeled -> Accuracy: 0.5360 (Train: 1.9s, Query: 0.01s)\n",
      "17:21:42 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5433, F1: 0.1920\n",
      "17:21:42 [INFO]   Run 2/5\n",
      "17:21:51 [INFO]     3,935 labeled -> Accuracy: 0.5372 (Train: 1.7s, Query: 0.01s)\n",
      "17:21:55 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5427, F1: 0.1865\n",
      "17:21:55 [INFO]   Run 3/5\n",
      "17:22:05 [INFO]     3,935 labeled -> Accuracy: 0.5268 (Train: 2.1s, Query: 0.01s)\n",
      "17:22:09 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5463, F1: 0.1963\n",
      "17:22:09 [INFO]   Run 4/5\n",
      "17:22:18 [INFO]     3,935 labeled -> Accuracy: 0.5049 (Train: 1.9s, Query: 0.01s)\n",
      "17:22:22 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5549, F1: 0.1981\n",
      "17:22:22 [INFO]   Run 5/5\n",
      "17:22:32 [INFO]     3,935 labeled -> Accuracy: 0.5213 (Train: 2.0s, Query: 0.01s)\n",
      "17:22:36 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5451, F1: 0.1807\n",
      "17:22:36 [INFO] \n",
      "Neural Network + Entropy Sampling - Budget: 80% (5,247 Samples)\n",
      "17:22:36 [INFO]   Run 1/5\n",
      "17:22:54 [INFO]     5,247 labeled -> Accuracy: 0.5323 (Train: 3.0s, Query: 0.01s)\n",
      "17:23:00 [INFO]     Final: 5,247 labeled -> Accuracy: 0.5470, F1: 0.1840\n",
      "17:23:00 [INFO]   Run 2/5\n",
      "17:23:17 [INFO]     5,247 labeled -> Accuracy: 0.5433 (Train: 2.8s, Query: 0.01s)\n",
      "17:23:23 [INFO]     Final: 5,247 labeled -> Accuracy: 0.5445, F1: 0.1884\n",
      "17:23:23 [INFO]   Run 3/5\n",
      "17:23:39 [INFO]     5,247 labeled -> Accuracy: 0.5427 (Train: 2.5s, Query: 0.00s)\n",
      "17:23:45 [INFO]     Final: 5,247 labeled -> Accuracy: 0.5524, F1: 0.1884\n",
      "17:23:45 [INFO]   Run 4/5\n",
      "17:24:00 [INFO]     5,247 labeled -> Accuracy: 0.5445 (Train: 2.6s, Query: 0.01s)\n",
      "17:24:06 [INFO]     Final: 5,247 labeled -> Accuracy: 0.5476, F1: 0.1902\n",
      "17:24:06 [INFO]   Run 5/5\n",
      "17:24:22 [INFO]     5,247 labeled -> Accuracy: 0.5372 (Train: 2.9s, Query: 0.01s)\n",
      "17:24:28 [INFO]     Final: 5,247 labeled -> Accuracy: 0.5409, F1: 0.1862\n",
      "17:24:28 [INFO] \n",
      "Neural Network + Entropy Sampling - Budget: 100% (6,559 Samples)\n",
      "17:24:28 [INFO]   Run 1/5\n",
      "17:24:55 [INFO]     6,559 labeled -> Accuracy: 0.5445 (Train: 3.5s, Query: 0.00s)\n",
      "17:25:03 [INFO]     Final: 6,559 labeled -> Accuracy: 0.5439, F1: 0.1815\n",
      "17:25:03 [INFO]   Run 2/5\n",
      "17:25:27 [INFO]     6,559 labeled -> Accuracy: 0.5488 (Train: 2.8s, Query: 0.00s)\n",
      "17:25:33 [INFO]     Final: 6,559 labeled -> Accuracy: 0.5384, F1: 0.1788\n",
      "17:25:33 [INFO]   Run 3/5\n",
      "17:25:56 [INFO]     6,559 labeled -> Accuracy: 0.5439 (Train: 3.3s, Query: 0.00s)\n",
      "17:26:03 [INFO]     Final: 6,559 labeled -> Accuracy: 0.5427, F1: 0.1896\n",
      "17:26:03 [INFO]   Run 4/5\n",
      "17:26:27 [INFO]     6,559 labeled -> Accuracy: 0.5457 (Train: 3.3s, Query: 0.00s)\n",
      "17:26:35 [INFO]     Final: 6,559 labeled -> Accuracy: 0.5494, F1: 0.1841\n",
      "17:26:35 [INFO]   Run 5/5\n",
      "17:26:58 [INFO]     6,559 labeled -> Accuracy: 0.5439 (Train: 3.3s, Query: 0.00s)\n",
      "17:27:05 [INFO]     Final: 6,559 labeled -> Accuracy: 0.5524, F1: 0.1859\n",
      "\n",
      "[ok] Neural Network + Entropy Sampling abgeschlossen in 6.7 Minuten\n",
      "\n",
      "============================================================\n",
      "Experiment 3/20: Neural Network + Margin Sampling\n",
      "============================================================\n",
      "17:27:05 [INFO] \n",
      "Neural Network + Margin Sampling - Budget: 20% (1,311 Samples)\n",
      "17:27:05 [INFO]   Run 1/5\n",
      "17:27:08 [INFO]     1,311 labeled -> Accuracy: 0.3902 (Train: 0.6s, Query: 0.01s)\n",
      "17:27:09 [INFO]     Final: 1,311 labeled -> Accuracy: 0.5207, F1: 0.1740\n",
      "17:27:09 [INFO]   Run 2/5\n",
      "17:27:11 [INFO]     1,311 labeled -> Accuracy: 0.4756 (Train: 0.6s, Query: 0.02s)\n",
      "17:27:13 [INFO]     Final: 1,311 labeled -> Accuracy: 0.5165, F1: 0.1838\n",
      "17:27:13 [INFO]   Run 3/5\n",
      "17:27:15 [INFO]     1,311 labeled -> Accuracy: 0.4683 (Train: 0.6s, Query: 0.02s)\n",
      "17:27:17 [INFO]     Final: 1,311 labeled -> Accuracy: 0.4890, F1: 0.1607\n",
      "17:27:17 [INFO]   Run 4/5\n",
      "17:27:19 [INFO]     1,311 labeled -> Accuracy: 0.4610 (Train: 0.6s, Query: 0.01s)\n",
      "17:27:21 [INFO]     Final: 1,311 labeled -> Accuracy: 0.4811, F1: 0.1680\n",
      "17:27:21 [INFO]   Run 5/5\n",
      "17:27:23 [INFO]     1,311 labeled -> Accuracy: 0.5079 (Train: 0.6s, Query: 0.01s)\n",
      "17:27:24 [INFO]     Final: 1,311 labeled -> Accuracy: 0.5104, F1: 0.1698\n",
      "17:27:24 [INFO] \n",
      "Neural Network + Margin Sampling - Budget: 40% (2,623 Samples)\n",
      "17:27:24 [INFO]   Run 1/5\n",
      "17:27:30 [INFO]     2,623 labeled -> Accuracy: 0.5329 (Train: 1.3s, Query: 0.01s)\n",
      "17:27:32 [INFO]     Final: 2,623 labeled -> Accuracy: 0.5402, F1: 0.1936\n",
      "17:27:32 [INFO]   Run 2/5\n",
      "17:27:38 [INFO]     2,623 labeled -> Accuracy: 0.5207 (Train: 1.3s, Query: 0.01s)\n",
      "17:27:41 [INFO]     Final: 2,623 labeled -> Accuracy: 0.5390, F1: 0.1825\n",
      "17:27:41 [INFO]   Run 3/5\n",
      "17:27:46 [INFO]     2,623 labeled -> Accuracy: 0.5476 (Train: 1.3s, Query: 0.01s)\n",
      "17:27:49 [INFO]     Final: 2,623 labeled -> Accuracy: 0.5445, F1: 0.1846\n",
      "17:27:49 [INFO]   Run 4/5\n",
      "17:27:55 [INFO]     2,623 labeled -> Accuracy: 0.5244 (Train: 1.3s, Query: 0.01s)\n",
      "17:27:57 [INFO]     Final: 2,623 labeled -> Accuracy: 0.5439, F1: 0.1867\n",
      "17:27:57 [INFO]   Run 5/5\n",
      "17:28:03 [INFO]     2,623 labeled -> Accuracy: 0.5402 (Train: 1.4s, Query: 0.01s)\n",
      "17:28:06 [INFO]     Final: 2,623 labeled -> Accuracy: 0.5482, F1: 0.1853\n",
      "17:28:06 [INFO] \n",
      "Neural Network + Margin Sampling - Budget: 60% (3,935 Samples)\n",
      "17:28:06 [INFO]   Run 1/5\n",
      "17:28:16 [INFO]     3,935 labeled -> Accuracy: 0.5390 (Train: 2.0s, Query: 0.01s)\n",
      "17:28:20 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5427, F1: 0.1800\n",
      "17:28:21 [INFO]   Run 2/5\n",
      "17:28:30 [INFO]     3,935 labeled -> Accuracy: 0.5378 (Train: 2.0s, Query: 0.01s)\n",
      "17:28:35 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5439, F1: 0.1860\n",
      "17:28:35 [INFO]   Run 3/5\n",
      "17:28:44 [INFO]     3,935 labeled -> Accuracy: 0.5524 (Train: 1.8s, Query: 0.01s)\n",
      "17:28:48 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5488, F1: 0.1911\n",
      "17:28:48 [INFO]   Run 4/5\n",
      "17:28:58 [INFO]     3,935 labeled -> Accuracy: 0.5433 (Train: 1.8s, Query: 0.01s)\n",
      "17:29:02 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5433, F1: 0.1874\n",
      "17:29:02 [INFO]   Run 5/5\n",
      "17:29:11 [INFO]     3,935 labeled -> Accuracy: 0.5463 (Train: 1.8s, Query: 0.01s)\n",
      "17:29:15 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5421, F1: 0.1847\n",
      "17:29:15 [INFO] \n",
      "Neural Network + Margin Sampling - Budget: 80% (5,247 Samples)\n",
      "17:29:15 [INFO]   Run 1/5\n",
      "17:29:32 [INFO]     5,247 labeled -> Accuracy: 0.5451 (Train: 2.5s, Query: 0.00s)\n",
      "17:29:37 [INFO]     Final: 5,247 labeled -> Accuracy: 0.5518, F1: 0.1878\n",
      "17:29:37 [INFO]   Run 2/5\n",
      "17:29:53 [INFO]     5,247 labeled -> Accuracy: 0.5439 (Train: 2.5s, Query: 0.01s)\n",
      "17:29:58 [INFO]     Final: 5,247 labeled -> Accuracy: 0.5537, F1: 0.1906\n",
      "17:29:58 [INFO]   Run 3/5\n",
      "17:30:16 [INFO]     5,247 labeled -> Accuracy: 0.5427 (Train: 2.8s, Query: 0.01s)\n",
      "17:30:22 [INFO]     Final: 5,247 labeled -> Accuracy: 0.5494, F1: 0.1845\n",
      "17:30:22 [INFO]   Run 4/5\n",
      "17:30:39 [INFO]     5,247 labeled -> Accuracy: 0.5421 (Train: 2.7s, Query: 0.01s)\n",
      "17:30:44 [INFO]     Final: 5,247 labeled -> Accuracy: 0.5488, F1: 0.1931\n",
      "17:30:44 [INFO]   Run 5/5\n",
      "17:31:00 [INFO]     5,247 labeled -> Accuracy: 0.5470 (Train: 2.5s, Query: 0.01s)\n",
      "17:31:06 [INFO]     Final: 5,247 labeled -> Accuracy: 0.5457, F1: 0.1856\n",
      "17:31:06 [INFO] \n",
      "Neural Network + Margin Sampling - Budget: 100% (6,559 Samples)\n",
      "17:31:06 [INFO]   Run 1/5\n",
      "17:31:29 [INFO]     6,559 labeled -> Accuracy: 0.5390 (Train: 3.2s, Query: 0.00s)\n",
      "17:31:36 [INFO]     Final: 6,559 labeled -> Accuracy: 0.5354, F1: 0.1733\n",
      "17:31:36 [INFO]   Run 2/5\n",
      "17:31:57 [INFO]     6,559 labeled -> Accuracy: 0.5470 (Train: 2.9s, Query: 0.00s)\n",
      "17:32:04 [INFO]     Final: 6,559 labeled -> Accuracy: 0.5433, F1: 0.1928\n",
      "17:32:04 [INFO]   Run 3/5\n",
      "17:32:26 [INFO]     6,559 labeled -> Accuracy: 0.5433 (Train: 2.9s, Query: 0.00s)\n",
      "17:32:32 [INFO]     Final: 6,559 labeled -> Accuracy: 0.5476, F1: 0.1971\n",
      "17:32:32 [INFO]   Run 4/5\n",
      "17:32:55 [INFO]     6,559 labeled -> Accuracy: 0.5421 (Train: 3.2s, Query: 0.00s)\n",
      "17:33:01 [INFO]     Final: 6,559 labeled -> Accuracy: 0.5421, F1: 0.1697\n",
      "17:33:01 [INFO]   Run 5/5\n",
      "17:33:25 [INFO]     6,559 labeled -> Accuracy: 0.5366 (Train: 3.4s, Query: 0.00s)\n",
      "17:33:32 [INFO]     Final: 6,559 labeled -> Accuracy: 0.5482, F1: 0.1859\n",
      "\n",
      "[ok] Neural Network + Margin Sampling abgeschlossen in 6.4 Minuten\n",
      "\n",
      "============================================================\n",
      "Experiment 4/20: Neural Network + Least Confidence\n",
      "============================================================\n",
      "17:33:32 [INFO] \n",
      "Neural Network + Least Confidence - Budget: 20% (1,311 Samples)\n",
      "17:33:32 [INFO]   Run 1/5\n",
      "17:33:34 [INFO]     1,311 labeled -> Accuracy: 0.3012 (Train: 0.5s, Query: 0.01s)\n",
      "17:33:35 [INFO]     Final: 1,311 labeled -> Accuracy: 0.4951, F1: 0.1836\n",
      "17:33:35 [INFO]   Run 2/5\n",
      "17:33:37 [INFO]     1,311 labeled -> Accuracy: 0.4695 (Train: 0.5s, Query: 0.01s)\n",
      "17:33:38 [INFO]     Final: 1,311 labeled -> Accuracy: 0.5134, F1: 0.1931\n",
      "17:33:38 [INFO]   Run 3/5\n",
      "17:33:40 [INFO]     1,311 labeled -> Accuracy: 0.3579 (Train: 0.5s, Query: 0.01s)\n",
      "17:33:41 [INFO]     Final: 1,311 labeled -> Accuracy: 0.4939, F1: 0.1601\n",
      "17:33:41 [INFO]   Run 4/5\n",
      "17:33:43 [INFO]     1,311 labeled -> Accuracy: 0.4171 (Train: 0.5s, Query: 0.01s)\n",
      "17:33:45 [INFO]     Final: 1,311 labeled -> Accuracy: 0.5213, F1: 0.1814\n",
      "17:33:45 [INFO]   Run 5/5\n",
      "17:33:47 [INFO]     1,311 labeled -> Accuracy: 0.5073 (Train: 0.6s, Query: 0.02s)\n",
      "17:33:49 [INFO]     Final: 1,311 labeled -> Accuracy: 0.5220, F1: 0.1765\n",
      "17:33:49 [INFO] \n",
      "Neural Network + Least Confidence - Budget: 40% (2,623 Samples)\n",
      "17:33:49 [INFO]   Run 1/5\n",
      "17:33:54 [INFO]     2,623 labeled -> Accuracy: 0.5415 (Train: 1.3s, Query: 0.01s)\n",
      "17:33:57 [INFO]     Final: 2,623 labeled -> Accuracy: 0.5415, F1: 0.1898\n",
      "17:33:57 [INFO]   Run 2/5\n",
      "17:34:02 [INFO]     2,623 labeled -> Accuracy: 0.5409 (Train: 1.2s, Query: 0.01s)\n",
      "17:34:05 [INFO]     Final: 2,623 labeled -> Accuracy: 0.5488, F1: 0.1856\n",
      "17:34:05 [INFO]   Run 3/5\n",
      "17:34:10 [INFO]     2,623 labeled -> Accuracy: 0.5146 (Train: 1.2s, Query: 0.01s)\n",
      "17:34:13 [INFO]     Final: 2,623 labeled -> Accuracy: 0.5177, F1: 0.1479\n",
      "17:34:13 [INFO]   Run 4/5\n",
      "17:34:17 [INFO]     2,623 labeled -> Accuracy: 0.5238 (Train: 1.2s, Query: 0.01s)\n",
      "17:34:20 [INFO]     Final: 2,623 labeled -> Accuracy: 0.5335, F1: 0.1781\n",
      "17:34:20 [INFO]   Run 5/5\n",
      "17:34:25 [INFO]     2,623 labeled -> Accuracy: 0.5183 (Train: 1.4s, Query: 0.01s)\n",
      "17:34:29 [INFO]     Final: 2,623 labeled -> Accuracy: 0.5378, F1: 0.1937\n",
      "17:34:29 [INFO] \n",
      "Neural Network + Least Confidence - Budget: 60% (3,935 Samples)\n",
      "17:34:29 [INFO]   Run 1/5\n",
      "17:34:38 [INFO]     3,935 labeled -> Accuracy: 0.5366 (Train: 1.9s, Query: 0.01s)\n",
      "17:34:43 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5470, F1: 0.1968\n",
      "17:34:43 [INFO]   Run 2/5\n",
      "17:34:52 [INFO]     3,935 labeled -> Accuracy: 0.5439 (Train: 1.9s, Query: 0.01s)\n",
      "17:34:56 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5482, F1: 0.1805\n",
      "17:34:56 [INFO]   Run 3/5\n",
      "17:35:05 [INFO]     3,935 labeled -> Accuracy: 0.5476 (Train: 1.9s, Query: 0.01s)\n",
      "17:35:09 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5451, F1: 0.1892\n",
      "17:35:09 [INFO]   Run 4/5\n",
      "17:35:18 [INFO]     3,935 labeled -> Accuracy: 0.5409 (Train: 1.9s, Query: 0.01s)\n",
      "17:35:23 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5537, F1: 0.1991\n",
      "17:35:23 [INFO]   Run 5/5\n",
      "17:35:33 [INFO]     3,935 labeled -> Accuracy: 0.5354 (Train: 1.9s, Query: 0.01s)\n",
      "17:35:37 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5482, F1: 0.1934\n",
      "17:35:37 [INFO] \n",
      "Neural Network + Least Confidence - Budget: 80% (5,247 Samples)\n",
      "17:35:37 [INFO]   Run 1/5\n",
      "17:35:53 [INFO]     5,247 labeled -> Accuracy: 0.5445 (Train: 2.5s, Query: 0.01s)\n",
      "17:35:58 [INFO]     Final: 5,247 labeled -> Accuracy: 0.5512, F1: 0.1940\n",
      "17:35:58 [INFO]   Run 2/5\n",
      "17:36:18 [INFO]     5,247 labeled -> Accuracy: 0.5470 (Train: 4.7s, Query: 0.01s)\n",
      "17:36:24 [INFO]     Final: 5,247 labeled -> Accuracy: 0.5494, F1: 0.1915\n",
      "17:36:24 [INFO]   Run 3/5\n",
      "17:36:41 [INFO]     5,247 labeled -> Accuracy: 0.5427 (Train: 3.1s, Query: 0.01s)\n",
      "17:36:47 [INFO]     Final: 5,247 labeled -> Accuracy: 0.5482, F1: 0.1910\n",
      "17:36:47 [INFO]   Run 4/5\n",
      "17:37:04 [INFO]     5,247 labeled -> Accuracy: 0.5427 (Train: 2.8s, Query: 0.01s)\n",
      "17:37:12 [INFO]     Final: 5,247 labeled -> Accuracy: 0.5561, F1: 0.1999\n",
      "17:37:12 [INFO]   Run 5/5\n",
      "17:37:28 [INFO]     5,247 labeled -> Accuracy: 0.5433 (Train: 2.5s, Query: 0.00s)\n",
      "17:37:33 [INFO]     Final: 5,247 labeled -> Accuracy: 0.5488, F1: 0.1952\n",
      "17:37:33 [INFO] \n",
      "Neural Network + Least Confidence - Budget: 100% (6,559 Samples)\n",
      "17:37:33 [INFO]   Run 1/5\n",
      "17:37:56 [INFO]     6,559 labeled -> Accuracy: 0.5433 (Train: 3.2s, Query: 0.00s)\n",
      "17:38:03 [INFO]     Final: 6,559 labeled -> Accuracy: 0.5396, F1: 0.1815\n",
      "17:38:03 [INFO]   Run 2/5\n",
      "17:38:26 [INFO]     6,559 labeled -> Accuracy: 0.5433 (Train: 3.3s, Query: 0.01s)\n",
      "17:38:33 [INFO]     Final: 6,559 labeled -> Accuracy: 0.5463, F1: 0.1974\n",
      "17:38:33 [INFO]   Run 3/5\n",
      "17:38:58 [INFO]     6,559 labeled -> Accuracy: 0.5470 (Train: 3.3s, Query: 0.00s)\n",
      "17:39:05 [INFO]     Final: 6,559 labeled -> Accuracy: 0.5488, F1: 0.2000\n",
      "17:39:05 [INFO]   Run 4/5\n",
      "17:39:29 [INFO]     6,559 labeled -> Accuracy: 0.5463 (Train: 3.3s, Query: 0.00s)\n",
      "17:39:36 [INFO]     Final: 6,559 labeled -> Accuracy: 0.5384, F1: 0.1631\n",
      "17:39:36 [INFO]   Run 5/5\n",
      "17:39:59 [INFO]     6,559 labeled -> Accuracy: 0.5384 (Train: 3.1s, Query: 0.00s)\n",
      "17:40:06 [INFO]     Final: 6,559 labeled -> Accuracy: 0.5537, F1: 0.1920\n",
      "\n",
      "[ok] Neural Network + Least Confidence abgeschlossen in 6.6 Minuten\n",
      "\n",
      "============================================================\n",
      "Experiment 5/20: Naive Bayes + Random Sampling\n",
      "============================================================\n",
      "17:40:06 [INFO] \n",
      "Naive Bayes + Random Sampling - Budget: 20% (1,311 Samples)\n",
      "17:40:06 [INFO]   Run 1/5\n",
      "17:40:06 [INFO]     1,311 labeled -> Accuracy: 0.0579 (Train: 0.0s, Query: 0.00s)\n",
      "17:40:06 [INFO]     Final: 1,311 labeled -> Accuracy: 0.0640, F1: 0.0640\n",
      "17:40:06 [INFO]   Run 2/5\n",
      "17:40:06 [INFO]     1,311 labeled -> Accuracy: 0.0720 (Train: 0.0s, Query: 0.00s)\n",
      "17:40:06 [INFO]     Final: 1,311 labeled -> Accuracy: 0.0659, F1: 0.0626\n",
      "17:40:06 [INFO]   Run 3/5\n",
      "17:40:06 [INFO]     1,311 labeled -> Accuracy: 0.0512 (Train: 0.0s, Query: 0.00s)\n",
      "17:40:06 [INFO]     Final: 1,311 labeled -> Accuracy: 0.0567, F1: 0.0539\n",
      "17:40:06 [INFO]   Run 4/5\n",
      "17:40:06 [INFO]     1,311 labeled -> Accuracy: 0.0488 (Train: 0.0s, Query: 0.00s)\n",
      "17:40:06 [INFO]     Final: 1,311 labeled -> Accuracy: 0.0488, F1: 0.0581\n",
      "17:40:06 [INFO]   Run 5/5\n",
      "17:40:06 [INFO]     1,311 labeled -> Accuracy: 0.0701 (Train: 0.0s, Query: 0.00s)\n",
      "17:40:06 [INFO]     Final: 1,311 labeled -> Accuracy: 0.0506, F1: 0.0500\n",
      "17:40:06 [INFO] \n",
      "Naive Bayes + Random Sampling - Budget: 40% (2,623 Samples)\n",
      "17:40:06 [INFO]   Run 1/5\n",
      "17:40:07 [INFO]     2,623 labeled -> Accuracy: 0.0396 (Train: 0.0s, Query: 0.00s)\n",
      "17:40:07 [INFO]     Final: 2,623 labeled -> Accuracy: 0.0396, F1: 0.0347\n",
      "17:40:07 [INFO]   Run 2/5\n",
      "17:40:07 [INFO]     2,623 labeled -> Accuracy: 0.0378 (Train: 0.0s, Query: 0.00s)\n",
      "17:40:07 [INFO]     Final: 2,623 labeled -> Accuracy: 0.0378, F1: 0.0408\n",
      "17:40:07 [INFO]   Run 3/5\n",
      "17:40:07 [INFO]     2,623 labeled -> Accuracy: 0.0372 (Train: 0.0s, Query: 0.00s)\n",
      "17:40:07 [INFO]     Final: 2,623 labeled -> Accuracy: 0.0378, F1: 0.0327\n",
      "17:40:07 [INFO]   Run 4/5\n",
      "17:40:07 [INFO]     2,623 labeled -> Accuracy: 0.0348 (Train: 0.0s, Query: 0.00s)\n",
      "17:40:07 [INFO]     Final: 2,623 labeled -> Accuracy: 0.0348, F1: 0.0385\n",
      "17:40:07 [INFO]   Run 5/5\n",
      "17:40:07 [INFO]     2,623 labeled -> Accuracy: 0.0287 (Train: 0.0s, Query: 0.00s)\n",
      "17:40:07 [INFO]     Final: 2,623 labeled -> Accuracy: 0.0287, F1: 0.0252\n",
      "17:40:07 [INFO] \n",
      "Naive Bayes + Random Sampling - Budget: 60% (3,935 Samples)\n",
      "17:40:07 [INFO]   Run 1/5\n",
      "17:40:07 [INFO]     3,935 labeled -> Accuracy: 0.0232 (Train: 0.0s, Query: 0.00s)\n",
      "17:40:07 [INFO]     Final: 3,935 labeled -> Accuracy: 0.0268, F1: 0.0318\n",
      "17:40:07 [INFO]   Run 2/5\n",
      "17:40:07 [INFO]     3,935 labeled -> Accuracy: 0.0317 (Train: 0.0s, Query: 0.00s)\n",
      "17:40:07 [INFO]     Final: 3,935 labeled -> Accuracy: 0.0280, F1: 0.0292\n",
      "17:40:07 [INFO]   Run 3/5\n",
      "17:40:07 [INFO]     3,935 labeled -> Accuracy: 0.0268 (Train: 0.0s, Query: 0.00s)\n",
      "17:40:07 [INFO]     Final: 3,935 labeled -> Accuracy: 0.0244, F1: 0.0227\n",
      "17:40:07 [INFO]   Run 4/5\n",
      "17:40:07 [INFO]     3,935 labeled -> Accuracy: 0.0238 (Train: 0.0s, Query: 0.00s)\n",
      "17:40:07 [INFO]     Final: 3,935 labeled -> Accuracy: 0.0244, F1: 0.0239\n",
      "17:40:07 [INFO]   Run 5/5\n",
      "17:40:07 [INFO]     3,935 labeled -> Accuracy: 0.0317 (Train: 0.0s, Query: 0.00s)\n",
      "17:40:07 [INFO]     Final: 3,935 labeled -> Accuracy: 0.0299, F1: 0.0279\n",
      "17:40:07 [INFO] \n",
      "Naive Bayes + Random Sampling - Budget: 80% (5,247 Samples)\n",
      "17:40:07 [INFO]   Run 1/5\n",
      "17:40:07 [INFO]     5,247 labeled -> Accuracy: 0.0207 (Train: 0.0s, Query: 0.00s)\n",
      "17:40:07 [INFO]     Final: 5,247 labeled -> Accuracy: 0.0250, F1: 0.0243\n",
      "17:40:07 [INFO]   Run 2/5\n",
      "17:40:08 [INFO]     5,247 labeled -> Accuracy: 0.0256 (Train: 0.0s, Query: 0.00s)\n",
      "17:40:08 [INFO]     Final: 5,247 labeled -> Accuracy: 0.0262, F1: 0.0310\n",
      "17:40:08 [INFO]   Run 3/5\n",
      "17:40:08 [INFO]     5,247 labeled -> Accuracy: 0.0238 (Train: 0.0s, Query: 0.00s)\n",
      "17:40:08 [INFO]     Final: 5,247 labeled -> Accuracy: 0.0268, F1: 0.0252\n",
      "17:40:08 [INFO]   Run 4/5\n",
      "17:40:08 [INFO]     5,247 labeled -> Accuracy: 0.0256 (Train: 0.0s, Query: 0.00s)\n",
      "17:40:08 [INFO]     Final: 5,247 labeled -> Accuracy: 0.0250, F1: 0.0258\n",
      "17:40:08 [INFO]   Run 5/5\n",
      "17:40:08 [INFO]     5,247 labeled -> Accuracy: 0.0268 (Train: 0.0s, Query: 0.00s)\n",
      "17:40:08 [INFO]     Final: 5,247 labeled -> Accuracy: 0.0287, F1: 0.0282\n",
      "17:40:08 [INFO] \n",
      "Naive Bayes + Random Sampling - Budget: 100% (6,559 Samples)\n",
      "17:40:08 [INFO]   Run 1/5\n",
      "17:40:08 [INFO]     6,559 labeled -> Accuracy: 0.0268 (Train: 0.0s, Query: 0.00s)\n",
      "17:40:08 [INFO]     Final: 6,559 labeled -> Accuracy: 0.0262, F1: 0.0289\n",
      "17:40:08 [INFO]   Run 2/5\n",
      "17:40:08 [INFO]     6,559 labeled -> Accuracy: 0.0256 (Train: 0.0s, Query: 0.00s)\n",
      "17:40:08 [INFO]     Final: 6,559 labeled -> Accuracy: 0.0262, F1: 0.0289\n",
      "17:40:08 [INFO]   Run 3/5\n",
      "17:40:08 [INFO]     6,559 labeled -> Accuracy: 0.0274 (Train: 0.0s, Query: 0.00s)\n",
      "17:40:08 [INFO]     Final: 6,559 labeled -> Accuracy: 0.0262, F1: 0.0289\n",
      "17:40:08 [INFO]   Run 4/5\n",
      "17:40:09 [INFO]     6,559 labeled -> Accuracy: 0.0256 (Train: 0.0s, Query: 0.00s)\n",
      "17:40:09 [INFO]     Final: 6,559 labeled -> Accuracy: 0.0262, F1: 0.0289\n",
      "17:40:09 [INFO]   Run 5/5\n",
      "17:40:09 [INFO]     6,559 labeled -> Accuracy: 0.0256 (Train: 0.0s, Query: 0.00s)\n",
      "17:40:09 [INFO]     Final: 6,559 labeled -> Accuracy: 0.0262, F1: 0.0289\n",
      "\n",
      "[ok] Naive Bayes + Random Sampling abgeschlossen in 0.0 Minuten\n",
      "\n",
      "============================================================\n",
      "Experiment 6/20: Naive Bayes + Entropy Sampling\n",
      "============================================================\n",
      "17:40:09 [INFO] \n",
      "Naive Bayes + Entropy Sampling - Budget: 20% (1,311 Samples)\n",
      "17:40:09 [INFO]   Run 1/5\n",
      "17:40:09 [INFO]     1,311 labeled -> Accuracy: 0.0744 (Train: 0.0s, Query: 0.08s)\n",
      "17:40:09 [INFO]     Final: 1,311 labeled -> Accuracy: 0.0701, F1: 0.0718\n",
      "17:40:09 [INFO]   Run 2/5\n",
      "17:40:09 [INFO]     1,311 labeled -> Accuracy: 0.0994 (Train: 0.0s, Query: 0.04s)\n",
      "17:40:09 [INFO]     Final: 1,311 labeled -> Accuracy: 0.0848, F1: 0.0704\n",
      "17:40:09 [INFO]   Run 3/5\n",
      "17:40:09 [INFO]     1,311 labeled -> Accuracy: 0.1335 (Train: 0.0s, Query: 0.02s)\n",
      "17:40:09 [INFO]     Final: 1,311 labeled -> Accuracy: 0.1024, F1: 0.0630\n",
      "17:40:09 [INFO]   Run 4/5\n",
      "17:40:09 [INFO]     1,311 labeled -> Accuracy: 0.1659 (Train: 0.0s, Query: 0.02s)\n",
      "17:40:09 [INFO]     Final: 1,311 labeled -> Accuracy: 0.1701, F1: 0.1154\n",
      "17:40:09 [INFO]   Run 5/5\n",
      "17:40:09 [INFO]     1,311 labeled -> Accuracy: 0.0579 (Train: 0.0s, Query: 0.02s)\n",
      "17:40:09 [INFO]     Final: 1,311 labeled -> Accuracy: 0.0524, F1: 0.0473\n",
      "17:40:09 [INFO] \n",
      "Naive Bayes + Entropy Sampling - Budget: 40% (2,623 Samples)\n",
      "17:40:09 [INFO]   Run 1/5\n",
      "17:40:10 [INFO]     2,623 labeled -> Accuracy: 0.0494 (Train: 0.0s, Query: 0.01s)\n",
      "17:40:10 [INFO]     Final: 2,623 labeled -> Accuracy: 0.0573, F1: 0.0663\n",
      "17:40:10 [INFO]   Run 2/5\n",
      "17:40:10 [INFO]     2,623 labeled -> Accuracy: 0.0567 (Train: 0.0s, Query: 0.01s)\n",
      "17:40:10 [INFO]     Final: 2,623 labeled -> Accuracy: 0.0567, F1: 0.0386\n",
      "17:40:10 [INFO]   Run 3/5\n",
      "17:40:10 [INFO]     2,623 labeled -> Accuracy: 0.1012 (Train: 0.0s, Query: 0.01s)\n",
      "17:40:10 [INFO]     Final: 2,623 labeled -> Accuracy: 0.1012, F1: 0.0599\n",
      "17:40:10 [INFO]   Run 4/5\n",
      "17:40:10 [INFO]     2,623 labeled -> Accuracy: 0.1268 (Train: 0.0s, Query: 0.01s)\n",
      "17:40:10 [INFO]     Final: 2,623 labeled -> Accuracy: 0.1268, F1: 0.0792\n",
      "17:40:10 [INFO]   Run 5/5\n",
      "17:40:10 [INFO]     2,623 labeled -> Accuracy: 0.0451 (Train: 0.0s, Query: 0.01s)\n",
      "17:40:10 [INFO]     Final: 2,623 labeled -> Accuracy: 0.0494, F1: 0.0458\n",
      "17:40:10 [INFO] \n",
      "Naive Bayes + Entropy Sampling - Budget: 60% (3,935 Samples)\n",
      "17:40:10 [INFO]   Run 1/5\n",
      "17:40:11 [INFO]     3,935 labeled -> Accuracy: 0.0506 (Train: 0.0s, Query: 0.01s)\n",
      "17:40:11 [INFO]     Final: 3,935 labeled -> Accuracy: 0.0500, F1: 0.0538\n",
      "17:40:11 [INFO]   Run 2/5\n",
      "17:40:11 [INFO]     3,935 labeled -> Accuracy: 0.0573 (Train: 0.0s, Query: 0.01s)\n",
      "17:40:11 [INFO]     Final: 3,935 labeled -> Accuracy: 0.0610, F1: 0.0515\n",
      "17:40:11 [INFO]   Run 3/5\n",
      "17:40:11 [INFO]     3,935 labeled -> Accuracy: 0.0927 (Train: 0.0s, Query: 0.01s)\n",
      "17:40:11 [INFO]     Final: 3,935 labeled -> Accuracy: 0.0927, F1: 0.0606\n",
      "17:40:11 [INFO]   Run 4/5\n",
      "17:40:11 [INFO]     3,935 labeled -> Accuracy: 0.1128 (Train: 0.0s, Query: 0.01s)\n",
      "17:40:11 [INFO]     Final: 3,935 labeled -> Accuracy: 0.1152, F1: 0.0790\n",
      "17:40:11 [INFO]   Run 5/5\n",
      "17:40:11 [INFO]     3,935 labeled -> Accuracy: 0.0470 (Train: 0.0s, Query: 0.01s)\n",
      "17:40:11 [INFO]     Final: 3,935 labeled -> Accuracy: 0.0470, F1: 0.0446\n",
      "17:40:11 [INFO] \n",
      "Naive Bayes + Entropy Sampling - Budget: 80% (5,247 Samples)\n",
      "17:40:11 [INFO]   Run 1/5\n",
      "17:40:12 [INFO]     5,247 labeled -> Accuracy: 0.0530 (Train: 0.0s, Query: 0.00s)\n",
      "17:40:12 [INFO]     Final: 5,247 labeled -> Accuracy: 0.0500, F1: 0.0519\n",
      "17:40:12 [INFO]   Run 2/5\n",
      "17:40:12 [INFO]     5,247 labeled -> Accuracy: 0.0585 (Train: 0.0s, Query: 0.00s)\n",
      "17:40:12 [INFO]     Final: 5,247 labeled -> Accuracy: 0.0427, F1: 0.0328\n",
      "17:40:12 [INFO]   Run 3/5\n",
      "17:40:12 [INFO]     5,247 labeled -> Accuracy: 0.0909 (Train: 0.0s, Query: 0.00s)\n",
      "17:40:12 [INFO]     Final: 5,247 labeled -> Accuracy: 0.0902, F1: 0.0592\n",
      "17:40:12 [INFO]   Run 4/5\n",
      "17:40:12 [INFO]     5,247 labeled -> Accuracy: 0.1134 (Train: 0.0s, Query: 0.00s)\n",
      "17:40:12 [INFO]     Final: 5,247 labeled -> Accuracy: 0.1104, F1: 0.0742\n",
      "17:40:13 [INFO]   Run 5/5\n",
      "17:40:13 [INFO]     5,247 labeled -> Accuracy: 0.0561 (Train: 0.0s, Query: 0.00s)\n",
      "17:40:13 [INFO]     Final: 5,247 labeled -> Accuracy: 0.0604, F1: 0.0567\n",
      "17:40:13 [INFO] \n",
      "Naive Bayes + Entropy Sampling - Budget: 100% (6,559 Samples)\n",
      "17:40:13 [INFO]   Run 1/5\n",
      "17:40:13 [INFO]     6,559 labeled -> Accuracy: 0.0402 (Train: 0.0s, Query: 0.00s)\n",
      "17:40:13 [INFO]     Final: 6,559 labeled -> Accuracy: 0.0262, F1: 0.0289\n",
      "17:40:13 [INFO]   Run 2/5\n",
      "17:40:13 [INFO]     6,559 labeled -> Accuracy: 0.0457 (Train: 0.0s, Query: 0.00s)\n",
      "17:40:13 [INFO]     Final: 6,559 labeled -> Accuracy: 0.0262, F1: 0.0289\n",
      "17:40:13 [INFO]   Run 3/5\n",
      "17:40:14 [INFO]     6,559 labeled -> Accuracy: 0.0402 (Train: 0.0s, Query: 0.00s)\n",
      "17:40:14 [INFO]     Final: 6,559 labeled -> Accuracy: 0.0262, F1: 0.0289\n",
      "17:40:14 [INFO]   Run 4/5\n",
      "17:40:14 [INFO]     6,559 labeled -> Accuracy: 0.0415 (Train: 0.0s, Query: 0.00s)\n",
      "17:40:14 [INFO]     Final: 6,559 labeled -> Accuracy: 0.0262, F1: 0.0289\n",
      "17:40:14 [INFO]   Run 5/5\n",
      "17:40:14 [INFO]     6,559 labeled -> Accuracy: 0.0238 (Train: 0.0s, Query: 0.00s)\n",
      "17:40:14 [INFO]     Final: 6,559 labeled -> Accuracy: 0.0262, F1: 0.0289\n",
      "\n",
      "[ok] Naive Bayes + Entropy Sampling abgeschlossen in 0.1 Minuten\n",
      "\n",
      "============================================================\n",
      "Experiment 7/20: Naive Bayes + Margin Sampling\n",
      "============================================================\n",
      "17:40:14 [INFO] \n",
      "Naive Bayes + Margin Sampling - Budget: 20% (1,311 Samples)\n",
      "17:40:14 [INFO]   Run 1/5\n",
      "17:40:15 [INFO]     1,311 labeled -> Accuracy: 0.0744 (Train: 0.0s, Query: 0.02s)\n",
      "17:40:15 [INFO]     Final: 1,311 labeled -> Accuracy: 0.0695, F1: 0.0727\n",
      "17:40:15 [INFO]   Run 2/5\n",
      "17:40:15 [INFO]     1,311 labeled -> Accuracy: 0.0994 (Train: 0.0s, Query: 0.02s)\n",
      "17:40:15 [INFO]     Final: 1,311 labeled -> Accuracy: 0.0902, F1: 0.0842\n",
      "17:40:15 [INFO]   Run 3/5\n",
      "17:40:15 [INFO]     1,311 labeled -> Accuracy: 0.1335 (Train: 0.0s, Query: 0.02s)\n",
      "17:40:15 [INFO]     Final: 1,311 labeled -> Accuracy: 0.1024, F1: 0.0619\n",
      "17:40:15 [INFO]   Run 4/5\n",
      "17:40:15 [INFO]     1,311 labeled -> Accuracy: 0.1701 (Train: 0.0s, Query: 0.02s)\n",
      "17:40:15 [INFO]     Final: 1,311 labeled -> Accuracy: 0.1488, F1: 0.1033\n",
      "17:40:15 [INFO]   Run 5/5\n",
      "17:40:15 [INFO]     1,311 labeled -> Accuracy: 0.0579 (Train: 0.0s, Query: 0.02s)\n",
      "17:40:15 [INFO]     Final: 1,311 labeled -> Accuracy: 0.0524, F1: 0.0473\n",
      "17:40:15 [INFO] \n",
      "Naive Bayes + Margin Sampling - Budget: 40% (2,623 Samples)\n",
      "17:40:15 [INFO]   Run 1/5\n",
      "17:40:15 [INFO]     2,623 labeled -> Accuracy: 0.0585 (Train: 0.0s, Query: 0.01s)\n",
      "17:40:15 [INFO]     Final: 2,623 labeled -> Accuracy: 0.0585, F1: 0.0593\n",
      "17:40:15 [INFO]   Run 2/5\n",
      "17:40:15 [INFO]     2,623 labeled -> Accuracy: 0.0646 (Train: 0.0s, Query: 0.01s)\n",
      "17:40:15 [INFO]     Final: 2,623 labeled -> Accuracy: 0.0646, F1: 0.0518\n",
      "17:40:15 [INFO]   Run 3/5\n",
      "17:40:16 [INFO]     2,623 labeled -> Accuracy: 0.1012 (Train: 0.0s, Query: 0.01s)\n",
      "17:40:16 [INFO]     Final: 2,623 labeled -> Accuracy: 0.1012, F1: 0.0599\n",
      "17:40:16 [INFO]   Run 4/5\n",
      "17:40:16 [INFO]     2,623 labeled -> Accuracy: 0.1226 (Train: 0.0s, Query: 0.01s)\n",
      "17:40:16 [INFO]     Final: 2,623 labeled -> Accuracy: 0.1226, F1: 0.0740\n",
      "17:40:16 [INFO]   Run 5/5\n",
      "17:40:16 [INFO]     2,623 labeled -> Accuracy: 0.0451 (Train: 0.0s, Query: 0.01s)\n",
      "17:40:16 [INFO]     Final: 2,623 labeled -> Accuracy: 0.0494, F1: 0.0458\n",
      "17:40:16 [INFO] \n",
      "Naive Bayes + Margin Sampling - Budget: 60% (3,935 Samples)\n",
      "17:40:16 [INFO]   Run 1/5\n",
      "17:40:16 [INFO]     3,935 labeled -> Accuracy: 0.0543 (Train: 0.0s, Query: 0.01s)\n",
      "17:40:16 [INFO]     Final: 3,935 labeled -> Accuracy: 0.0543, F1: 0.0558\n",
      "17:40:16 [INFO]   Run 2/5\n",
      "17:40:16 [INFO]     3,935 labeled -> Accuracy: 0.0738 (Train: 0.0s, Query: 0.01s)\n",
      "17:40:16 [INFO]     Final: 3,935 labeled -> Accuracy: 0.0738, F1: 0.0666\n",
      "17:40:16 [INFO]   Run 3/5\n",
      "17:40:17 [INFO]     3,935 labeled -> Accuracy: 0.0927 (Train: 0.0s, Query: 0.01s)\n",
      "17:40:17 [INFO]     Final: 3,935 labeled -> Accuracy: 0.0927, F1: 0.0606\n",
      "17:40:17 [INFO]   Run 4/5\n",
      "17:40:17 [INFO]     3,935 labeled -> Accuracy: 0.0463 (Train: 0.0s, Query: 0.01s)\n",
      "17:40:17 [INFO]     Final: 3,935 labeled -> Accuracy: 0.0463, F1: 0.0452\n",
      "17:40:17 [INFO]   Run 5/5\n",
      "17:40:17 [INFO]     3,935 labeled -> Accuracy: 0.0470 (Train: 0.0s, Query: 0.01s)\n",
      "17:40:17 [INFO]     Final: 3,935 labeled -> Accuracy: 0.0470, F1: 0.0446\n",
      "17:40:17 [INFO] \n",
      "Naive Bayes + Margin Sampling - Budget: 80% (5,247 Samples)\n",
      "17:40:17 [INFO]   Run 1/5\n",
      "17:40:17 [INFO]     5,247 labeled -> Accuracy: 0.0598 (Train: 0.0s, Query: 0.00s)\n",
      "17:40:17 [INFO]     Final: 5,247 labeled -> Accuracy: 0.0530, F1: 0.0565\n",
      "17:40:17 [INFO]   Run 2/5\n",
      "17:40:18 [INFO]     5,247 labeled -> Accuracy: 0.0622 (Train: 0.0s, Query: 0.00s)\n",
      "17:40:18 [INFO]     Final: 5,247 labeled -> Accuracy: 0.0622, F1: 0.0585\n",
      "17:40:18 [INFO]   Run 3/5\n",
      "17:40:18 [INFO]     5,247 labeled -> Accuracy: 0.0348 (Train: 0.0s, Query: 0.00s)\n",
      "17:40:18 [INFO]     Final: 5,247 labeled -> Accuracy: 0.0329, F1: 0.0369\n",
      "17:40:18 [INFO]   Run 4/5\n",
      "17:40:18 [INFO]     5,247 labeled -> Accuracy: 0.0494 (Train: 0.0s, Query: 0.00s)\n",
      "17:40:18 [INFO]     Final: 5,247 labeled -> Accuracy: 0.0482, F1: 0.0511\n",
      "17:40:18 [INFO]   Run 5/5\n",
      "17:40:18 [INFO]     5,247 labeled -> Accuracy: 0.0628 (Train: 0.0s, Query: 0.00s)\n",
      "17:40:18 [INFO]     Final: 5,247 labeled -> Accuracy: 0.0671, F1: 0.0706\n",
      "17:40:18 [INFO] \n",
      "Naive Bayes + Margin Sampling - Budget: 100% (6,559 Samples)\n",
      "17:40:18 [INFO]   Run 1/5\n",
      "17:40:19 [INFO]     6,559 labeled -> Accuracy: 0.0305 (Train: 0.0s, Query: 0.00s)\n",
      "17:40:19 [INFO]     Final: 6,559 labeled -> Accuracy: 0.0262, F1: 0.0289\n",
      "17:40:19 [INFO]   Run 2/5\n",
      "17:40:19 [INFO]     6,559 labeled -> Accuracy: 0.0268 (Train: 0.0s, Query: 0.00s)\n",
      "17:40:19 [INFO]     Final: 6,559 labeled -> Accuracy: 0.0262, F1: 0.0289\n",
      "17:40:19 [INFO]   Run 3/5\n",
      "17:40:19 [INFO]     6,559 labeled -> Accuracy: 0.0250 (Train: 0.0s, Query: 0.00s)\n",
      "17:40:19 [INFO]     Final: 6,559 labeled -> Accuracy: 0.0262, F1: 0.0289\n",
      "17:40:19 [INFO]   Run 4/5\n",
      "17:40:20 [INFO]     6,559 labeled -> Accuracy: 0.0250 (Train: 0.0s, Query: 0.00s)\n",
      "17:40:20 [INFO]     Final: 6,559 labeled -> Accuracy: 0.0262, F1: 0.0289\n",
      "17:40:20 [INFO]   Run 5/5\n",
      "17:40:20 [INFO]     6,559 labeled -> Accuracy: 0.0317 (Train: 0.0s, Query: 0.00s)\n",
      "17:40:20 [INFO]     Final: 6,559 labeled -> Accuracy: 0.0262, F1: 0.0289\n",
      "\n",
      "[ok] Naive Bayes + Margin Sampling abgeschlossen in 0.1 Minuten\n",
      "\n",
      "============================================================\n",
      "Experiment 8/20: Naive Bayes + Least Confidence\n",
      "============================================================\n",
      "17:40:20 [INFO] \n",
      "Naive Bayes + Least Confidence - Budget: 20% (1,311 Samples)\n",
      "17:40:20 [INFO]   Run 1/5\n",
      "17:40:20 [INFO]     1,311 labeled -> Accuracy: 0.0744 (Train: 0.0s, Query: 0.03s)\n",
      "17:40:20 [INFO]     Final: 1,311 labeled -> Accuracy: 0.0695, F1: 0.0727\n",
      "17:40:20 [INFO]   Run 2/5\n",
      "17:40:20 [INFO]     1,311 labeled -> Accuracy: 0.0994 (Train: 0.0s, Query: 0.02s)\n",
      "17:40:20 [INFO]     Final: 1,311 labeled -> Accuracy: 0.0854, F1: 0.0724\n",
      "17:40:20 [INFO]   Run 3/5\n",
      "17:40:20 [INFO]     1,311 labeled -> Accuracy: 0.1335 (Train: 0.0s, Query: 0.02s)\n",
      "17:40:20 [INFO]     Final: 1,311 labeled -> Accuracy: 0.1024, F1: 0.0619\n",
      "17:40:20 [INFO]   Run 4/5\n",
      "17:40:21 [INFO]     1,311 labeled -> Accuracy: 0.1707 (Train: 0.0s, Query: 0.02s)\n",
      "17:40:21 [INFO]     Final: 1,311 labeled -> Accuracy: 0.1488, F1: 0.1032\n",
      "17:40:21 [INFO]   Run 5/5\n",
      "17:40:21 [INFO]     1,311 labeled -> Accuracy: 0.0579 (Train: 0.0s, Query: 0.03s)\n",
      "17:40:21 [INFO]     Final: 1,311 labeled -> Accuracy: 0.0524, F1: 0.0473\n",
      "17:40:21 [INFO] \n",
      "Naive Bayes + Least Confidence - Budget: 40% (2,623 Samples)\n",
      "17:40:21 [INFO]   Run 1/5\n",
      "17:40:21 [INFO]     2,623 labeled -> Accuracy: 0.0585 (Train: 0.0s, Query: 0.01s)\n",
      "17:40:21 [INFO]     Final: 2,623 labeled -> Accuracy: 0.0585, F1: 0.0593\n",
      "17:40:21 [INFO]   Run 2/5\n",
      "17:40:21 [INFO]     2,623 labeled -> Accuracy: 0.0567 (Train: 0.0s, Query: 0.01s)\n",
      "17:40:21 [INFO]     Final: 2,623 labeled -> Accuracy: 0.0567, F1: 0.0386\n",
      "17:40:21 [INFO]   Run 3/5\n",
      "17:40:21 [INFO]     2,623 labeled -> Accuracy: 0.1012 (Train: 0.0s, Query: 0.01s)\n",
      "17:40:21 [INFO]     Final: 2,623 labeled -> Accuracy: 0.1012, F1: 0.0599\n",
      "17:40:21 [INFO]   Run 4/5\n",
      "17:40:21 [INFO]     2,623 labeled -> Accuracy: 0.1159 (Train: 0.0s, Query: 0.01s)\n",
      "17:40:21 [INFO]     Final: 2,623 labeled -> Accuracy: 0.1159, F1: 0.0603\n",
      "17:40:21 [INFO]   Run 5/5\n",
      "17:40:22 [INFO]     2,623 labeled -> Accuracy: 0.0451 (Train: 0.0s, Query: 0.01s)\n",
      "17:40:22 [INFO]     Final: 2,623 labeled -> Accuracy: 0.0494, F1: 0.0458\n",
      "17:40:22 [INFO] \n",
      "Naive Bayes + Least Confidence - Budget: 60% (3,935 Samples)\n",
      "17:40:22 [INFO]   Run 1/5\n",
      "17:40:22 [INFO]     3,935 labeled -> Accuracy: 0.0543 (Train: 0.0s, Query: 0.01s)\n",
      "17:40:22 [INFO]     Final: 3,935 labeled -> Accuracy: 0.0543, F1: 0.0558\n",
      "17:40:22 [INFO]   Run 2/5\n",
      "17:40:22 [INFO]     3,935 labeled -> Accuracy: 0.0573 (Train: 0.0s, Query: 0.01s)\n",
      "17:40:22 [INFO]     Final: 3,935 labeled -> Accuracy: 0.0610, F1: 0.0518\n",
      "17:40:22 [INFO]   Run 3/5\n",
      "17:40:22 [INFO]     3,935 labeled -> Accuracy: 0.0927 (Train: 0.0s, Query: 0.01s)\n",
      "17:40:22 [INFO]     Final: 3,935 labeled -> Accuracy: 0.0927, F1: 0.0606\n",
      "17:40:22 [INFO]   Run 4/5\n",
      "17:40:23 [INFO]     3,935 labeled -> Accuracy: 0.1171 (Train: 0.0s, Query: 0.01s)\n",
      "17:40:23 [INFO]     Final: 3,935 labeled -> Accuracy: 0.1165, F1: 0.0640\n",
      "17:40:23 [INFO]   Run 5/5\n",
      "17:40:23 [INFO]     3,935 labeled -> Accuracy: 0.0470 (Train: 0.0s, Query: 0.01s)\n",
      "17:40:23 [INFO]     Final: 3,935 labeled -> Accuracy: 0.0470, F1: 0.0446\n",
      "17:40:23 [INFO] \n",
      "Naive Bayes + Least Confidence - Budget: 80% (5,247 Samples)\n",
      "17:40:23 [INFO]   Run 1/5\n",
      "17:40:23 [INFO]     5,247 labeled -> Accuracy: 0.0598 (Train: 0.0s, Query: 0.00s)\n",
      "17:40:23 [INFO]     Final: 5,247 labeled -> Accuracy: 0.0366, F1: 0.0391\n",
      "17:40:23 [INFO]   Run 2/5\n",
      "17:40:23 [INFO]     5,247 labeled -> Accuracy: 0.0573 (Train: 0.0s, Query: 0.00s)\n",
      "17:40:23 [INFO]     Final: 5,247 labeled -> Accuracy: 0.0500, F1: 0.0480\n",
      "17:40:23 [INFO]   Run 3/5\n",
      "17:40:24 [INFO]     5,247 labeled -> Accuracy: 0.0348 (Train: 0.0s, Query: 0.00s)\n",
      "17:40:24 [INFO]     Final: 5,247 labeled -> Accuracy: 0.0335, F1: 0.0362\n",
      "17:40:24 [INFO]   Run 4/5\n",
      "17:40:24 [INFO]     5,247 labeled -> Accuracy: 0.1037 (Train: 0.0s, Query: 0.01s)\n",
      "17:40:24 [INFO]     Final: 5,247 labeled -> Accuracy: 0.1061, F1: 0.0616\n",
      "17:40:24 [INFO]   Run 5/5\n",
      "17:40:24 [INFO]     5,247 labeled -> Accuracy: 0.0622 (Train: 0.0s, Query: 0.00s)\n",
      "17:40:24 [INFO]     Final: 5,247 labeled -> Accuracy: 0.0604, F1: 0.0648\n",
      "17:40:24 [INFO] \n",
      "Naive Bayes + Least Confidence - Budget: 100% (6,559 Samples)\n",
      "17:40:24 [INFO]   Run 1/5\n",
      "17:40:24 [INFO]     6,559 labeled -> Accuracy: 0.0299 (Train: 0.0s, Query: 0.00s)\n",
      "17:40:24 [INFO]     Final: 6,559 labeled -> Accuracy: 0.0262, F1: 0.0289\n",
      "17:40:24 [INFO]   Run 2/5\n",
      "17:40:25 [INFO]     6,559 labeled -> Accuracy: 0.0244 (Train: 0.0s, Query: 0.00s)\n",
      "17:40:25 [INFO]     Final: 6,559 labeled -> Accuracy: 0.0262, F1: 0.0289\n",
      "17:40:25 [INFO]   Run 3/5\n",
      "17:40:25 [INFO]     6,559 labeled -> Accuracy: 0.0244 (Train: 0.0s, Query: 0.00s)\n",
      "17:40:25 [INFO]     Final: 6,559 labeled -> Accuracy: 0.0262, F1: 0.0289\n",
      "17:40:25 [INFO]   Run 4/5\n",
      "17:40:25 [INFO]     6,559 labeled -> Accuracy: 0.0226 (Train: 0.0s, Query: 0.00s)\n",
      "17:40:25 [INFO]     Final: 6,559 labeled -> Accuracy: 0.0262, F1: 0.0289\n",
      "17:40:25 [INFO]   Run 5/5\n",
      "17:40:26 [INFO]     6,559 labeled -> Accuracy: 0.0433 (Train: 0.0s, Query: 0.00s)\n",
      "17:40:26 [INFO]     Final: 6,559 labeled -> Accuracy: 0.0262, F1: 0.0289\n",
      "\n",
      "[ok] Naive Bayes + Least Confidence abgeschlossen in 0.1 Minuten\n",
      "\n",
      "============================================================\n",
      "Experiment 9/20: Random Forest + Random Sampling\n",
      "============================================================\n",
      "17:40:26 [INFO] \n",
      "Random Forest + Random Sampling - Budget: 20% (1,311 Samples)\n",
      "17:40:26 [INFO]   Run 1/5\n",
      "17:40:27 [INFO]     1,311 labeled -> Accuracy: 0.4817 (Train: 0.4s, Query: 0.00s)\n",
      "17:40:28 [INFO]     Final: 1,311 labeled -> Accuracy: 0.4811, F1: 0.2031\n",
      "17:40:28 [INFO]   Run 2/5\n",
      "17:40:29 [INFO]     1,311 labeled -> Accuracy: 0.4707 (Train: 0.4s, Query: 0.00s)\n",
      "17:40:29 [INFO]     Final: 1,311 labeled -> Accuracy: 0.4659, F1: 0.2231\n",
      "17:40:29 [INFO]   Run 3/5\n",
      "17:40:31 [INFO]     1,311 labeled -> Accuracy: 0.4787 (Train: 0.4s, Query: 0.00s)\n",
      "17:40:31 [INFO]     Final: 1,311 labeled -> Accuracy: 0.4823, F1: 0.2358\n",
      "17:40:31 [INFO]   Run 4/5\n",
      "17:40:33 [INFO]     1,311 labeled -> Accuracy: 0.4659 (Train: 0.4s, Query: 0.00s)\n",
      "17:40:33 [INFO]     Final: 1,311 labeled -> Accuracy: 0.4677, F1: 0.2366\n",
      "17:40:33 [INFO]   Run 5/5\n",
      "17:40:34 [INFO]     1,311 labeled -> Accuracy: 0.4744 (Train: 0.4s, Query: 0.00s)\n",
      "17:40:35 [INFO]     Final: 1,311 labeled -> Accuracy: 0.4610, F1: 0.2208\n",
      "17:40:35 [INFO] \n",
      "Random Forest + Random Sampling - Budget: 40% (2,623 Samples)\n",
      "17:40:35 [INFO]   Run 1/5\n",
      "17:40:38 [INFO]     2,623 labeled -> Accuracy: 0.4811 (Train: 0.4s, Query: 0.00s)\n",
      "17:40:38 [INFO]     Final: 2,623 labeled -> Accuracy: 0.4799, F1: 0.2166\n",
      "17:40:38 [INFO]   Run 2/5\n",
      "17:40:41 [INFO]     2,623 labeled -> Accuracy: 0.4549 (Train: 0.4s, Query: 0.00s)\n",
      "17:40:42 [INFO]     Final: 2,623 labeled -> Accuracy: 0.4537, F1: 0.2016\n",
      "17:40:42 [INFO]   Run 3/5\n",
      "17:40:44 [INFO]     2,623 labeled -> Accuracy: 0.4671 (Train: 0.5s, Query: 0.00s)\n",
      "17:40:45 [INFO]     Final: 2,623 labeled -> Accuracy: 0.4659, F1: 0.2373\n",
      "17:40:45 [INFO]   Run 4/5\n",
      "17:40:48 [INFO]     2,623 labeled -> Accuracy: 0.4817 (Train: 0.4s, Query: 0.00s)\n",
      "17:40:48 [INFO]     Final: 2,623 labeled -> Accuracy: 0.4793, F1: 0.2383\n",
      "17:40:48 [INFO]   Run 5/5\n",
      "17:40:51 [INFO]     2,623 labeled -> Accuracy: 0.4622 (Train: 0.4s, Query: 0.00s)\n",
      "17:40:52 [INFO]     Final: 2,623 labeled -> Accuracy: 0.4604, F1: 0.2214\n",
      "17:40:52 [INFO] \n",
      "Random Forest + Random Sampling - Budget: 60% (3,935 Samples)\n",
      "17:40:52 [INFO]   Run 1/5\n",
      "17:40:55 [INFO]     3,935 labeled -> Accuracy: 0.4646 (Train: 0.5s, Query: 0.00s)\n",
      "17:40:56 [INFO]     Final: 3,935 labeled -> Accuracy: 0.4689, F1: 0.2164\n",
      "17:40:56 [INFO]   Run 2/5\n",
      "17:41:00 [INFO]     3,935 labeled -> Accuracy: 0.4524 (Train: 0.6s, Query: 0.00s)\n",
      "17:41:01 [INFO]     Final: 3,935 labeled -> Accuracy: 0.4591, F1: 0.2065\n",
      "17:41:01 [INFO]   Run 3/5\n",
      "17:41:05 [INFO]     3,935 labeled -> Accuracy: 0.4720 (Train: 0.5s, Query: 0.00s)\n",
      "17:41:05 [INFO]     Final: 3,935 labeled -> Accuracy: 0.4750, F1: 0.2299\n",
      "17:41:05 [INFO]   Run 4/5\n",
      "17:41:09 [INFO]     3,935 labeled -> Accuracy: 0.4799 (Train: 0.4s, Query: 0.00s)\n",
      "17:41:10 [INFO]     Final: 3,935 labeled -> Accuracy: 0.4732, F1: 0.2243\n",
      "17:41:10 [INFO]   Run 5/5\n",
      "17:41:14 [INFO]     3,935 labeled -> Accuracy: 0.4823 (Train: 0.4s, Query: 0.00s)\n",
      "17:41:14 [INFO]     Final: 3,935 labeled -> Accuracy: 0.4793, F1: 0.2328\n",
      "17:41:14 [INFO] \n",
      "Random Forest + Random Sampling - Budget: 80% (5,247 Samples)\n",
      "17:41:14 [INFO]   Run 1/5\n",
      "17:41:20 [INFO]     5,247 labeled -> Accuracy: 0.4683 (Train: 0.5s, Query: 0.00s)\n",
      "17:41:21 [INFO]     Final: 5,247 labeled -> Accuracy: 0.4701, F1: 0.2246\n",
      "17:41:21 [INFO]   Run 2/5\n",
      "17:41:26 [INFO]     5,247 labeled -> Accuracy: 0.4561 (Train: 0.6s, Query: 0.00s)\n",
      "17:41:27 [INFO]     Final: 5,247 labeled -> Accuracy: 0.4561, F1: 0.2270\n",
      "17:41:27 [INFO]   Run 3/5\n",
      "17:41:33 [INFO]     5,247 labeled -> Accuracy: 0.4713 (Train: 0.6s, Query: 0.00s)\n",
      "17:41:33 [INFO]     Final: 5,247 labeled -> Accuracy: 0.4732, F1: 0.2255\n",
      "17:41:33 [INFO]   Run 4/5\n",
      "17:41:38 [INFO]     5,247 labeled -> Accuracy: 0.4604 (Train: 0.5s, Query: 0.00s)\n",
      "17:41:39 [INFO]     Final: 5,247 labeled -> Accuracy: 0.4604, F1: 0.2162\n",
      "17:41:39 [INFO]   Run 5/5\n",
      "17:41:44 [INFO]     5,247 labeled -> Accuracy: 0.4683 (Train: 0.5s, Query: 0.00s)\n",
      "17:41:45 [INFO]     Final: 5,247 labeled -> Accuracy: 0.4713, F1: 0.2244\n",
      "17:41:45 [INFO] \n",
      "Random Forest + Random Sampling - Budget: 100% (6,559 Samples)\n",
      "17:41:45 [INFO]   Run 1/5\n",
      "17:41:52 [INFO]     6,559 labeled -> Accuracy: 0.4713 (Train: 0.6s, Query: 0.00s)\n",
      "17:41:53 [INFO]     Final: 6,559 labeled -> Accuracy: 0.4726, F1: 0.2328\n",
      "17:41:53 [INFO]   Run 2/5\n",
      "17:41:59 [INFO]     6,559 labeled -> Accuracy: 0.4646 (Train: 0.6s, Query: 0.00s)\n",
      "17:41:59 [INFO]     Final: 6,559 labeled -> Accuracy: 0.4707, F1: 0.2304\n",
      "17:41:59 [INFO]   Run 3/5\n",
      "17:42:06 [INFO]     6,559 labeled -> Accuracy: 0.4689 (Train: 0.6s, Query: 0.00s)\n",
      "17:42:07 [INFO]     Final: 6,559 labeled -> Accuracy: 0.4713, F1: 0.2342\n",
      "17:42:07 [INFO]   Run 4/5\n",
      "17:42:13 [INFO]     6,559 labeled -> Accuracy: 0.4720 (Train: 0.6s, Query: 0.00s)\n",
      "17:42:14 [INFO]     Final: 6,559 labeled -> Accuracy: 0.4713, F1: 0.2305\n",
      "17:42:14 [INFO]   Run 5/5\n",
      "17:42:20 [INFO]     6,559 labeled -> Accuracy: 0.4732 (Train: 0.6s, Query: 0.00s)\n",
      "17:42:21 [INFO]     Final: 6,559 labeled -> Accuracy: 0.4707, F1: 0.2297\n",
      "\n",
      "[ok] Random Forest + Random Sampling abgeschlossen in 1.9 Minuten\n",
      "\n",
      "============================================================\n",
      "Experiment 10/20: Random Forest + Entropy Sampling\n",
      "============================================================\n",
      "17:42:21 [INFO] \n",
      "Random Forest + Entropy Sampling - Budget: 20% (1,311 Samples)\n",
      "17:42:21 [INFO]   Run 1/5\n",
      "17:42:23 [INFO]     1,311 labeled -> Accuracy: 0.4726 (Train: 0.4s, Query: 0.07s)\n",
      "17:42:23 [INFO]     Final: 1,311 labeled -> Accuracy: 0.4793, F1: 0.2353\n",
      "17:42:23 [INFO]   Run 2/5\n",
      "17:42:25 [INFO]     1,311 labeled -> Accuracy: 0.4439 (Train: 0.4s, Query: 0.07s)\n",
      "17:42:25 [INFO]     Final: 1,311 labeled -> Accuracy: 0.4585, F1: 0.2179\n",
      "17:42:25 [INFO]   Run 3/5\n",
      "17:42:27 [INFO]     1,311 labeled -> Accuracy: 0.4945 (Train: 0.4s, Query: 0.07s)\n",
      "17:42:27 [INFO]     Final: 1,311 labeled -> Accuracy: 0.4933, F1: 0.2554\n",
      "17:42:27 [INFO]   Run 4/5\n",
      "17:42:29 [INFO]     1,311 labeled -> Accuracy: 0.4915 (Train: 0.4s, Query: 0.07s)\n",
      "17:42:29 [INFO]     Final: 1,311 labeled -> Accuracy: 0.4945, F1: 0.2121\n",
      "17:42:29 [INFO]   Run 5/5\n",
      "17:42:31 [INFO]     1,311 labeled -> Accuracy: 0.4573 (Train: 0.3s, Query: 0.05s)\n",
      "17:42:31 [INFO]     Final: 1,311 labeled -> Accuracy: 0.4634, F1: 0.2126\n",
      "17:42:31 [INFO] \n",
      "Random Forest + Entropy Sampling - Budget: 40% (2,623 Samples)\n",
      "17:42:31 [INFO]   Run 1/5\n",
      "17:42:34 [INFO]     2,623 labeled -> Accuracy: 0.5043 (Train: 0.5s, Query: 0.08s)\n",
      "17:42:35 [INFO]     Final: 2,623 labeled -> Accuracy: 0.5061, F1: 0.2338\n",
      "17:42:35 [INFO]   Run 2/5\n",
      "17:42:39 [INFO]     2,623 labeled -> Accuracy: 0.5037 (Train: 0.5s, Query: 0.08s)\n",
      "17:42:39 [INFO]     Final: 2,623 labeled -> Accuracy: 0.5104, F1: 0.2244\n",
      "17:42:39 [INFO]   Run 3/5\n",
      "17:42:42 [INFO]     2,623 labeled -> Accuracy: 0.5018 (Train: 0.5s, Query: 0.12s)\n",
      "17:42:43 [INFO]     Final: 2,623 labeled -> Accuracy: 0.5000, F1: 0.2381\n",
      "17:42:43 [INFO]   Run 4/5\n",
      "17:42:46 [INFO]     2,623 labeled -> Accuracy: 0.4835 (Train: 0.5s, Query: 0.06s)\n",
      "17:42:46 [INFO]     Final: 2,623 labeled -> Accuracy: 0.4841, F1: 0.2185\n",
      "17:42:46 [INFO]   Run 5/5\n",
      "17:42:50 [INFO]     2,623 labeled -> Accuracy: 0.4579 (Train: 0.4s, Query: 0.05s)\n",
      "17:42:50 [INFO]     Final: 2,623 labeled -> Accuracy: 0.4604, F1: 0.2112\n",
      "17:42:50 [INFO] \n",
      "Random Forest + Entropy Sampling - Budget: 60% (3,935 Samples)\n",
      "17:42:50 [INFO]   Run 1/5\n",
      "17:42:54 [INFO]     3,935 labeled -> Accuracy: 0.5061 (Train: 0.4s, Query: 0.04s)\n",
      "17:42:55 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5079, F1: 0.2219\n",
      "17:42:55 [INFO]   Run 2/5\n",
      "17:42:59 [INFO]     3,935 labeled -> Accuracy: 0.5000 (Train: 0.5s, Query: 0.05s)\n",
      "17:43:00 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5128, F1: 0.2221\n",
      "17:43:00 [INFO]   Run 3/5\n",
      "17:43:04 [INFO]     3,935 labeled -> Accuracy: 0.4988 (Train: 0.5s, Query: 0.07s)\n",
      "17:43:05 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5134, F1: 0.2305\n",
      "17:43:05 [INFO]   Run 4/5\n",
      "17:43:09 [INFO]     3,935 labeled -> Accuracy: 0.4927 (Train: 0.5s, Query: 0.05s)\n",
      "17:43:10 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5098, F1: 0.2441\n",
      "17:43:10 [INFO]   Run 5/5\n",
      "17:43:14 [INFO]     3,935 labeled -> Accuracy: 0.4732 (Train: 0.5s, Query: 0.10s)\n",
      "17:43:15 [INFO]     Final: 3,935 labeled -> Accuracy: 0.4896, F1: 0.2270\n",
      "17:43:15 [INFO] \n",
      "Random Forest + Entropy Sampling - Budget: 80% (5,247 Samples)\n",
      "17:43:15 [INFO]   Run 1/5\n",
      "17:43:22 [INFO]     5,247 labeled -> Accuracy: 0.5049 (Train: 0.6s, Query: 0.08s)\n",
      "17:43:22 [INFO]     Final: 5,247 labeled -> Accuracy: 0.5037, F1: 0.2345\n",
      "17:43:22 [INFO]   Run 2/5\n",
      "17:43:28 [INFO]     5,247 labeled -> Accuracy: 0.5030 (Train: 0.6s, Query: 0.07s)\n",
      "17:43:29 [INFO]     Final: 5,247 labeled -> Accuracy: 0.5030, F1: 0.2249\n",
      "17:43:29 [INFO]   Run 3/5\n",
      "17:43:36 [INFO]     5,247 labeled -> Accuracy: 0.4927 (Train: 0.6s, Query: 0.07s)\n",
      "17:43:36 [INFO]     Final: 5,247 labeled -> Accuracy: 0.4933, F1: 0.2155\n",
      "17:43:36 [INFO]   Run 4/5\n",
      "17:43:43 [INFO]     5,247 labeled -> Accuracy: 0.5018 (Train: 0.6s, Query: 0.07s)\n",
      "17:43:44 [INFO]     Final: 5,247 labeled -> Accuracy: 0.5049, F1: 0.2363\n",
      "17:43:44 [INFO]   Run 5/5\n",
      "17:43:51 [INFO]     5,247 labeled -> Accuracy: 0.4909 (Train: 0.6s, Query: 0.06s)\n",
      "17:43:51 [INFO]     Final: 5,247 labeled -> Accuracy: 0.4921, F1: 0.2305\n",
      "17:43:51 [INFO] \n",
      "Random Forest + Entropy Sampling - Budget: 100% (6,559 Samples)\n",
      "17:43:51 [INFO]   Run 1/5\n",
      "17:44:00 [INFO]     6,559 labeled -> Accuracy: 0.4774 (Train: 0.8s, Query: 0.06s)\n",
      "17:44:01 [INFO]     Final: 6,559 labeled -> Accuracy: 0.4701, F1: 0.2297\n",
      "17:44:01 [INFO]   Run 2/5\n",
      "17:44:09 [INFO]     6,559 labeled -> Accuracy: 0.4817 (Train: 0.5s, Query: 0.05s)\n",
      "17:44:09 [INFO]     Final: 6,559 labeled -> Accuracy: 0.4732, F1: 0.2363\n",
      "17:44:09 [INFO]   Run 3/5\n",
      "17:44:17 [INFO]     6,559 labeled -> Accuracy: 0.4774 (Train: 0.6s, Query: 0.04s)\n",
      "17:44:18 [INFO]     Final: 6,559 labeled -> Accuracy: 0.4720, F1: 0.2305\n",
      "17:44:18 [INFO]   Run 4/5\n",
      "17:44:26 [INFO]     6,559 labeled -> Accuracy: 0.4848 (Train: 0.7s, Query: 0.06s)\n",
      "17:44:27 [INFO]     Final: 6,559 labeled -> Accuracy: 0.4720, F1: 0.2348\n",
      "17:44:27 [INFO]   Run 5/5\n",
      "17:44:35 [INFO]     6,559 labeled -> Accuracy: 0.4720 (Train: 0.6s, Query: 0.07s)\n",
      "17:44:36 [INFO]     Final: 6,559 labeled -> Accuracy: 0.4713, F1: 0.2260\n",
      "\n",
      "[ok] Random Forest + Entropy Sampling abgeschlossen in 2.2 Minuten\n",
      "\n",
      "============================================================\n",
      "Experiment 11/20: Random Forest + Margin Sampling\n",
      "============================================================\n",
      "17:44:36 [INFO] \n",
      "Random Forest + Margin Sampling - Budget: 20% (1,311 Samples)\n",
      "17:44:36 [INFO]   Run 1/5\n",
      "17:44:37 [INFO]     1,311 labeled -> Accuracy: 0.4659 (Train: 0.4s, Query: 0.07s)\n",
      "17:44:38 [INFO]     Final: 1,311 labeled -> Accuracy: 0.4683, F1: 0.2241\n",
      "17:44:38 [INFO]   Run 2/5\n",
      "17:44:39 [INFO]     1,311 labeled -> Accuracy: 0.4933 (Train: 0.3s, Query: 0.05s)\n",
      "17:44:39 [INFO]     Final: 1,311 labeled -> Accuracy: 0.4988, F1: 0.2162\n",
      "17:44:39 [INFO]   Run 3/5\n",
      "17:44:41 [INFO]     1,311 labeled -> Accuracy: 0.4970 (Train: 0.3s, Query: 0.07s)\n",
      "17:44:41 [INFO]     Final: 1,311 labeled -> Accuracy: 0.5012, F1: 0.2246\n",
      "17:44:41 [INFO]   Run 4/5\n",
      "17:44:43 [INFO]     1,311 labeled -> Accuracy: 0.5226 (Train: 0.3s, Query: 0.04s)\n",
      "17:44:43 [INFO]     Final: 1,311 labeled -> Accuracy: 0.5213, F1: 0.2449\n",
      "17:44:43 [INFO]   Run 5/5\n",
      "17:44:45 [INFO]     1,311 labeled -> Accuracy: 0.4738 (Train: 0.4s, Query: 0.09s)\n",
      "17:44:45 [INFO]     Final: 1,311 labeled -> Accuracy: 0.4848, F1: 0.2053\n",
      "17:44:45 [INFO] \n",
      "Random Forest + Margin Sampling - Budget: 40% (2,623 Samples)\n",
      "17:44:45 [INFO]   Run 1/5\n",
      "17:44:48 [INFO]     2,623 labeled -> Accuracy: 0.5073 (Train: 0.4s, Query: 0.06s)\n",
      "17:44:49 [INFO]     Final: 2,623 labeled -> Accuracy: 0.5024, F1: 0.2278\n",
      "17:44:49 [INFO]   Run 2/5\n",
      "17:44:52 [INFO]     2,623 labeled -> Accuracy: 0.5195 (Train: 0.4s, Query: 0.07s)\n",
      "17:44:52 [INFO]     Final: 2,623 labeled -> Accuracy: 0.5195, F1: 0.2240\n",
      "17:44:52 [INFO]   Run 3/5\n",
      "17:44:55 [INFO]     2,623 labeled -> Accuracy: 0.5189 (Train: 0.4s, Query: 0.07s)\n",
      "17:44:55 [INFO]     Final: 2,623 labeled -> Accuracy: 0.5201, F1: 0.2352\n",
      "17:44:55 [INFO]   Run 4/5\n",
      "17:44:58 [INFO]     2,623 labeled -> Accuracy: 0.5299 (Train: 0.4s, Query: 0.05s)\n",
      "17:44:59 [INFO]     Final: 2,623 labeled -> Accuracy: 0.5280, F1: 0.2282\n",
      "17:44:59 [INFO]   Run 5/5\n",
      "17:45:02 [INFO]     2,623 labeled -> Accuracy: 0.5152 (Train: 0.5s, Query: 0.08s)\n",
      "17:45:03 [INFO]     Final: 2,623 labeled -> Accuracy: 0.5165, F1: 0.2189\n",
      "17:45:03 [INFO] \n",
      "Random Forest + Margin Sampling - Budget: 60% (3,935 Samples)\n",
      "17:45:03 [INFO]   Run 1/5\n",
      "17:45:08 [INFO]     3,935 labeled -> Accuracy: 0.5061 (Train: 0.5s, Query: 0.08s)\n",
      "17:45:08 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5061, F1: 0.2174\n",
      "17:45:08 [INFO]   Run 2/5\n",
      "17:45:12 [INFO]     3,935 labeled -> Accuracy: 0.5177 (Train: 0.5s, Query: 0.07s)\n",
      "17:45:13 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5146, F1: 0.2226\n",
      "17:45:13 [INFO]   Run 3/5\n",
      "17:45:18 [INFO]     3,935 labeled -> Accuracy: 0.5098 (Train: 0.5s, Query: 0.06s)\n",
      "17:45:18 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5055, F1: 0.2240\n",
      "17:45:18 [INFO]   Run 4/5\n",
      "17:45:23 [INFO]     3,935 labeled -> Accuracy: 0.5274 (Train: 0.5s, Query: 0.07s)\n",
      "17:45:23 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5268, F1: 0.2405\n",
      "17:45:23 [INFO]   Run 5/5\n",
      "17:45:27 [INFO]     3,935 labeled -> Accuracy: 0.5122 (Train: 0.5s, Query: 0.07s)\n",
      "17:45:28 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5104, F1: 0.2202\n",
      "17:45:28 [INFO] \n",
      "Random Forest + Margin Sampling - Budget: 80% (5,247 Samples)\n",
      "17:45:28 [INFO]   Run 1/5\n",
      "17:45:34 [INFO]     5,247 labeled -> Accuracy: 0.5000 (Train: 0.6s, Query: 0.06s)\n",
      "17:45:35 [INFO]     Final: 5,247 labeled -> Accuracy: 0.4970, F1: 0.2371\n",
      "17:45:35 [INFO]   Run 2/5\n",
      "17:45:41 [INFO]     5,247 labeled -> Accuracy: 0.5024 (Train: 0.6s, Query: 0.11s)\n",
      "17:45:42 [INFO]     Final: 5,247 labeled -> Accuracy: 0.4970, F1: 0.2149\n",
      "17:45:42 [INFO]   Run 3/5\n",
      "17:45:48 [INFO]     5,247 labeled -> Accuracy: 0.4915 (Train: 0.6s, Query: 0.06s)\n",
      "17:45:49 [INFO]     Final: 5,247 labeled -> Accuracy: 0.4951, F1: 0.2234\n",
      "17:45:49 [INFO]   Run 4/5\n",
      "17:45:56 [INFO]     5,247 labeled -> Accuracy: 0.4896 (Train: 0.6s, Query: 0.07s)\n",
      "17:45:57 [INFO]     Final: 5,247 labeled -> Accuracy: 0.4915, F1: 0.2257\n",
      "17:45:57 [INFO]   Run 5/5\n",
      "17:46:04 [INFO]     5,247 labeled -> Accuracy: 0.5018 (Train: 0.6s, Query: 0.07s)\n",
      "17:46:04 [INFO]     Final: 5,247 labeled -> Accuracy: 0.4988, F1: 0.2240\n",
      "17:46:04 [INFO] \n",
      "Random Forest + Margin Sampling - Budget: 100% (6,559 Samples)\n",
      "17:46:04 [INFO]   Run 1/5\n",
      "17:46:13 [INFO]     6,559 labeled -> Accuracy: 0.4823 (Train: 0.8s, Query: 0.06s)\n",
      "17:46:14 [INFO]     Final: 6,559 labeled -> Accuracy: 0.4732, F1: 0.2363\n",
      "17:46:14 [INFO]   Run 2/5\n",
      "17:46:22 [INFO]     6,559 labeled -> Accuracy: 0.4744 (Train: 0.6s, Query: 0.06s)\n",
      "17:46:23 [INFO]     Final: 6,559 labeled -> Accuracy: 0.4707, F1: 0.2304\n",
      "17:46:23 [INFO]   Run 3/5\n",
      "17:46:31 [INFO]     6,559 labeled -> Accuracy: 0.4732 (Train: 0.6s, Query: 0.04s)\n",
      "17:46:32 [INFO]     Final: 6,559 labeled -> Accuracy: 0.4726, F1: 0.2309\n",
      "17:46:32 [INFO]   Run 4/5\n",
      "17:46:41 [INFO]     6,559 labeled -> Accuracy: 0.4817 (Train: 0.6s, Query: 0.06s)\n",
      "17:46:41 [INFO]     Final: 6,559 labeled -> Accuracy: 0.4707, F1: 0.2273\n",
      "17:46:41 [INFO]   Run 5/5\n",
      "17:46:50 [INFO]     6,559 labeled -> Accuracy: 0.4732 (Train: 0.6s, Query: 0.09s)\n",
      "17:46:51 [INFO]     Final: 6,559 labeled -> Accuracy: 0.4695, F1: 0.2262\n",
      "\n",
      "[ok] Random Forest + Margin Sampling abgeschlossen in 2.2 Minuten\n",
      "\n",
      "============================================================\n",
      "Experiment 12/20: Random Forest + Least Confidence\n",
      "============================================================\n",
      "17:46:51 [INFO] \n",
      "Random Forest + Least Confidence - Budget: 20% (1,311 Samples)\n",
      "17:46:51 [INFO]   Run 1/5\n",
      "17:46:52 [INFO]     1,311 labeled -> Accuracy: 0.4854 (Train: 0.4s, Query: 0.07s)\n",
      "17:46:53 [INFO]     Final: 1,311 labeled -> Accuracy: 0.4927, F1: 0.2380\n",
      "17:46:53 [INFO]   Run 2/5\n",
      "17:46:54 [INFO]     1,311 labeled -> Accuracy: 0.4902 (Train: 0.4s, Query: 0.08s)\n",
      "17:46:55 [INFO]     Final: 1,311 labeled -> Accuracy: 0.5000, F1: 0.2266\n",
      "17:46:55 [INFO]   Run 3/5\n",
      "17:46:56 [INFO]     1,311 labeled -> Accuracy: 0.4848 (Train: 0.4s, Query: 0.07s)\n",
      "17:46:57 [INFO]     Final: 1,311 labeled -> Accuracy: 0.4787, F1: 0.2212\n",
      "17:46:57 [INFO]   Run 4/5\n",
      "17:46:58 [INFO]     1,311 labeled -> Accuracy: 0.4902 (Train: 0.4s, Query: 0.07s)\n",
      "17:46:59 [INFO]     Final: 1,311 labeled -> Accuracy: 0.5024, F1: 0.2015\n",
      "17:46:59 [INFO]   Run 5/5\n",
      "17:47:00 [INFO]     1,311 labeled -> Accuracy: 0.4732 (Train: 0.4s, Query: 0.08s)\n",
      "17:47:01 [INFO]     Final: 1,311 labeled -> Accuracy: 0.4848, F1: 0.2137\n",
      "17:47:01 [INFO] \n",
      "Random Forest + Least Confidence - Budget: 40% (2,623 Samples)\n",
      "17:47:01 [INFO]   Run 1/5\n",
      "17:47:04 [INFO]     2,623 labeled -> Accuracy: 0.5177 (Train: 0.4s, Query: 0.06s)\n",
      "17:47:05 [INFO]     Final: 2,623 labeled -> Accuracy: 0.5183, F1: 0.2582\n",
      "17:47:05 [INFO]   Run 2/5\n",
      "17:47:08 [INFO]     2,623 labeled -> Accuracy: 0.5177 (Train: 0.5s, Query: 0.08s)\n",
      "17:47:09 [INFO]     Final: 2,623 labeled -> Accuracy: 0.5110, F1: 0.2108\n",
      "17:47:09 [INFO]   Run 3/5\n",
      "17:47:12 [INFO]     2,623 labeled -> Accuracy: 0.4963 (Train: 0.4s, Query: 0.07s)\n",
      "17:47:13 [INFO]     Final: 2,623 labeled -> Accuracy: 0.5018, F1: 0.2402\n",
      "17:47:13 [INFO]   Run 4/5\n",
      "17:47:16 [INFO]     2,623 labeled -> Accuracy: 0.5104 (Train: 0.5s, Query: 0.08s)\n",
      "17:47:17 [INFO]     Final: 2,623 labeled -> Accuracy: 0.5110, F1: 0.2183\n",
      "17:47:17 [INFO]   Run 5/5\n",
      "17:47:20 [INFO]     2,623 labeled -> Accuracy: 0.4957 (Train: 0.5s, Query: 0.09s)\n",
      "17:47:21 [INFO]     Final: 2,623 labeled -> Accuracy: 0.4994, F1: 0.2267\n",
      "17:47:21 [INFO] \n",
      "Random Forest + Least Confidence - Budget: 60% (3,935 Samples)\n",
      "17:47:21 [INFO]   Run 1/5\n",
      "17:47:25 [INFO]     3,935 labeled -> Accuracy: 0.5171 (Train: 0.5s, Query: 0.06s)\n",
      "17:47:26 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5128, F1: 0.2309\n",
      "17:47:26 [INFO]   Run 2/5\n",
      "17:47:30 [INFO]     3,935 labeled -> Accuracy: 0.5220 (Train: 0.5s, Query: 0.08s)\n",
      "17:47:31 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5226, F1: 0.2278\n",
      "17:47:31 [INFO]   Run 3/5\n",
      "17:47:36 [INFO]     3,935 labeled -> Accuracy: 0.5146 (Train: 0.6s, Query: 0.07s)\n",
      "17:47:37 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5091, F1: 0.2265\n",
      "17:47:37 [INFO]   Run 4/5\n",
      "17:47:42 [INFO]     3,935 labeled -> Accuracy: 0.5189 (Train: 0.6s, Query: 0.07s)\n",
      "17:47:42 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5165, F1: 0.2203\n",
      "17:47:42 [INFO]   Run 5/5\n",
      "17:47:47 [INFO]     3,935 labeled -> Accuracy: 0.5091 (Train: 0.5s, Query: 0.07s)\n",
      "17:47:48 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5067, F1: 0.2251\n",
      "17:47:48 [INFO] \n",
      "Random Forest + Least Confidence - Budget: 80% (5,247 Samples)\n",
      "17:47:48 [INFO]   Run 1/5\n",
      "17:47:55 [INFO]     5,247 labeled -> Accuracy: 0.4957 (Train: 0.7s, Query: 0.08s)\n",
      "17:47:55 [INFO]     Final: 5,247 labeled -> Accuracy: 0.4970, F1: 0.2358\n",
      "17:47:55 [INFO]   Run 2/5\n",
      "17:48:03 [INFO]     5,247 labeled -> Accuracy: 0.5079 (Train: 0.6s, Query: 0.07s)\n",
      "17:48:03 [INFO]     Final: 5,247 labeled -> Accuracy: 0.5030, F1: 0.2213\n",
      "17:48:03 [INFO]   Run 3/5\n",
      "17:48:10 [INFO]     5,247 labeled -> Accuracy: 0.4939 (Train: 0.6s, Query: 0.08s)\n",
      "17:48:11 [INFO]     Final: 5,247 labeled -> Accuracy: 0.4945, F1: 0.2093\n",
      "17:48:11 [INFO]   Run 4/5\n",
      "17:48:17 [INFO]     5,247 labeled -> Accuracy: 0.5024 (Train: 0.5s, Query: 0.05s)\n",
      "17:48:18 [INFO]     Final: 5,247 labeled -> Accuracy: 0.5043, F1: 0.2284\n",
      "17:48:18 [INFO]   Run 5/5\n",
      "17:48:25 [INFO]     5,247 labeled -> Accuracy: 0.4976 (Train: 0.6s, Query: 0.08s)\n",
      "17:48:26 [INFO]     Final: 5,247 labeled -> Accuracy: 0.4939, F1: 0.2362\n",
      "17:48:26 [INFO] \n",
      "Random Forest + Least Confidence - Budget: 100% (6,559 Samples)\n",
      "17:48:26 [INFO]   Run 1/5\n",
      "17:48:33 [INFO]     6,559 labeled -> Accuracy: 0.4780 (Train: 0.6s, Query: 0.04s)\n",
      "17:48:34 [INFO]     Final: 6,559 labeled -> Accuracy: 0.4713, F1: 0.2300\n",
      "17:48:34 [INFO]   Run 2/5\n",
      "17:48:42 [INFO]     6,559 labeled -> Accuracy: 0.4799 (Train: 0.7s, Query: 0.06s)\n",
      "17:48:43 [INFO]     Final: 6,559 labeled -> Accuracy: 0.4720, F1: 0.2319\n",
      "17:48:43 [INFO]   Run 3/5\n",
      "17:48:52 [INFO]     6,559 labeled -> Accuracy: 0.4756 (Train: 0.6s, Query: 0.08s)\n",
      "17:48:53 [INFO]     Final: 6,559 labeled -> Accuracy: 0.4720, F1: 0.2306\n",
      "17:48:53 [INFO]   Run 4/5\n",
      "17:49:01 [INFO]     6,559 labeled -> Accuracy: 0.4768 (Train: 0.7s, Query: 0.07s)\n",
      "17:49:02 [INFO]     Final: 6,559 labeled -> Accuracy: 0.4726, F1: 0.2281\n",
      "17:49:02 [INFO]   Run 5/5\n",
      "17:49:10 [INFO]     6,559 labeled -> Accuracy: 0.4787 (Train: 0.6s, Query: 0.05s)\n",
      "17:49:10 [INFO]     Final: 6,559 labeled -> Accuracy: 0.4707, F1: 0.2301\n",
      "\n",
      "[ok] Random Forest + Least Confidence abgeschlossen in 2.3 Minuten\n",
      "\n",
      "============================================================\n",
      "Experiment 13/20: Logistic Regression + Random Sampling\n",
      "============================================================\n",
      "17:49:10 [INFO] \n",
      "Logistic Regression + Random Sampling - Budget: 20% (1,311 Samples)\n",
      "17:49:10 [INFO]   Run 1/5\n",
      "17:49:14 [INFO]     1,311 labeled -> Accuracy: 0.5299 (Train: 2.1s, Query: 0.00s)\n",
      "17:49:16 [INFO]     Final: 1,311 labeled -> Accuracy: 0.5354, F1: 0.1839\n",
      "17:49:16 [INFO]   Run 2/5\n",
      "17:49:19 [INFO]     1,311 labeled -> Accuracy: 0.5433 (Train: 1.6s, Query: 0.00s)\n",
      "17:49:22 [INFO]     Final: 1,311 labeled -> Accuracy: 0.5439, F1: 0.1784\n",
      "17:49:22 [INFO]   Run 3/5\n",
      "17:49:25 [INFO]     1,311 labeled -> Accuracy: 0.5396 (Train: 2.1s, Query: 0.00s)\n",
      "17:49:27 [INFO]     Final: 1,311 labeled -> Accuracy: 0.5488, F1: 0.1879\n",
      "17:49:27 [INFO]   Run 4/5\n",
      "17:49:30 [INFO]     1,311 labeled -> Accuracy: 0.5439 (Train: 2.0s, Query: 0.00s)\n",
      "17:49:33 [INFO]     Final: 1,311 labeled -> Accuracy: 0.5543, F1: 0.1845\n",
      "17:49:33 [INFO]   Run 5/5\n",
      "17:49:36 [INFO]     1,311 labeled -> Accuracy: 0.5341 (Train: 2.2s, Query: 0.00s)\n",
      "17:49:39 [INFO]     Final: 1,311 labeled -> Accuracy: 0.5354, F1: 0.1736\n",
      "17:49:39 [INFO] \n",
      "Logistic Regression + Random Sampling - Budget: 40% (2,623 Samples)\n",
      "17:49:39 [INFO]   Run 1/5\n",
      "17:49:55 [INFO]     2,623 labeled -> Accuracy: 0.5317 (Train: 5.3s, Query: 0.00s)\n",
      "17:50:00 [INFO]     Final: 2,623 labeled -> Accuracy: 0.5311, F1: 0.1789\n",
      "17:50:00 [INFO]   Run 2/5\n",
      "17:50:15 [INFO]     2,623 labeled -> Accuracy: 0.5512 (Train: 4.9s, Query: 0.00s)\n",
      "17:50:20 [INFO]     Final: 2,623 labeled -> Accuracy: 0.5530, F1: 0.1956\n",
      "17:50:20 [INFO]   Run 3/5\n",
      "17:50:35 [INFO]     2,623 labeled -> Accuracy: 0.5518 (Train: 4.9s, Query: 0.00s)\n",
      "17:50:40 [INFO]     Final: 2,623 labeled -> Accuracy: 0.5518, F1: 0.1848\n",
      "17:50:40 [INFO]   Run 4/5\n",
      "17:50:55 [INFO]     2,623 labeled -> Accuracy: 0.5524 (Train: 5.1s, Query: 0.00s)\n",
      "17:51:00 [INFO]     Final: 2,623 labeled -> Accuracy: 0.5518, F1: 0.1856\n",
      "17:51:00 [INFO]   Run 5/5\n",
      "17:51:16 [INFO]     2,623 labeled -> Accuracy: 0.5451 (Train: 5.0s, Query: 0.00s)\n",
      "17:51:21 [INFO]     Final: 2,623 labeled -> Accuracy: 0.5445, F1: 0.1848\n",
      "17:51:21 [INFO] \n",
      "Logistic Regression + Random Sampling - Budget: 60% (3,935 Samples)\n",
      "17:51:21 [INFO]   Run 1/5\n",
      "17:51:50 [INFO]     3,935 labeled -> Accuracy: 0.5402 (Train: 7.2s, Query: 0.00s)\n",
      "17:51:58 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5372, F1: 0.1835\n",
      "17:51:58 [INFO]   Run 2/5\n",
      "17:52:26 [INFO]     3,935 labeled -> Accuracy: 0.5421 (Train: 6.7s, Query: 0.00s)\n",
      "17:52:34 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5402, F1: 0.1930\n",
      "17:52:34 [INFO]   Run 3/5\n",
      "17:53:01 [INFO]     3,935 labeled -> Accuracy: 0.5488 (Train: 7.1s, Query: 0.00s)\n",
      "17:53:09 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5445, F1: 0.1831\n",
      "17:53:09 [INFO]   Run 4/5\n",
      "17:53:38 [INFO]     3,935 labeled -> Accuracy: 0.5585 (Train: 7.1s, Query: 0.00s)\n",
      "17:53:45 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5591, F1: 0.1925\n",
      "17:53:45 [INFO]   Run 5/5\n",
      "17:54:14 [INFO]     3,935 labeled -> Accuracy: 0.5476 (Train: 7.2s, Query: 0.00s)\n",
      "17:54:22 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5482, F1: 0.1882\n",
      "17:54:22 [INFO] \n",
      "Logistic Regression + Random Sampling - Budget: 80% (5,247 Samples)\n",
      "17:54:22 [INFO]   Run 1/5\n",
      "17:55:21 [INFO]     5,247 labeled -> Accuracy: 0.5396 (Train: 10.1s, Query: 0.00s)\n",
      "17:55:32 [INFO]     Final: 5,247 labeled -> Accuracy: 0.5348, F1: 0.1839\n",
      "17:55:32 [INFO]   Run 2/5\n",
      "17:56:32 [INFO]     5,247 labeled -> Accuracy: 0.5329 (Train: 10.6s, Query: 0.00s)\n",
      "17:56:43 [INFO]     Final: 5,247 labeled -> Accuracy: 0.5372, F1: 0.1890\n",
      "17:56:43 [INFO]   Run 3/5\n",
      "17:57:42 [INFO]     5,247 labeled -> Accuracy: 0.5482 (Train: 11.1s, Query: 0.00s)\n",
      "17:57:53 [INFO]     Final: 5,247 labeled -> Accuracy: 0.5470, F1: 0.1858\n",
      "17:57:53 [INFO]   Run 4/5\n",
      "17:58:53 [INFO]     5,247 labeled -> Accuracy: 0.5470 (Train: 11.2s, Query: 0.00s)\n",
      "17:59:05 [INFO]     Final: 5,247 labeled -> Accuracy: 0.5500, F1: 0.1931\n",
      "17:59:05 [INFO]   Run 5/5\n",
      "18:00:05 [INFO]     5,247 labeled -> Accuracy: 0.5445 (Train: 11.6s, Query: 0.00s)\n",
      "18:00:17 [INFO]     Final: 5,247 labeled -> Accuracy: 0.5384, F1: 0.1862\n",
      "18:00:17 [INFO] \n",
      "Logistic Regression + Random Sampling - Budget: 100% (6,559 Samples)\n",
      "18:00:17 [INFO]   Run 1/5\n",
      "18:01:39 [INFO]     6,559 labeled -> Accuracy: 0.5372 (Train: 12.3s, Query: 0.00s)\n",
      "18:01:53 [INFO]     Final: 6,559 labeled -> Accuracy: 0.5396, F1: 0.1883\n",
      "18:01:53 [INFO]   Run 2/5\n",
      "18:03:12 [INFO]     6,559 labeled -> Accuracy: 0.5396 (Train: 12.6s, Query: 0.00s)\n",
      "18:03:26 [INFO]     Final: 6,559 labeled -> Accuracy: 0.5396, F1: 0.1883\n",
      "18:03:26 [INFO]   Run 3/5\n",
      "18:04:43 [INFO]     6,559 labeled -> Accuracy: 0.5445 (Train: 11.6s, Query: 0.00s)\n",
      "18:04:57 [INFO]     Final: 6,559 labeled -> Accuracy: 0.5396, F1: 0.1883\n",
      "18:04:57 [INFO]   Run 4/5\n",
      "18:06:18 [INFO]     6,559 labeled -> Accuracy: 0.5360 (Train: 12.2s, Query: 0.00s)\n",
      "18:06:31 [INFO]     Final: 6,559 labeled -> Accuracy: 0.5396, F1: 0.1883\n",
      "18:06:31 [INFO]   Run 5/5\n",
      "18:07:52 [INFO]     6,559 labeled -> Accuracy: 0.5421 (Train: 11.9s, Query: 0.00s)\n",
      "18:08:05 [INFO]     Final: 6,559 labeled -> Accuracy: 0.5396, F1: 0.1883\n",
      "\n",
      "[ok] Logistic Regression + Random Sampling abgeschlossen in 18.9 Minuten\n",
      "\n",
      "============================================================\n",
      "Experiment 14/20: Logistic Regression + Entropy Sampling\n",
      "============================================================\n",
      "18:08:05 [INFO] \n",
      "Logistic Regression + Entropy Sampling - Budget: 20% (1,311 Samples)\n",
      "18:08:05 [INFO]   Run 1/5\n",
      "18:08:09 [INFO]     1,311 labeled -> Accuracy: 0.4848 (Train: 2.0s, Query: 0.01s)\n",
      "18:08:11 [INFO]     Final: 1,311 labeled -> Accuracy: 0.4817, F1: 0.2015\n",
      "18:08:11 [INFO]   Run 2/5\n",
      "18:08:14 [INFO]     1,311 labeled -> Accuracy: 0.4555 (Train: 1.9s, Query: 0.00s)\n",
      "18:08:16 [INFO]     Final: 1,311 labeled -> Accuracy: 0.4689, F1: 0.1633\n",
      "18:08:16 [INFO]   Run 3/5\n",
      "18:08:19 [INFO]     1,311 labeled -> Accuracy: 0.5183 (Train: 1.8s, Query: 0.00s)\n",
      "18:08:22 [INFO]     Final: 1,311 labeled -> Accuracy: 0.5030, F1: 0.1607\n",
      "18:08:22 [INFO]   Run 4/5\n",
      "18:08:25 [INFO]     1,311 labeled -> Accuracy: 0.4957 (Train: 1.9s, Query: 0.00s)\n",
      "18:08:27 [INFO]     Final: 1,311 labeled -> Accuracy: 0.5000, F1: 0.1931\n",
      "18:08:27 [INFO]   Run 5/5\n",
      "18:08:30 [INFO]     1,311 labeled -> Accuracy: 0.5220 (Train: 2.1s, Query: 0.00s)\n",
      "18:08:33 [INFO]     Final: 1,311 labeled -> Accuracy: 0.5220, F1: 0.1773\n",
      "18:08:33 [INFO] \n",
      "Logistic Regression + Entropy Sampling - Budget: 40% (2,623 Samples)\n",
      "18:08:33 [INFO]   Run 1/5\n",
      "18:08:48 [INFO]     2,623 labeled -> Accuracy: 0.4793 (Train: 4.9s, Query: 0.00s)\n",
      "18:08:53 [INFO]     Final: 2,623 labeled -> Accuracy: 0.4927, F1: 0.1967\n",
      "18:08:53 [INFO]   Run 2/5\n",
      "18:09:08 [INFO]     2,623 labeled -> Accuracy: 0.5165 (Train: 5.2s, Query: 0.01s)\n",
      "18:09:13 [INFO]     Final: 2,623 labeled -> Accuracy: 0.5165, F1: 0.1758\n",
      "18:09:13 [INFO]   Run 3/5\n",
      "18:09:29 [INFO]     2,623 labeled -> Accuracy: 0.5457 (Train: 5.3s, Query: 0.00s)\n",
      "18:09:34 [INFO]     Final: 2,623 labeled -> Accuracy: 0.5463, F1: 0.1836\n",
      "18:09:34 [INFO]   Run 4/5\n",
      "18:09:50 [INFO]     2,623 labeled -> Accuracy: 0.5421 (Train: 5.3s, Query: 0.01s)\n",
      "18:09:55 [INFO]     Final: 2,623 labeled -> Accuracy: 0.5439, F1: 0.1746\n",
      "18:09:55 [INFO]   Run 5/5\n",
      "18:10:11 [INFO]     2,623 labeled -> Accuracy: 0.5311 (Train: 5.3s, Query: 0.00s)\n",
      "18:10:16 [INFO]     Final: 2,623 labeled -> Accuracy: 0.5305, F1: 0.1904\n",
      "18:10:16 [INFO] \n",
      "Logistic Regression + Entropy Sampling - Budget: 60% (3,935 Samples)\n",
      "18:10:16 [INFO]   Run 1/5\n",
      "18:10:43 [INFO]     3,935 labeled -> Accuracy: 0.5250 (Train: 6.9s, Query: 0.00s)\n",
      "18:10:51 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5341, F1: 0.1781\n",
      "18:10:51 [INFO]   Run 2/5\n",
      "18:11:19 [INFO]     3,935 labeled -> Accuracy: 0.5360 (Train: 6.6s, Query: 0.01s)\n",
      "18:11:26 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5329, F1: 0.1757\n",
      "18:11:26 [INFO]   Run 3/5\n",
      "18:11:53 [INFO]     3,935 labeled -> Accuracy: 0.5402 (Train: 6.7s, Query: 0.00s)\n",
      "18:12:00 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5433, F1: 0.1871\n",
      "18:12:00 [INFO]   Run 4/5\n",
      "18:12:27 [INFO]     3,935 labeled -> Accuracy: 0.5445 (Train: 6.6s, Query: 0.00s)\n",
      "18:12:35 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5421, F1: 0.1739\n",
      "18:12:35 [INFO]   Run 5/5\n",
      "18:13:01 [INFO]     3,935 labeled -> Accuracy: 0.5415 (Train: 7.2s, Query: 0.01s)\n",
      "18:13:09 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5427, F1: 0.1912\n",
      "18:13:09 [INFO] \n",
      "Logistic Regression + Entropy Sampling - Budget: 80% (5,247 Samples)\n",
      "18:13:09 [INFO]   Run 1/5\n",
      "18:14:03 [INFO]     5,247 labeled -> Accuracy: 0.5305 (Train: 9.4s, Query: 0.00s)\n",
      "18:14:13 [INFO]     Final: 5,247 labeled -> Accuracy: 0.5317, F1: 0.1983\n",
      "18:14:13 [INFO]   Run 2/5\n",
      "18:15:06 [INFO]     5,247 labeled -> Accuracy: 0.5494 (Train: 10.4s, Query: 0.00s)\n",
      "18:15:17 [INFO]     Final: 5,247 labeled -> Accuracy: 0.5463, F1: 0.1971\n",
      "18:15:17 [INFO]   Run 3/5\n",
      "18:16:09 [INFO]     5,247 labeled -> Accuracy: 0.5476 (Train: 10.0s, Query: 0.00s)\n",
      "18:16:19 [INFO]     Final: 5,247 labeled -> Accuracy: 0.5482, F1: 0.1866\n",
      "18:16:19 [INFO]   Run 4/5\n",
      "18:17:12 [INFO]     5,247 labeled -> Accuracy: 0.5494 (Train: 10.7s, Query: 0.00s)\n",
      "18:17:22 [INFO]     Final: 5,247 labeled -> Accuracy: 0.5433, F1: 0.1878\n",
      "18:17:22 [INFO]   Run 5/5\n",
      "18:18:16 [INFO]     5,247 labeled -> Accuracy: 0.5524 (Train: 9.9s, Query: 0.00s)\n",
      "18:18:27 [INFO]     Final: 5,247 labeled -> Accuracy: 0.5506, F1: 0.2025\n",
      "18:18:27 [INFO] \n",
      "Logistic Regression + Entropy Sampling - Budget: 100% (6,559 Samples)\n",
      "18:18:27 [INFO]   Run 1/5\n",
      "18:19:43 [INFO]     6,559 labeled -> Accuracy: 0.5268 (Train: 11.7s, Query: 0.00s)\n",
      "18:19:56 [INFO]     Final: 6,559 labeled -> Accuracy: 0.5396, F1: 0.1883\n",
      "18:19:56 [INFO]   Run 2/5\n",
      "18:21:14 [INFO]     6,559 labeled -> Accuracy: 0.5463 (Train: 11.9s, Query: 0.00s)\n",
      "18:21:26 [INFO]     Final: 6,559 labeled -> Accuracy: 0.5396, F1: 0.1883\n",
      "18:21:26 [INFO]   Run 3/5\n",
      "18:22:43 [INFO]     6,559 labeled -> Accuracy: 0.5354 (Train: 11.6s, Query: 0.00s)\n",
      "18:22:56 [INFO]     Final: 6,559 labeled -> Accuracy: 0.5396, F1: 0.1883\n",
      "18:22:56 [INFO]   Run 4/5\n",
      "18:24:13 [INFO]     6,559 labeled -> Accuracy: 0.5396 (Train: 12.3s, Query: 0.00s)\n",
      "18:24:27 [INFO]     Final: 6,559 labeled -> Accuracy: 0.5396, F1: 0.1883\n",
      "18:24:27 [INFO]   Run 5/5\n",
      "18:25:46 [INFO]     6,559 labeled -> Accuracy: 0.5451 (Train: 12.6s, Query: 0.00s)\n",
      "18:26:00 [INFO]     Final: 6,559 labeled -> Accuracy: 0.5396, F1: 0.1883\n",
      "\n",
      "[ok] Logistic Regression + Entropy Sampling abgeschlossen in 17.9 Minuten\n",
      "\n",
      "============================================================\n",
      "Experiment 15/20: Logistic Regression + Margin Sampling\n",
      "============================================================\n",
      "18:26:00 [INFO] \n",
      "Logistic Regression + Margin Sampling - Budget: 20% (1,311 Samples)\n",
      "18:26:00 [INFO]   Run 1/5\n",
      "18:26:04 [INFO]     1,311 labeled -> Accuracy: 0.5073 (Train: 2.1s, Query: 0.01s)\n",
      "18:26:06 [INFO]     Final: 1,311 labeled -> Accuracy: 0.5030, F1: 0.1518\n",
      "18:26:06 [INFO]   Run 2/5\n",
      "18:26:09 [INFO]     1,311 labeled -> Accuracy: 0.5134 (Train: 2.2s, Query: 0.00s)\n",
      "18:26:12 [INFO]     Final: 1,311 labeled -> Accuracy: 0.5280, F1: 0.1724\n",
      "18:26:12 [INFO]   Run 3/5\n",
      "18:26:14 [INFO]     1,311 labeled -> Accuracy: 0.5165 (Train: 2.0s, Query: 0.00s)\n",
      "18:26:17 [INFO]     Final: 1,311 labeled -> Accuracy: 0.5250, F1: 0.1791\n",
      "18:26:17 [INFO]   Run 4/5\n",
      "18:26:20 [INFO]     1,311 labeled -> Accuracy: 0.5262 (Train: 2.1s, Query: 0.00s)\n",
      "18:26:22 [INFO]     Final: 1,311 labeled -> Accuracy: 0.5335, F1: 0.1926\n",
      "18:26:22 [INFO]   Run 5/5\n",
      "18:26:25 [INFO]     1,311 labeled -> Accuracy: 0.5311 (Train: 2.0s, Query: 0.01s)\n",
      "18:26:28 [INFO]     Final: 1,311 labeled -> Accuracy: 0.5329, F1: 0.1813\n",
      "18:26:28 [INFO] \n",
      "Logistic Regression + Margin Sampling - Budget: 40% (2,623 Samples)\n",
      "18:26:28 [INFO]   Run 1/5\n",
      "18:26:43 [INFO]     2,623 labeled -> Accuracy: 0.5317 (Train: 5.0s, Query: 0.00s)\n",
      "18:26:48 [INFO]     Final: 2,623 labeled -> Accuracy: 0.5323, F1: 0.1870\n",
      "18:26:48 [INFO]   Run 2/5\n",
      "18:27:04 [INFO]     2,623 labeled -> Accuracy: 0.5348 (Train: 4.9s, Query: 0.00s)\n",
      "18:27:09 [INFO]     Final: 2,623 labeled -> Accuracy: 0.5354, F1: 0.1790\n",
      "18:27:09 [INFO]   Run 3/5\n",
      "18:27:23 [INFO]     2,623 labeled -> Accuracy: 0.5488 (Train: 4.7s, Query: 0.00s)\n",
      "18:27:28 [INFO]     Final: 2,623 labeled -> Accuracy: 0.5463, F1: 0.1912\n",
      "18:27:28 [INFO]   Run 4/5\n",
      "18:27:43 [INFO]     2,623 labeled -> Accuracy: 0.5549 (Train: 5.1s, Query: 0.01s)\n",
      "18:27:49 [INFO]     Final: 2,623 labeled -> Accuracy: 0.5555, F1: 0.1929\n",
      "18:27:49 [INFO]   Run 5/5\n",
      "18:28:04 [INFO]     2,623 labeled -> Accuracy: 0.5409 (Train: 5.1s, Query: 0.01s)\n",
      "18:28:10 [INFO]     Final: 2,623 labeled -> Accuracy: 0.5409, F1: 0.1900\n",
      "18:28:10 [INFO] \n",
      "Logistic Regression + Margin Sampling - Budget: 60% (3,935 Samples)\n",
      "18:28:10 [INFO]   Run 1/5\n",
      "18:28:40 [INFO]     3,935 labeled -> Accuracy: 0.5244 (Train: 7.4s, Query: 0.01s)\n",
      "18:28:48 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5232, F1: 0.1871\n",
      "18:28:48 [INFO]   Run 2/5\n",
      "18:29:18 [INFO]     3,935 labeled -> Accuracy: 0.5238 (Train: 7.4s, Query: 0.00s)\n",
      "18:29:26 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5250, F1: 0.1782\n",
      "18:29:26 [INFO]   Run 3/5\n",
      "18:29:56 [INFO]     3,935 labeled -> Accuracy: 0.5439 (Train: 7.2s, Query: 0.01s)\n",
      "18:30:05 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5396, F1: 0.1844\n",
      "18:30:05 [INFO]   Run 4/5\n",
      "18:30:35 [INFO]     3,935 labeled -> Accuracy: 0.5494 (Train: 7.7s, Query: 0.01s)\n",
      "18:30:43 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5427, F1: 0.1848\n",
      "18:30:43 [INFO]   Run 5/5\n",
      "18:31:12 [INFO]     3,935 labeled -> Accuracy: 0.5366 (Train: 7.3s, Query: 0.00s)\n",
      "18:31:19 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5348, F1: 0.1835\n",
      "18:31:19 [INFO] \n",
      "Logistic Regression + Margin Sampling - Budget: 80% (5,247 Samples)\n",
      "18:31:19 [INFO]   Run 1/5\n",
      "18:32:14 [INFO]     5,247 labeled -> Accuracy: 0.5262 (Train: 10.7s, Query: 0.01s)\n",
      "18:32:25 [INFO]     Final: 5,247 labeled -> Accuracy: 0.5287, F1: 0.1894\n",
      "18:32:25 [INFO]   Run 2/5\n",
      "18:33:20 [INFO]     5,247 labeled -> Accuracy: 0.5323 (Train: 9.9s, Query: 0.00s)\n",
      "18:33:30 [INFO]     Final: 5,247 labeled -> Accuracy: 0.5378, F1: 0.1879\n",
      "18:33:30 [INFO]   Run 3/5\n",
      "18:34:23 [INFO]     5,247 labeled -> Accuracy: 0.5524 (Train: 9.5s, Query: 0.00s)\n",
      "18:34:33 [INFO]     Final: 5,247 labeled -> Accuracy: 0.5518, F1: 0.1959\n",
      "18:34:33 [INFO]   Run 4/5\n",
      "18:35:28 [INFO]     5,247 labeled -> Accuracy: 0.5451 (Train: 9.9s, Query: 0.00s)\n",
      "18:35:38 [INFO]     Final: 5,247 labeled -> Accuracy: 0.5451, F1: 0.1824\n",
      "18:35:38 [INFO]   Run 5/5\n",
      "18:36:31 [INFO]     5,247 labeled -> Accuracy: 0.5451 (Train: 10.2s, Query: 0.00s)\n",
      "18:36:42 [INFO]     Final: 5,247 labeled -> Accuracy: 0.5402, F1: 0.1867\n",
      "18:36:42 [INFO] \n",
      "Logistic Regression + Margin Sampling - Budget: 100% (6,559 Samples)\n",
      "18:36:42 [INFO]   Run 1/5\n",
      "18:37:56 [INFO]     6,559 labeled -> Accuracy: 0.5280 (Train: 11.7s, Query: 0.00s)\n",
      "18:38:09 [INFO]     Final: 6,559 labeled -> Accuracy: 0.5396, F1: 0.1883\n",
      "18:38:09 [INFO]   Run 2/5\n",
      "18:39:25 [INFO]     6,559 labeled -> Accuracy: 0.5232 (Train: 11.3s, Query: 0.00s)\n",
      "18:39:38 [INFO]     Final: 6,559 labeled -> Accuracy: 0.5396, F1: 0.1883\n",
      "18:39:38 [INFO]   Run 3/5\n",
      "18:40:55 [INFO]     6,559 labeled -> Accuracy: 0.5323 (Train: 13.0s, Query: 0.00s)\n",
      "18:41:09 [INFO]     Final: 6,559 labeled -> Accuracy: 0.5396, F1: 0.1883\n",
      "18:41:09 [INFO]   Run 4/5\n",
      "18:42:24 [INFO]     6,559 labeled -> Accuracy: 0.5390 (Train: 11.4s, Query: 0.00s)\n",
      "18:42:37 [INFO]     Final: 6,559 labeled -> Accuracy: 0.5396, F1: 0.1883\n",
      "18:42:37 [INFO]   Run 5/5\n",
      "18:43:52 [INFO]     6,559 labeled -> Accuracy: 0.5409 (Train: 12.4s, Query: 0.00s)\n",
      "18:44:05 [INFO]     Final: 6,559 labeled -> Accuracy: 0.5396, F1: 0.1883\n",
      "\n",
      "[ok] Logistic Regression + Margin Sampling abgeschlossen in 18.1 Minuten\n",
      "\n",
      "============================================================\n",
      "Experiment 16/20: Logistic Regression + Least Confidence\n",
      "============================================================\n",
      "18:44:05 [INFO] \n",
      "Logistic Regression + Least Confidence - Budget: 20% (1,311 Samples)\n",
      "18:44:05 [INFO]   Run 1/5\n",
      "18:44:09 [INFO]     1,311 labeled -> Accuracy: 0.4762 (Train: 2.2s, Query: 0.01s)\n",
      "18:44:11 [INFO]     Final: 1,311 labeled -> Accuracy: 0.4902, F1: 0.2067\n",
      "18:44:11 [INFO]   Run 2/5\n",
      "18:44:14 [INFO]     1,311 labeled -> Accuracy: 0.5177 (Train: 2.0s, Query: 0.00s)\n",
      "18:44:16 [INFO]     Final: 1,311 labeled -> Accuracy: 0.5341, F1: 0.1787\n",
      "18:44:16 [INFO]   Run 3/5\n",
      "18:44:19 [INFO]     1,311 labeled -> Accuracy: 0.5165 (Train: 1.9s, Query: 0.00s)\n",
      "18:44:22 [INFO]     Final: 1,311 labeled -> Accuracy: 0.5293, F1: 0.1695\n",
      "18:44:22 [INFO]   Run 4/5\n",
      "18:44:25 [INFO]     1,311 labeled -> Accuracy: 0.4860 (Train: 2.0s, Query: 0.00s)\n",
      "18:44:27 [INFO]     Final: 1,311 labeled -> Accuracy: 0.5177, F1: 0.1799\n",
      "18:44:27 [INFO]   Run 5/5\n",
      "18:44:29 [INFO]     1,311 labeled -> Accuracy: 0.5256 (Train: 1.7s, Query: 0.00s)\n",
      "18:44:32 [INFO]     Final: 1,311 labeled -> Accuracy: 0.5366, F1: 0.1878\n",
      "18:44:32 [INFO] \n",
      "Logistic Regression + Least Confidence - Budget: 40% (2,623 Samples)\n",
      "18:44:32 [INFO]   Run 1/5\n",
      "18:44:46 [INFO]     2,623 labeled -> Accuracy: 0.5091 (Train: 4.8s, Query: 0.00s)\n",
      "18:44:51 [INFO]     Final: 2,623 labeled -> Accuracy: 0.5128, F1: 0.1829\n",
      "18:44:51 [INFO]   Run 2/5\n",
      "18:45:06 [INFO]     2,623 labeled -> Accuracy: 0.5348 (Train: 4.8s, Query: 0.00s)\n",
      "18:45:11 [INFO]     Final: 2,623 labeled -> Accuracy: 0.5378, F1: 0.1871\n",
      "18:45:11 [INFO]   Run 3/5\n",
      "18:45:25 [INFO]     2,623 labeled -> Accuracy: 0.5445 (Train: 4.6s, Query: 0.00s)\n",
      "18:45:29 [INFO]     Final: 2,623 labeled -> Accuracy: 0.5470, F1: 0.1888\n",
      "18:45:29 [INFO]   Run 4/5\n",
      "18:45:43 [INFO]     2,623 labeled -> Accuracy: 0.5280 (Train: 4.7s, Query: 0.00s)\n",
      "18:45:48 [INFO]     Final: 2,623 labeled -> Accuracy: 0.5305, F1: 0.1776\n",
      "18:45:48 [INFO]   Run 5/5\n",
      "18:46:02 [INFO]     2,623 labeled -> Accuracy: 0.5457 (Train: 4.6s, Query: 0.00s)\n",
      "18:46:07 [INFO]     Final: 2,623 labeled -> Accuracy: 0.5439, F1: 0.1935\n",
      "18:46:07 [INFO] \n",
      "Logistic Regression + Least Confidence - Budget: 60% (3,935 Samples)\n",
      "18:46:07 [INFO]   Run 1/5\n",
      "18:46:34 [INFO]     3,935 labeled -> Accuracy: 0.5329 (Train: 6.9s, Query: 0.00s)\n",
      "18:46:41 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5317, F1: 0.1985\n",
      "18:46:41 [INFO]   Run 2/5\n",
      "18:47:08 [INFO]     3,935 labeled -> Accuracy: 0.5488 (Train: 7.0s, Query: 0.00s)\n",
      "18:47:15 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5488, F1: 0.1930\n",
      "18:47:15 [INFO]   Run 3/5\n",
      "18:47:41 [INFO]     3,935 labeled -> Accuracy: 0.5482 (Train: 6.4s, Query: 0.00s)\n",
      "18:47:48 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5439, F1: 0.1823\n",
      "18:47:48 [INFO]   Run 4/5\n",
      "18:48:14 [INFO]     3,935 labeled -> Accuracy: 0.5402 (Train: 6.8s, Query: 0.00s)\n",
      "18:48:22 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5451, F1: 0.1866\n",
      "18:48:22 [INFO]   Run 5/5\n",
      "18:48:48 [INFO]     3,935 labeled -> Accuracy: 0.5451 (Train: 6.4s, Query: 0.00s)\n",
      "18:48:55 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5402, F1: 0.1887\n",
      "18:48:55 [INFO] \n",
      "Logistic Regression + Least Confidence - Budget: 80% (5,247 Samples)\n",
      "18:48:55 [INFO]   Run 1/5\n",
      "18:49:47 [INFO]     5,247 labeled -> Accuracy: 0.5354 (Train: 8.8s, Query: 0.00s)\n",
      "18:49:57 [INFO]     Final: 5,247 labeled -> Accuracy: 0.5372, F1: 0.2060\n",
      "18:49:57 [INFO]   Run 2/5\n",
      "18:50:49 [INFO]     5,247 labeled -> Accuracy: 0.5445 (Train: 9.5s, Query: 0.00s)\n",
      "18:50:59 [INFO]     Final: 5,247 labeled -> Accuracy: 0.5427, F1: 0.1880\n",
      "18:50:59 [INFO]   Run 3/5\n",
      "18:51:49 [INFO]     5,247 labeled -> Accuracy: 0.5463 (Train: 9.4s, Query: 0.00s)\n",
      "18:51:58 [INFO]     Final: 5,247 labeled -> Accuracy: 0.5457, F1: 0.1874\n",
      "18:51:58 [INFO]   Run 4/5\n",
      "18:52:48 [INFO]     5,247 labeled -> Accuracy: 0.5555 (Train: 9.1s, Query: 0.00s)\n",
      "18:52:57 [INFO]     Final: 5,247 labeled -> Accuracy: 0.5476, F1: 0.1969\n",
      "18:52:57 [INFO]   Run 5/5\n",
      "18:53:49 [INFO]     5,247 labeled -> Accuracy: 0.5354 (Train: 9.9s, Query: 0.00s)\n",
      "18:53:58 [INFO]     Final: 5,247 labeled -> Accuracy: 0.5366, F1: 0.1846\n",
      "18:53:58 [INFO] \n",
      "Logistic Regression + Least Confidence - Budget: 100% (6,559 Samples)\n",
      "18:53:58 [INFO]   Run 1/5\n",
      "18:55:14 [INFO]     6,559 labeled -> Accuracy: 0.5317 (Train: 12.0s, Query: 0.00s)\n",
      "18:55:27 [INFO]     Final: 6,559 labeled -> Accuracy: 0.5396, F1: 0.1883\n",
      "18:55:27 [INFO]   Run 2/5\n",
      "18:56:42 [INFO]     6,559 labeled -> Accuracy: 0.5482 (Train: 12.4s, Query: 0.00s)\n",
      "18:56:55 [INFO]     Final: 6,559 labeled -> Accuracy: 0.5396, F1: 0.1883\n",
      "18:56:55 [INFO]   Run 3/5\n",
      "18:58:13 [INFO]     6,559 labeled -> Accuracy: 0.5366 (Train: 12.0s, Query: 0.00s)\n",
      "18:58:25 [INFO]     Final: 6,559 labeled -> Accuracy: 0.5396, F1: 0.1883\n",
      "18:58:25 [INFO]   Run 4/5\n",
      "18:59:41 [INFO]     6,559 labeled -> Accuracy: 0.5421 (Train: 12.1s, Query: 0.00s)\n",
      "18:59:54 [INFO]     Final: 6,559 labeled -> Accuracy: 0.5396, F1: 0.1883\n",
      "18:59:54 [INFO]   Run 5/5\n",
      "19:01:12 [INFO]     6,559 labeled -> Accuracy: 0.5341 (Train: 11.9s, Query: 0.00s)\n",
      "19:01:26 [INFO]     Final: 6,559 labeled -> Accuracy: 0.5396, F1: 0.1883\n",
      "\n",
      "[ok] Logistic Regression + Least Confidence abgeschlossen in 17.3 Minuten\n",
      "\n",
      "============================================================\n",
      "Experiment 17/20: SVM + Random Sampling\n",
      "============================================================\n",
      "19:01:26 [INFO] \n",
      "SVM + Random Sampling - Budget: 20% (1,311 Samples)\n",
      "19:01:26 [INFO]   Run 1/5\n",
      "19:01:27 [INFO]     1,311 labeled -> Accuracy: 0.5305 (Train: 0.6s, Query: 0.00s)\n",
      "19:01:28 [INFO]     Final: 1,311 labeled -> Accuracy: 0.5390, F1: 0.1683\n",
      "19:01:28 [INFO]   Run 2/5\n",
      "19:01:29 [INFO]     1,311 labeled -> Accuracy: 0.5341 (Train: 0.6s, Query: 0.00s)\n",
      "19:01:30 [INFO]     Final: 1,311 labeled -> Accuracy: 0.5335, F1: 0.1599\n",
      "19:01:30 [INFO]   Run 3/5\n",
      "19:01:31 [INFO]     1,311 labeled -> Accuracy: 0.5354 (Train: 0.6s, Query: 0.00s)\n",
      "19:01:32 [INFO]     Final: 1,311 labeled -> Accuracy: 0.5390, F1: 0.1648\n",
      "19:01:32 [INFO]   Run 4/5\n",
      "19:01:33 [INFO]     1,311 labeled -> Accuracy: 0.5256 (Train: 0.6s, Query: 0.00s)\n",
      "19:01:34 [INFO]     Final: 1,311 labeled -> Accuracy: 0.5226, F1: 0.1449\n",
      "19:01:34 [INFO]   Run 5/5\n",
      "19:01:35 [INFO]     1,311 labeled -> Accuracy: 0.5317 (Train: 0.5s, Query: 0.00s)\n",
      "19:01:36 [INFO]     Final: 1,311 labeled -> Accuracy: 0.5421, F1: 0.1767\n",
      "19:01:36 [INFO] \n",
      "SVM + Random Sampling - Budget: 40% (2,623 Samples)\n",
      "19:01:36 [INFO]   Run 1/5\n",
      "19:01:45 [INFO]     2,623 labeled -> Accuracy: 0.5482 (Train: 2.8s, Query: 0.00s)\n",
      "19:01:48 [INFO]     Final: 2,623 labeled -> Accuracy: 0.5476, F1: 0.1832\n",
      "19:01:48 [INFO]   Run 2/5\n",
      "19:01:56 [INFO]     2,623 labeled -> Accuracy: 0.5482 (Train: 2.8s, Query: 0.00s)\n",
      "19:01:59 [INFO]     Final: 2,623 labeled -> Accuracy: 0.5482, F1: 0.1814\n",
      "19:01:59 [INFO]   Run 3/5\n",
      "19:02:08 [INFO]     2,623 labeled -> Accuracy: 0.5488 (Train: 3.0s, Query: 0.00s)\n",
      "19:02:11 [INFO]     Final: 2,623 labeled -> Accuracy: 0.5488, F1: 0.1823\n",
      "19:02:11 [INFO]   Run 4/5\n",
      "19:02:19 [INFO]     2,623 labeled -> Accuracy: 0.5402 (Train: 2.8s, Query: 0.00s)\n",
      "19:02:23 [INFO]     Final: 2,623 labeled -> Accuracy: 0.5451, F1: 0.1796\n",
      "19:02:23 [INFO]   Run 5/5\n",
      "19:02:31 [INFO]     2,623 labeled -> Accuracy: 0.5451 (Train: 3.1s, Query: 0.00s)\n",
      "19:02:35 [INFO]     Final: 2,623 labeled -> Accuracy: 0.5451, F1: 0.1730\n",
      "19:02:35 [INFO] \n",
      "SVM + Random Sampling - Budget: 60% (3,935 Samples)\n",
      "19:02:35 [INFO]   Run 1/5\n",
      "19:02:55 [INFO]     3,935 labeled -> Accuracy: 0.5500 (Train: 5.7s, Query: 0.00s)\n",
      "19:03:02 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5494, F1: 0.1852\n",
      "19:03:02 [INFO]   Run 2/5\n",
      "19:03:23 [INFO]     3,935 labeled -> Accuracy: 0.5463 (Train: 5.9s, Query: 0.00s)\n",
      "19:03:31 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5518, F1: 0.1869\n",
      "19:03:31 [INFO]   Run 3/5\n",
      "19:03:51 [INFO]     3,935 labeled -> Accuracy: 0.5445 (Train: 5.8s, Query: 0.00s)\n",
      "19:03:58 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5445, F1: 0.1754\n",
      "19:03:58 [INFO]   Run 4/5\n",
      "19:04:17 [INFO]     3,935 labeled -> Accuracy: 0.5463 (Train: 5.6s, Query: 0.00s)\n",
      "19:04:25 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5488, F1: 0.1881\n",
      "19:04:25 [INFO]   Run 5/5\n",
      "19:04:44 [INFO]     3,935 labeled -> Accuracy: 0.5402 (Train: 5.5s, Query: 0.00s)\n",
      "19:04:51 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5470, F1: 0.1773\n",
      "19:04:51 [INFO] \n",
      "SVM + Random Sampling - Budget: 80% (5,247 Samples)\n",
      "19:04:51 [INFO]   Run 1/5\n",
      "19:05:43 [INFO]     5,247 labeled -> Accuracy: 0.5470 (Train: 11.0s, Query: 0.00s)\n",
      "19:05:56 [INFO]     Final: 5,247 labeled -> Accuracy: 0.5463, F1: 0.1768\n",
      "19:05:56 [INFO]   Run 2/5\n",
      "19:06:46 [INFO]     5,247 labeled -> Accuracy: 0.5512 (Train: 11.3s, Query: 0.00s)\n",
      "19:06:59 [INFO]     Final: 5,247 labeled -> Accuracy: 0.5506, F1: 0.1855\n",
      "19:06:59 [INFO]   Run 3/5\n",
      "19:07:48 [INFO]     5,247 labeled -> Accuracy: 0.5488 (Train: 11.1s, Query: 0.00s)\n",
      "19:08:01 [INFO]     Final: 5,247 labeled -> Accuracy: 0.5488, F1: 0.1838\n",
      "19:08:01 [INFO]   Run 4/5\n",
      "19:08:53 [INFO]     5,247 labeled -> Accuracy: 0.5476 (Train: 11.7s, Query: 0.00s)\n",
      "19:09:06 [INFO]     Final: 5,247 labeled -> Accuracy: 0.5488, F1: 0.1889\n",
      "19:09:06 [INFO]   Run 5/5\n",
      "19:09:56 [INFO]     5,247 labeled -> Accuracy: 0.5470 (Train: 11.3s, Query: 0.00s)\n",
      "19:10:09 [INFO]     Final: 5,247 labeled -> Accuracy: 0.5476, F1: 0.1781\n",
      "19:10:09 [INFO] \n",
      "SVM + Random Sampling - Budget: 100% (6,559 Samples)\n",
      "19:10:09 [INFO]   Run 1/5\n",
      "19:11:35 [INFO]     6,559 labeled -> Accuracy: 0.5463 (Train: 17.0s, Query: 0.00s)\n",
      "19:11:55 [INFO]     Final: 6,559 labeled -> Accuracy: 0.5470, F1: 0.1774\n",
      "19:11:55 [INFO]   Run 2/5\n",
      "19:13:18 [INFO]     6,559 labeled -> Accuracy: 0.5476 (Train: 16.2s, Query: 0.00s)\n",
      "19:13:38 [INFO]     Final: 6,559 labeled -> Accuracy: 0.5470, F1: 0.1774\n",
      "19:13:38 [INFO]   Run 3/5\n",
      "19:15:01 [INFO]     6,559 labeled -> Accuracy: 0.5457 (Train: 16.7s, Query: 0.00s)\n",
      "19:15:22 [INFO]     Final: 6,559 labeled -> Accuracy: 0.5470, F1: 0.1774\n",
      "19:15:22 [INFO]   Run 4/5\n",
      "19:16:46 [INFO]     6,559 labeled -> Accuracy: 0.5463 (Train: 16.5s, Query: 0.00s)\n",
      "19:17:07 [INFO]     Final: 6,559 labeled -> Accuracy: 0.5470, F1: 0.1774\n",
      "19:17:07 [INFO]   Run 5/5\n",
      "19:18:30 [INFO]     6,559 labeled -> Accuracy: 0.5470 (Train: 15.9s, Query: 0.00s)\n",
      "19:18:49 [INFO]     Final: 6,559 labeled -> Accuracy: 0.5470, F1: 0.1774\n",
      "\n",
      "[ok] SVM + Random Sampling abgeschlossen in 17.4 Minuten\n",
      "\n",
      "============================================================\n",
      "Experiment 18/20: SVM + Entropy Sampling\n",
      "============================================================\n",
      "19:18:49 [INFO] \n",
      "SVM + Entropy Sampling - Budget: 20% (1,311 Samples)\n",
      "19:18:49 [INFO]   Run 1/5\n",
      "19:18:52 [INFO]     1,311 labeled -> Accuracy: 0.5146 (Train: 0.6s, Query: 0.78s)\n",
      "19:18:53 [INFO]     Final: 1,311 labeled -> Accuracy: 0.5433, F1: 0.1809\n",
      "19:18:53 [INFO]   Run 2/5\n",
      "19:18:56 [INFO]     1,311 labeled -> Accuracy: 0.4524 (Train: 0.6s, Query: 0.70s)\n",
      "19:18:57 [INFO]     Final: 1,311 labeled -> Accuracy: 0.4659, F1: 0.1477\n",
      "19:18:57 [INFO]   Run 3/5\n",
      "19:18:59 [INFO]     1,311 labeled -> Accuracy: 0.4921 (Train: 0.6s, Query: 0.83s)\n",
      "19:19:00 [INFO]     Final: 1,311 labeled -> Accuracy: 0.5305, F1: 0.1719\n",
      "19:19:00 [INFO]   Run 4/5\n",
      "19:19:03 [INFO]     1,311 labeled -> Accuracy: 0.5354 (Train: 0.6s, Query: 0.79s)\n",
      "19:19:04 [INFO]     Final: 1,311 labeled -> Accuracy: 0.5348, F1: 0.1604\n",
      "19:19:04 [INFO]   Run 5/5\n",
      "19:19:07 [INFO]     1,311 labeled -> Accuracy: 0.5262 (Train: 0.7s, Query: 0.81s)\n",
      "19:19:08 [INFO]     Final: 1,311 labeled -> Accuracy: 0.5177, F1: 0.1414\n",
      "19:19:08 [INFO] \n",
      "SVM + Entropy Sampling - Budget: 40% (2,623 Samples)\n",
      "19:19:08 [INFO]   Run 1/5\n",
      "19:19:21 [INFO]     2,623 labeled -> Accuracy: 0.5476 (Train: 2.8s, Query: 1.16s)\n",
      "19:19:24 [INFO]     Final: 2,623 labeled -> Accuracy: 0.5476, F1: 0.1781\n",
      "19:19:24 [INFO]   Run 2/5\n",
      "19:19:37 [INFO]     2,623 labeled -> Accuracy: 0.5396 (Train: 3.0s, Query: 1.27s)\n",
      "19:19:41 [INFO]     Final: 2,623 labeled -> Accuracy: 0.5396, F1: 0.1617\n",
      "19:19:41 [INFO]   Run 3/5\n",
      "19:19:55 [INFO]     2,623 labeled -> Accuracy: 0.5372 (Train: 3.2s, Query: 1.29s)\n",
      "19:19:59 [INFO]     Final: 2,623 labeled -> Accuracy: 0.5317, F1: 0.1671\n",
      "19:19:59 [INFO]   Run 4/5\n",
      "19:20:13 [INFO]     2,623 labeled -> Accuracy: 0.5463 (Train: 3.0s, Query: 1.33s)\n",
      "19:20:16 [INFO]     Final: 2,623 labeled -> Accuracy: 0.5378, F1: 0.1769\n",
      "19:20:16 [INFO]   Run 5/5\n",
      "19:20:31 [INFO]     2,623 labeled -> Accuracy: 0.4939 (Train: 3.2s, Query: 1.44s)\n",
      "19:20:35 [INFO]     Final: 2,623 labeled -> Accuracy: 0.4787, F1: 0.1870\n",
      "19:20:35 [INFO] \n",
      "SVM + Entropy Sampling - Budget: 60% (3,935 Samples)\n",
      "19:20:35 [INFO]   Run 1/5\n",
      "19:21:02 [INFO]     3,935 labeled -> Accuracy: 0.5488 (Train: 5.5s, Query: 1.21s)\n",
      "19:21:09 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5470, F1: 0.1772\n",
      "19:21:09 [INFO]   Run 2/5\n",
      "19:21:35 [INFO]     3,935 labeled -> Accuracy: 0.5476 (Train: 5.2s, Query: 1.19s)\n",
      "19:21:42 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5476, F1: 0.1782\n",
      "19:21:42 [INFO]   Run 3/5\n",
      "19:22:12 [INFO]     3,935 labeled -> Accuracy: 0.5482 (Train: 6.3s, Query: 1.17s)\n",
      "19:22:19 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5506, F1: 0.1943\n",
      "19:22:19 [INFO]   Run 4/5\n",
      "19:22:47 [INFO]     3,935 labeled -> Accuracy: 0.5476 (Train: 5.9s, Query: 1.31s)\n",
      "19:22:54 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5482, F1: 0.1787\n",
      "19:22:54 [INFO]   Run 5/5\n",
      "19:23:24 [INFO]     3,935 labeled -> Accuracy: 0.5470 (Train: 6.1s, Query: 1.31s)\n",
      "19:23:32 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5488, F1: 0.1776\n",
      "19:23:32 [INFO] \n",
      "SVM + Entropy Sampling - Budget: 80% (5,247 Samples)\n",
      "19:23:32 [INFO]   Run 1/5\n",
      "19:24:29 [INFO]     5,247 labeled -> Accuracy: 0.5470 (Train: 11.0s, Query: 0.88s)\n",
      "19:24:43 [INFO]     Final: 5,247 labeled -> Accuracy: 0.5470, F1: 0.1773\n",
      "19:24:43 [INFO]   Run 2/5\n",
      "19:25:45 [INFO]     5,247 labeled -> Accuracy: 0.5512 (Train: 10.9s, Query: 0.82s)\n",
      "19:25:58 [INFO]     Final: 5,247 labeled -> Accuracy: 0.5512, F1: 0.1861\n",
      "19:25:58 [INFO]   Run 3/5\n",
      "19:27:03 [INFO]     5,247 labeled -> Accuracy: 0.5470 (Train: 12.1s, Query: 0.91s)\n",
      "19:27:16 [INFO]     Final: 5,247 labeled -> Accuracy: 0.5470, F1: 0.1773\n",
      "19:27:16 [INFO]   Run 4/5\n",
      "19:28:18 [INFO]     5,247 labeled -> Accuracy: 0.5476 (Train: 11.3s, Query: 0.87s)\n",
      "19:28:30 [INFO]     Final: 5,247 labeled -> Accuracy: 0.5470, F1: 0.1773\n",
      "19:28:30 [INFO]   Run 5/5\n",
      "19:29:33 [INFO]     5,247 labeled -> Accuracy: 0.5512 (Train: 11.4s, Query: 0.84s)\n",
      "19:29:47 [INFO]     Final: 5,247 labeled -> Accuracy: 0.5512, F1: 0.1861\n",
      "19:29:47 [INFO] \n",
      "SVM + Entropy Sampling - Budget: 100% (6,559 Samples)\n",
      "19:29:47 [INFO]   Run 1/5\n",
      "19:31:16 [INFO]     6,559 labeled -> Accuracy: 0.5512 (Train: 15.0s, Query: 0.30s)\n",
      "19:31:34 [INFO]     Final: 6,559 labeled -> Accuracy: 0.5470, F1: 0.1774\n",
      "19:31:34 [INFO]   Run 2/5\n",
      "19:33:04 [INFO]     6,559 labeled -> Accuracy: 0.5470 (Train: 16.1s, Query: 0.35s)\n",
      "19:33:24 [INFO]     Final: 6,559 labeled -> Accuracy: 0.5470, F1: 0.1774\n",
      "19:33:24 [INFO]   Run 3/5\n",
      "19:35:02 [INFO]     6,559 labeled -> Accuracy: 0.5470 (Train: 16.0s, Query: 0.29s)\n",
      "19:35:21 [INFO]     Final: 6,559 labeled -> Accuracy: 0.5470, F1: 0.1774\n",
      "19:35:21 [INFO]   Run 4/5\n",
      "19:36:54 [INFO]     6,559 labeled -> Accuracy: 0.5470 (Train: 16.7s, Query: 0.35s)\n",
      "19:37:15 [INFO]     Final: 6,559 labeled -> Accuracy: 0.5470, F1: 0.1774\n",
      "19:37:15 [INFO]   Run 5/5\n",
      "19:38:51 [INFO]     6,559 labeled -> Accuracy: 0.5512 (Train: 15.1s, Query: 0.32s)\n",
      "19:39:10 [INFO]     Final: 6,559 labeled -> Accuracy: 0.5470, F1: 0.1774\n",
      "\n",
      "[ok] SVM + Entropy Sampling abgeschlossen in 20.4 Minuten\n",
      "\n",
      "============================================================\n",
      "Experiment 19/20: SVM + Margin Sampling\n",
      "============================================================\n",
      "19:39:11 [INFO] \n",
      "SVM + Margin Sampling - Budget: 20% (1,311 Samples)\n",
      "19:39:11 [INFO]   Run 1/5\n",
      "19:39:13 [INFO]     1,311 labeled -> Accuracy: 0.5329 (Train: 0.6s, Query: 0.75s)\n",
      "19:39:14 [INFO]     Final: 1,311 labeled -> Accuracy: 0.5409, F1: 0.1862\n",
      "19:39:14 [INFO]   Run 2/5\n",
      "19:39:17 [INFO]     1,311 labeled -> Accuracy: 0.5262 (Train: 0.7s, Query: 0.83s)\n",
      "19:39:18 [INFO]     Final: 1,311 labeled -> Accuracy: 0.5360, F1: 0.1521\n",
      "19:39:18 [INFO]   Run 3/5\n",
      "19:39:21 [INFO]     1,311 labeled -> Accuracy: 0.5329 (Train: 0.7s, Query: 0.84s)\n",
      "19:39:22 [INFO]     Final: 1,311 labeled -> Accuracy: 0.5354, F1: 0.1680\n",
      "19:39:22 [INFO]   Run 4/5\n",
      "19:39:25 [INFO]     1,311 labeled -> Accuracy: 0.5360 (Train: 0.6s, Query: 0.80s)\n",
      "19:39:26 [INFO]     Final: 1,311 labeled -> Accuracy: 0.5390, F1: 0.1628\n",
      "19:39:26 [INFO]   Run 5/5\n",
      "19:39:28 [INFO]     1,311 labeled -> Accuracy: 0.5201 (Train: 0.6s, Query: 0.84s)\n",
      "19:39:30 [INFO]     Final: 1,311 labeled -> Accuracy: 0.5085, F1: 0.1283\n",
      "19:39:30 [INFO] \n",
      "SVM + Margin Sampling - Budget: 40% (2,623 Samples)\n",
      "19:39:30 [INFO]   Run 1/5\n",
      "19:39:43 [INFO]     2,623 labeled -> Accuracy: 0.5280 (Train: 3.0s, Query: 1.23s)\n",
      "19:39:46 [INFO]     Final: 2,623 labeled -> Accuracy: 0.5305, F1: 0.1664\n",
      "19:39:46 [INFO]   Run 2/5\n",
      "19:39:59 [INFO]     2,623 labeled -> Accuracy: 0.5476 (Train: 2.9s, Query: 1.19s)\n",
      "19:40:03 [INFO]     Final: 2,623 labeled -> Accuracy: 0.5476, F1: 0.1780\n",
      "19:40:03 [INFO]   Run 3/5\n",
      "19:40:17 [INFO]     2,623 labeled -> Accuracy: 0.5427 (Train: 3.4s, Query: 1.24s)\n",
      "19:40:21 [INFO]     Final: 2,623 labeled -> Accuracy: 0.5433, F1: 0.1756\n",
      "19:40:21 [INFO]   Run 4/5\n",
      "19:40:36 [INFO]     2,623 labeled -> Accuracy: 0.5470 (Train: 3.3s, Query: 1.26s)\n",
      "19:40:40 [INFO]     Final: 2,623 labeled -> Accuracy: 0.5482, F1: 0.1768\n",
      "19:40:40 [INFO]   Run 5/5\n",
      "19:40:55 [INFO]     2,623 labeled -> Accuracy: 0.4902 (Train: 3.6s, Query: 1.40s)\n",
      "19:41:00 [INFO]     Final: 2,623 labeled -> Accuracy: 0.4866, F1: 0.1560\n",
      "19:41:00 [INFO] \n",
      "SVM + Margin Sampling - Budget: 60% (3,935 Samples)\n",
      "19:41:00 [INFO]   Run 1/5\n",
      "19:41:26 [INFO]     3,935 labeled -> Accuracy: 0.5488 (Train: 5.5s, Query: 1.15s)\n",
      "19:41:33 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5445, F1: 0.1659\n",
      "19:41:33 [INFO]   Run 2/5\n",
      "19:41:58 [INFO]     3,935 labeled -> Accuracy: 0.5482 (Train: 5.5s, Query: 1.19s)\n",
      "19:42:05 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5524, F1: 0.1877\n",
      "19:42:05 [INFO]   Run 3/5\n",
      "19:42:36 [INFO]     3,935 labeled -> Accuracy: 0.5512 (Train: 6.4s, Query: 1.29s)\n",
      "19:42:44 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5500, F1: 0.1834\n",
      "19:42:44 [INFO]   Run 4/5\n",
      "19:43:14 [INFO]     3,935 labeled -> Accuracy: 0.5488 (Train: 6.3s, Query: 1.31s)\n",
      "19:43:21 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5470, F1: 0.1772\n",
      "19:43:21 [INFO]   Run 5/5\n",
      "19:43:51 [INFO]     3,935 labeled -> Accuracy: 0.5299 (Train: 6.1s, Query: 1.27s)\n",
      "19:43:59 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5415, F1: 0.1644\n",
      "19:43:59 [INFO] \n",
      "SVM + Margin Sampling - Budget: 80% (5,247 Samples)\n",
      "19:43:59 [INFO]   Run 1/5\n",
      "19:44:54 [INFO]     5,247 labeled -> Accuracy: 0.5427 (Train: 10.1s, Query: 0.74s)\n",
      "19:45:06 [INFO]     Final: 5,247 labeled -> Accuracy: 0.5427, F1: 0.1642\n",
      "19:45:06 [INFO]   Run 2/5\n",
      "19:46:04 [INFO]     5,247 labeled -> Accuracy: 0.5476 (Train: 11.3s, Query: 0.74s)\n",
      "19:46:16 [INFO]     Final: 5,247 labeled -> Accuracy: 0.5470, F1: 0.1773\n",
      "19:46:16 [INFO]   Run 3/5\n",
      "19:47:18 [INFO]     5,247 labeled -> Accuracy: 0.5500 (Train: 10.8s, Query: 0.81s)\n",
      "19:47:31 [INFO]     Final: 5,247 labeled -> Accuracy: 0.5451, F1: 0.1851\n",
      "19:47:31 [INFO]   Run 4/5\n",
      "19:48:29 [INFO]     5,247 labeled -> Accuracy: 0.5512 (Train: 10.1s, Query: 0.86s)\n",
      "19:48:41 [INFO]     Final: 5,247 labeled -> Accuracy: 0.5470, F1: 0.1773\n",
      "19:48:41 [INFO]   Run 5/5\n",
      "19:49:41 [INFO]     5,247 labeled -> Accuracy: 0.5470 (Train: 10.6s, Query: 0.80s)\n",
      "19:49:53 [INFO]     Final: 5,247 labeled -> Accuracy: 0.5470, F1: 0.1774\n",
      "19:49:53 [INFO] \n",
      "SVM + Margin Sampling - Budget: 100% (6,559 Samples)\n",
      "19:49:53 [INFO]   Run 1/5\n",
      "19:51:13 [INFO]     6,559 labeled -> Accuracy: 0.5427 (Train: 13.7s, Query: 0.29s)\n",
      "19:51:30 [INFO]     Final: 6,559 labeled -> Accuracy: 0.5470, F1: 0.1774\n",
      "19:51:30 [INFO]   Run 2/5\n",
      "19:52:51 [INFO]     6,559 labeled -> Accuracy: 0.5470 (Train: 13.9s, Query: 0.32s)\n",
      "19:53:08 [INFO]     Final: 6,559 labeled -> Accuracy: 0.5470, F1: 0.1774\n",
      "19:53:08 [INFO]   Run 3/5\n",
      "19:54:36 [INFO]     6,559 labeled -> Accuracy: 0.5470 (Train: 13.8s, Query: 0.29s)\n",
      "19:54:53 [INFO]     Final: 6,559 labeled -> Accuracy: 0.5470, F1: 0.1774\n",
      "19:54:53 [INFO]   Run 4/5\n",
      "19:56:22 [INFO]     6,559 labeled -> Accuracy: 0.5470 (Train: 14.7s, Query: 0.28s)\n",
      "19:56:39 [INFO]     Final: 6,559 labeled -> Accuracy: 0.5470, F1: 0.1774\n",
      "19:56:39 [INFO]   Run 5/5\n",
      "19:58:15 [INFO]     6,559 labeled -> Accuracy: 0.5463 (Train: 15.4s, Query: 0.27s)\n",
      "19:58:32 [INFO]     Final: 6,559 labeled -> Accuracy: 0.5470, F1: 0.1774\n",
      "\n",
      "[ok] SVM + Margin Sampling abgeschlossen in 19.4 Minuten\n",
      "\n",
      "============================================================\n",
      "Experiment 20/20: SVM + Least Confidence\n",
      "============================================================\n",
      "19:58:32 [INFO] \n",
      "SVM + Least Confidence - Budget: 20% (1,311 Samples)\n",
      "19:58:32 [INFO]   Run 1/5\n",
      "19:58:34 [INFO]     1,311 labeled -> Accuracy: 0.5311 (Train: 0.5s, Query: 0.72s)\n",
      "19:58:35 [INFO]     Final: 1,311 labeled -> Accuracy: 0.5384, F1: 0.1843\n",
      "19:58:35 [INFO]   Run 2/5\n",
      "19:58:37 [INFO]     1,311 labeled -> Accuracy: 0.4549 (Train: 0.5s, Query: 0.80s)\n",
      "19:58:38 [INFO]     Final: 1,311 labeled -> Accuracy: 0.5421, F1: 0.1598\n",
      "19:58:38 [INFO]   Run 3/5\n",
      "19:58:41 [INFO]     1,311 labeled -> Accuracy: 0.5329 (Train: 0.6s, Query: 0.82s)\n",
      "19:58:42 [INFO]     Final: 1,311 labeled -> Accuracy: 0.5317, F1: 0.1766\n",
      "19:58:42 [INFO]   Run 4/5\n",
      "19:58:45 [INFO]     1,311 labeled -> Accuracy: 0.5341 (Train: 0.5s, Query: 0.75s)\n",
      "19:58:46 [INFO]     Final: 1,311 labeled -> Accuracy: 0.5293, F1: 0.1575\n",
      "19:58:46 [INFO]   Run 5/5\n",
      "19:58:48 [INFO]     1,311 labeled -> Accuracy: 0.5030 (Train: 0.7s, Query: 0.82s)\n",
      "19:58:49 [INFO]     Final: 1,311 labeled -> Accuracy: 0.5152, F1: 0.1373\n",
      "19:58:49 [INFO] \n",
      "SVM + Least Confidence - Budget: 40% (2,623 Samples)\n",
      "19:58:49 [INFO]   Run 1/5\n",
      "19:59:02 [INFO]     2,623 labeled -> Accuracy: 0.5445 (Train: 3.0s, Query: 1.25s)\n",
      "19:59:06 [INFO]     Final: 2,623 labeled -> Accuracy: 0.5445, F1: 0.1773\n",
      "19:59:06 [INFO]   Run 2/5\n",
      "19:59:18 [INFO]     2,623 labeled -> Accuracy: 0.5396 (Train: 2.6s, Query: 1.16s)\n",
      "19:59:22 [INFO]     Final: 2,623 labeled -> Accuracy: 0.5341, F1: 0.1531\n",
      "19:59:22 [INFO]   Run 3/5\n",
      "19:59:35 [INFO]     2,623 labeled -> Accuracy: 0.5463 (Train: 3.0s, Query: 1.24s)\n",
      "19:59:39 [INFO]     Final: 2,623 labeled -> Accuracy: 0.5427, F1: 0.1738\n",
      "19:59:39 [INFO]   Run 4/5\n",
      "19:59:51 [INFO]     2,623 labeled -> Accuracy: 0.5390 (Train: 2.6s, Query: 1.13s)\n",
      "19:59:54 [INFO]     Final: 2,623 labeled -> Accuracy: 0.5390, F1: 0.1669\n",
      "19:59:54 [INFO]   Run 5/5\n",
      "20:00:09 [INFO]     2,623 labeled -> Accuracy: 0.5043 (Train: 3.4s, Query: 1.35s)\n",
      "20:00:13 [INFO]     Final: 2,623 labeled -> Accuracy: 0.4963, F1: 0.1677\n",
      "20:00:13 [INFO] \n",
      "SVM + Least Confidence - Budget: 60% (3,935 Samples)\n",
      "20:00:13 [INFO]   Run 1/5\n",
      "20:00:39 [INFO]     3,935 labeled -> Accuracy: 0.5476 (Train: 5.1s, Query: 1.16s)\n",
      "20:00:45 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5470, F1: 0.1863\n",
      "20:00:45 [INFO]   Run 2/5\n",
      "20:01:11 [INFO]     3,935 labeled -> Accuracy: 0.5482 (Train: 5.2s, Query: 1.17s)\n",
      "20:01:18 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5482, F1: 0.1790\n",
      "20:01:18 [INFO]   Run 3/5\n",
      "20:01:46 [INFO]     3,935 labeled -> Accuracy: 0.5512 (Train: 5.9s, Query: 1.38s)\n",
      "20:01:54 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5463, F1: 0.1746\n",
      "20:01:54 [INFO]   Run 4/5\n",
      "20:02:23 [INFO]     3,935 labeled -> Accuracy: 0.5494 (Train: 5.8s, Query: 1.25s)\n",
      "20:02:30 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5488, F1: 0.1796\n",
      "20:02:30 [INFO]   Run 5/5\n",
      "20:02:59 [INFO]     3,935 labeled -> Accuracy: 0.5445 (Train: 6.2s, Query: 1.31s)\n",
      "20:03:07 [INFO]     Final: 3,935 labeled -> Accuracy: 0.5463, F1: 0.1777\n",
      "20:03:07 [INFO] \n",
      "SVM + Least Confidence - Budget: 80% (5,247 Samples)\n",
      "20:03:07 [INFO]   Run 1/5\n",
      "20:04:05 [INFO]     5,247 labeled -> Accuracy: 0.5512 (Train: 10.6s, Query: 0.82s)\n",
      "20:04:17 [INFO]     Final: 5,247 labeled -> Accuracy: 0.5512, F1: 0.1861\n",
      "20:04:17 [INFO]   Run 2/5\n",
      "20:05:13 [INFO]     5,247 labeled -> Accuracy: 0.5476 (Train: 10.1s, Query: 0.77s)\n",
      "20:05:25 [INFO]     Final: 5,247 labeled -> Accuracy: 0.5470, F1: 0.1773\n",
      "20:05:25 [INFO]   Run 3/5\n",
      "20:06:28 [INFO]     5,247 labeled -> Accuracy: 0.5470 (Train: 11.0s, Query: 0.85s)\n",
      "20:06:40 [INFO]     Final: 5,247 labeled -> Accuracy: 0.5470, F1: 0.1773\n",
      "20:06:40 [INFO]   Run 4/5\n",
      "20:07:40 [INFO]     5,247 labeled -> Accuracy: 0.5171 (Train: 11.4s, Query: 0.94s)\n",
      "20:07:54 [INFO]     Final: 5,247 labeled -> Accuracy: 0.5470, F1: 0.1773\n",
      "20:07:54 [INFO]   Run 5/5\n",
      "20:08:55 [INFO]     5,247 labeled -> Accuracy: 0.5476 (Train: 10.6s, Query: 0.84s)\n",
      "20:09:08 [INFO]     Final: 5,247 labeled -> Accuracy: 0.5470, F1: 0.1773\n",
      "20:09:08 [INFO] \n",
      "SVM + Least Confidence - Budget: 100% (6,559 Samples)\n",
      "20:09:08 [INFO]   Run 1/5\n",
      "20:10:37 [INFO]     6,559 labeled -> Accuracy: 0.5470 (Train: 15.4s, Query: 0.29s)\n",
      "20:10:57 [INFO]     Final: 6,559 labeled -> Accuracy: 0.5470, F1: 0.1774\n",
      "20:10:57 [INFO]   Run 2/5\n",
      "20:12:24 [INFO]     6,559 labeled -> Accuracy: 0.5470 (Train: 15.1s, Query: 0.28s)\n",
      "20:12:42 [INFO]     Final: 6,559 labeled -> Accuracy: 0.5470, F1: 0.1774\n",
      "20:12:42 [INFO]   Run 3/5\n",
      "20:14:12 [INFO]     6,559 labeled -> Accuracy: 0.5470 (Train: 14.8s, Query: 0.29s)\n",
      "20:14:31 [INFO]     Final: 6,559 labeled -> Accuracy: 0.5470, F1: 0.1774\n",
      "20:14:31 [INFO]   Run 4/5\n",
      "20:16:03 [INFO]     6,559 labeled -> Accuracy: 0.5470 (Train: 15.0s, Query: 0.30s)\n",
      "20:16:21 [INFO]     Final: 6,559 labeled -> Accuracy: 0.5470, F1: 0.1774\n",
      "20:16:21 [INFO]   Run 5/5\n",
      "20:17:53 [INFO]     6,559 labeled -> Accuracy: 0.5470 (Train: 14.7s, Query: 0.30s)\n",
      "20:18:12 [INFO]     Final: 6,559 labeled -> Accuracy: 0.5470, F1: 0.1774\n",
      "\n",
      "[ok] SVM + Least Confidence abgeschlossen in 19.7 Minuten\n",
      "\n",
      "[ok] Alle Experimente abgeschlossen in 184.3 Minuten\n",
      "\n",
      "============================================================\n",
      "Führe statistische Analyse durch...\n",
      "============================================================\n",
      "\n",
      "====================================================================================================\n",
      "DETAILLIERTER STATISTISCHER BERICHT - DACHMATERIAL\n",
      "====================================================================================================\n",
      "Signifikanzniveau: 0.05 (mit Bonferroni-Korrektur)\n",
      "Anzahl Runs pro Experiment: 5\n",
      "Statistischer Test: Wilcoxon Signed-Rank Test\n",
      "Effektstärkemaß: Cliff's Delta\n",
      "\n",
      "\n",
      "Keine signifikanten Verbesserungen gefunden!\n",
      "\n",
      "\n",
      "ZUSAMMENFASSUNG NACH STRATEGIE:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Entropy Sampling:\n",
      "  - Signifikante Verbesserungen: 0/25 (0.0%)\n",
      "  - Durchschnittliche Verbesserung: 20.92%\n",
      "  - Durchschnittliche Effektstärke: 0.134\n",
      "\n",
      "Margin Sampling:\n",
      "  - Signifikante Verbesserungen: 0/25 (0.0%)\n",
      "  - Durchschnittliche Verbesserung: 17.45%\n",
      "  - Durchschnittliche Effektstärke: 0.208\n",
      "\n",
      "Least Confidence:\n",
      "  - Signifikante Verbesserungen: 0/25 (0.0%)\n",
      "  - Durchschnittliche Verbesserung: 19.46%\n",
      "  - Durchschnittliche Effektstärke: 0.202\n",
      "\n",
      "\n",
      "ZUSAMMENFASSUNG NACH KLASSIFIKATOR:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Neural Network:\n",
      "  - Signifikante Verbesserungen: 0/15 (0.0%)\n",
      "\n",
      "Naive Bayes:\n",
      "  - Signifikante Verbesserungen: 0/15 (0.0%)\n",
      "\n",
      "Random Forest:\n",
      "  - Signifikante Verbesserungen: 0/15 (0.0%)\n",
      "\n",
      "Logistic Regression:\n",
      "  - Signifikante Verbesserungen: 0/15 (0.0%)\n",
      "\n",
      "SVM:\n",
      "  - Signifikante Verbesserungen: 0/15 (0.0%)\n",
      "\n",
      "====================================================================================================\n",
      "20:18:12 [INFO] [ok] Statistischer Bericht gespeichert: reports/dachmaterial_statistical_report.txt\n",
      "\n",
      "============================================================\n",
      "Berechne Label-Einsparungen...\n",
      "============================================================\n",
      "20:18:16 [INFO] [ok] Label-Einsparungs-Analyse erstellt: plots/dachmaterial_label_savings_analysis.png\n",
      "\n",
      "================================================================================\n",
      "LABEL-EINSPARUNGS-BERICHT - Dachmaterial\n",
      "================================================================================\n",
      "\n",
      "ZIEL: 90% der Baseline-Performance\n",
      "------------------------------------------------------------\n",
      "\n",
      "Logistic Regression:\n",
      "  Baseline (Random 100%): 0.5396\n",
      "  Ziel-Accuracy: 0.4857\n",
      "  Labels benötigt:\n",
      "    - Random Sampling     :    400 ±  244 ( 93.9% gespart)\n",
      "    - Margin Sampling     :    400 ±  244 ( 93.9% gespart)\n",
      "    - Least Confidence    :    898 ± 1281 ( 86.3% gespart)\n",
      "    - Entropy Sampling    :  1,655 ± 2087 ( 74.8% gespart)\n",
      "\n",
      "Naive Bayes:\n",
      "  Baseline (Random 100%): 0.0262\n",
      "  Ziel-Accuracy: 0.0236\n",
      "  Labels benötigt:\n",
      "    - Random Sampling     :    100 ±    0 ( 98.5% gespart)\n",
      "    - Entropy Sampling    :    100 ±    0 ( 98.5% gespart)\n",
      "    - Margin Sampling     :    100 ±    0 ( 98.5% gespart)\n",
      "    - Least Confidence    :    100 ±    0 ( 98.5% gespart)\n",
      "\n",
      "Neural Network:\n",
      "  Baseline (Random 100%): 0.5454\n",
      "  Ziel-Accuracy: 0.4908\n",
      "  Labels benötigt:\n",
      "    - Random Sampling     :  1,100 ±    0 ( 83.2% gespart)\n",
      "    - Margin Sampling     :  2,373 ± 1851 ( 63.8% gespart)\n",
      "    - Least Confidence    :  2,373 ± 1851 ( 63.8% gespart)\n",
      "    - Entropy Sampling    :  3,210 ± 1944 ( 51.1% gespart)\n",
      "\n",
      "Random Forest:\n",
      "  Baseline (Random 100%): 0.4713\n",
      "  Ziel-Accuracy: 0.4242\n",
      "  Labels benötigt:\n",
      "    - Random Sampling     :    200 ±  200 ( 97.0% gespart)\n",
      "    - Entropy Sampling    :    200 ±  200 ( 97.0% gespart)\n",
      "    - Margin Sampling     :    200 ±  200 ( 97.0% gespart)\n",
      "    - Least Confidence    :    200 ±  200 ( 97.0% gespart)\n",
      "\n",
      "SVM:\n",
      "  Baseline (Random 100%): 0.5470\n",
      "  Ziel-Accuracy: 0.4923\n",
      "  Labels benötigt:\n",
      "    - Random Sampling     :    300 ±  244 ( 95.4% gespart)\n",
      "    - Margin Sampling     :    300 ±  244 ( 95.4% gespart)\n",
      "    - Least Confidence    :    300 ±  244 ( 95.4% gespart)\n",
      "    - Entropy Sampling    :    400 ±  400 ( 93.9% gespart)\n",
      "\n",
      "ZIEL: 95% der Baseline-Performance\n",
      "------------------------------------------------------------\n",
      "\n",
      "Logistic Regression:\n",
      "  Baseline (Random 100%): 0.5396\n",
      "  Ziel-Accuracy: 0.5127\n",
      "  Labels benötigt:\n",
      "    - Random Sampling     :    600 ±    0 ( 90.9% gespart)\n",
      "    - Margin Sampling     :  1,178 ± 1223 ( 82.0% gespart)\n",
      "    - Least Confidence    :  1,875 ± 1896 ( 71.4% gespart)\n",
      "    - Entropy Sampling    :  2,513 ± 1954 ( 61.7% gespart)\n",
      "\n",
      "Naive Bayes:\n",
      "  Baseline (Random 100%): 0.0262\n",
      "  Ziel-Accuracy: 0.0249\n",
      "  Labels benötigt:\n",
      "    - Random Sampling     :    100 ±    0 ( 98.5% gespart)\n",
      "    - Entropy Sampling    :    100 ±    0 ( 98.5% gespart)\n",
      "    - Margin Sampling     :    100 ±    0 ( 98.5% gespart)\n",
      "    - Least Confidence    :    100 ±    0 ( 98.5% gespart)\n",
      "\n",
      "Neural Network:\n",
      "  Baseline (Random 100%): 0.5454\n",
      "  Ziel-Accuracy: 0.5181\n",
      "  Labels benötigt:\n",
      "    - Random Sampling     :  2,293 ± 1871 ( 65.0% gespart)\n",
      "    - Margin Sampling     :  3,071 ± 1775 ( 53.2% gespart)\n",
      "    - Least Confidence    :  3,311 ± 1633 ( 49.5% gespart)\n",
      "    - Entropy Sampling    :  4,325 ± 1695 ( 34.1% gespart)\n",
      "\n",
      "Random Forest:\n",
      "  Baseline (Random 100%): 0.4713\n",
      "  Ziel-Accuracy: 0.4478\n",
      "  Labels benötigt:\n",
      "    - Entropy Sampling    :    300 ±  244 ( 95.4% gespart)\n",
      "      -> 25.0% weniger Labels als Random Sampling\n",
      "    - Margin Sampling     :    300 ±  244 ( 95.4% gespart)\n",
      "      -> 25.0% weniger Labels als Random Sampling\n",
      "    - Least Confidence    :    300 ±  244 ( 95.4% gespart)\n",
      "      -> 25.0% weniger Labels als Random Sampling\n",
      "    - Random Sampling     :    400 ±  400 ( 93.9% gespart)\n",
      "\n",
      "SVM:\n",
      "  Baseline (Random 100%): 0.5470\n",
      "  Ziel-Accuracy: 0.5196\n",
      "  Labels benötigt:\n",
      "    - Random Sampling     :    600 ±    0 ( 90.9% gespart)\n",
      "    - Margin Sampling     :    800 ±  244 ( 87.8% gespart)\n",
      "    - Entropy Sampling    :  1,496 ± 1551 ( 77.2% gespart)\n",
      "    - Least Confidence    :  1,496 ± 1551 ( 77.2% gespart)\n",
      "\n",
      "ZIEL: 98% der Baseline-Performance\n",
      "------------------------------------------------------------\n",
      "\n",
      "Logistic Regression:\n",
      "  Baseline (Random 100%): 0.5396\n",
      "  Ziel-Accuracy: 0.5288\n",
      "  Labels benötigt:\n",
      "    - Random Sampling     :    800 ±  244 ( 87.8% gespart)\n",
      "    - Margin Sampling     :  2,453 ± 1848 ( 62.6% gespart)\n",
      "    - Least Confidence    :  3,408 ± 2089 ( 48.0% gespart)\n",
      "    - Entropy Sampling    :  3,946 ± 1892 ( 39.8% gespart)\n",
      "\n",
      "Naive Bayes:\n",
      "  Baseline (Random 100%): 0.0262\n",
      "  Ziel-Accuracy: 0.0257\n",
      "  Labels benötigt:\n",
      "    - Random Sampling     :    100 ±    0 ( 98.5% gespart)\n",
      "    - Entropy Sampling    :    100 ±    0 ( 98.5% gespart)\n",
      "    - Margin Sampling     :    100 ±    0 ( 98.5% gespart)\n",
      "    - Least Confidence    :    100 ±    0 ( 98.5% gespart)\n",
      "\n",
      "Neural Network:\n",
      "  Baseline (Random 100%): 0.5454\n",
      "  Ziel-Accuracy: 0.5345\n",
      "  Labels benötigt:\n",
      "    - Margin Sampling     :  3,668 ± 1840 ( 44.1% gespart)\n",
      "      -> 8.9% weniger Labels als Random Sampling\n",
      "    - Random Sampling     :  4,026 ± 1821 ( 38.6% gespart)\n",
      "    - Least Confidence    :  4,046 ± 1735 ( 38.3% gespart)\n",
      "    - Entropy Sampling    :  4,543 ± 1688 ( 30.7% gespart)\n",
      "\n",
      "Random Forest:\n",
      "  Baseline (Random 100%): 0.4713\n",
      "  Ziel-Accuracy: 0.4619\n",
      "  Labels benötigt:\n",
      "    - Margin Sampling     :    500 ±  200 ( 92.4% gespart)\n",
      "      -> 28.6% weniger Labels als Random Sampling\n",
      "    - Least Confidence    :    600 ±  316 ( 90.9% gespart)\n",
      "      -> 14.3% weniger Labels als Random Sampling\n",
      "    - Random Sampling     :    700 ±  374 ( 89.3% gespart)\n",
      "    - Entropy Sampling    :    998 ± 1233 ( 84.8% gespart)\n",
      "\n",
      "SVM:\n",
      "  Baseline (Random 100%): 0.5470\n",
      "  Ziel-Accuracy: 0.5360\n",
      "  Labels benötigt:\n",
      "    - Random Sampling     :  2,671 ± 1951 ( 59.3% gespart)\n",
      "    - Margin Sampling     :  2,911 ± 1854 ( 55.6% gespart)\n",
      "    - Entropy Sampling    :  3,050 ± 2027 ( 53.5% gespart)\n",
      "    - Least Confidence    :  3,290 ± 1885 ( 49.8% gespart)\n",
      "\n",
      "\n",
      "BESTE STRATEGIEN (bei 95% Performance):\n",
      "------------------------------------------------------------\n",
      "Logistic Regression: Random Sampling (nur 600 Labels = 90.9% Einsparung)\n",
      "Naive Bayes: Random Sampling (nur 100 Labels = 98.5% Einsparung)\n",
      "Neural Network: Random Sampling (nur 2,293 Labels = 65.0% Einsparung)\n",
      "Random Forest: Entropy Sampling (nur 300 Labels = 95.4% Einsparung)\n",
      "SVM: Random Sampling (nur 600 Labels = 90.9% Einsparung)\n",
      "\n",
      "\n",
      "DURCHSCHNITTLICHE EINSPARUNGEN ÜBER ALLE KLASSIFIKATOREN:\n",
      "------------------------------------------------------------\n",
      "Entropy Sampling: -106.4% weniger Labels als Random Sampling\n",
      "Least Confidence: -76.3% weniger Labels als Random Sampling\n",
      "Margin Sampling: -27.7% weniger Labels als Random Sampling\n",
      "\n",
      "================================================================================\n",
      "20:18:16 [INFO] [ok] Label-Einsparungs-Bericht gespeichert: reports/dachmaterial_label_savings_report.txt\n",
      "[ok] Label-Einsparungen gespeichert: results/dachmaterial_label_savings.csv\n",
      "\n",
      "============================================================\n",
      "Erstelle Visualisierungen...\n",
      "============================================================\n",
      "20:18:17 [INFO] [ok] Visualisierung mit Signifikanz für Logistic Regression erstellt: plots/dachmaterial_logistic_regression_with_significance.png\n",
      "20:18:19 [INFO] [ok] Visualisierung mit Signifikanz für Naive Bayes erstellt: plots/dachmaterial_naive_bayes_with_significance.png\n",
      "20:18:21 [INFO] [ok] Visualisierung mit Signifikanz für Neural Network erstellt: plots/dachmaterial_neural_network_with_significance.png\n",
      "20:18:23 [INFO] [ok] Visualisierung mit Signifikanz für Random Forest erstellt: plots/dachmaterial_random_forest_with_significance.png\n",
      "20:18:25 [INFO] [ok] Visualisierung mit Signifikanz für SVM erstellt: plots/dachmaterial_svm_with_significance.png\n",
      "20:18:25 [WARNING] Keine signifikanten Ergebnisse gefunden!\n",
      "20:18:28 [INFO] [ok] Statistische Zusammenfassung erstellt: plots/dachmaterial_statistical_summary.png\n",
      "20:18:29 [INFO] [ok] Finale Vergleichsvisualisierung erstellt: plots/dachmaterial_final_comparison.png\n",
      "\n",
      "============================================================\n",
      "BESTE KOMBINATION BEI 100% BUDGET:\n",
      "============================================================\n",
      "Klassifikator: SVM\n",
      "Query-Strategie: Random Sampling\n",
      "Test Accuracy: 0.5470 (±0.0000)\n",
      "F1-Score: 0.1774\n",
      "============================================================\n",
      "\n",
      "TOP 5 KOMBINATIONEN:\n",
      "------------------------------------------------------------\n",
      "1. SVM + Random Sampling: Acc=0.5470, F1=0.1774\n",
      "2. SVM + Entropy Sampling: Acc=0.5470, F1=0.1774\n",
      "3. SVM + Margin Sampling: Acc=0.5470, F1=0.1774\n",
      "4. SVM + Least Confidence: Acc=0.5470, F1=0.1774\n",
      "5. Neural Network + Entropy Sampling: Acc=0.5454, F1=0.1840\n",
      "20:18:30 [INFO] [ok] Improvement-Analyse erstellt: plots/dachmaterial_improvement_analysis.png\n",
      "[ok] Alle Visualisierungen erstellt\n",
      "\n",
      "[ok] Ergebnisse gespeichert in: results/dachmaterial_active_learning_results.csv\n",
      "[ok] Statistische Analyse gespeichert in: results/dachmaterial_statistical_analysis.csv\n",
      "[ok] Zusammenfassung gespeichert in: results/dachmaterial_active_learning_summary.xlsx\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENT ERFOLGREICH ABGESCHLOSSEN\n",
      "Gesamtanzahl Experimente: 500\n",
      "Datensatzgröße: 6,559 Trainingssamples\n",
      "Klassifikatoren: 5\n",
      "Query-Strategien: 4\n",
      "Budget-Stufen: 5\n",
      "Wiederholungen pro Experiment: 5\n",
      "\n",
      "Statistische Analyse:\n",
      "- Anzahl Vergleiche: 75\n",
      "- Signifikante Ergebnisse: 0 (0.0%)\n",
      "- Verwendeter Test: Wilcoxon Signed-Rank Test\n",
      "- Effektstärkemaß: Cliff's Delta\n",
      "- Multiple Vergleiche: Bonferroni-Korrektur\n",
      "\n",
      "Label-Einsparungs-Analyse durchgeführt!\n",
      "- Visualisierung: plots/dachmaterial_label_savings_analysis.png\n",
      "- Bericht: reports/dachmaterial_label_savings_report.txt\n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 0\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "=================================================================\n",
    "Active Learning für Dachmaterial-Klassifikation - Robuste Version mit statistischer Analyse\n",
    "=================================================================\n",
    "Professionelles Skript für Dachmaterial-Experimente mit fairem Vergleich\n",
    "zwischen aktivem Lernen und Random Sampling als Baseline.\n",
    "\n",
    "Verwendet den KOMPLETTEN Dachmaterial-Datensatz mit Budget-Stufen\n",
    "von 20%, 40%, 60%, 80% und 100%.\n",
    "\n",
    "Dataset: umrisse_with_all_data_and_shape_and_patch_and_normal.csv\n",
    "Zielvariable: mat_qgis (11 Klassen von Dachmaterialien)\n",
    "\n",
    "Version: 7.3 - Mit Mindest-Sample-Filterung für kleine Klassen\n",
    "            \n",
    "Klassifikatoren:\n",
    "- Naive Bayes\n",
    "- Random Forest\n",
    "- Logistic Regression\n",
    "- SVM\n",
    "- Neural Network (TabularNN)\n",
    "\n",
    "Query-Strategien:\n",
    "- Random Sampling (Baseline)\n",
    "- Entropy Sampling\n",
    "- Margin Sampling\n",
    "- Least Confidence\n",
    "\n",
    "Statistische Analyse:\n",
    "- Wilcoxon Signed-Rank Test\n",
    "- Cliff's Delta Effektstärke\n",
    "- Bonferroni-Korrektur für multiple Vergleiche\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Matplotlib Backend setzen bevor pyplot importiert wird\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # Für Server ohne GUI\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Seaborn mit Fehlerbehandlung\n",
    "try:\n",
    "    import seaborn as sns\n",
    "    # Prüfe ob der Style verfügbar ist\n",
    "    try:\n",
    "        plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    except:\n",
    "        try:\n",
    "            plt.style.use('seaborn-whitegrid')\n",
    "        except:\n",
    "            plt.style.use('ggplot')\n",
    "except ImportError:\n",
    "    print(\"Warnung: Seaborn nicht installiert. Verwende Standard-Matplotlib.\")\n",
    "    sns = None\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import sklearn\n",
    "\n",
    "# Statistische Tests\n",
    "import scipy\n",
    "from scipy import stats\n",
    "from scipy.stats import wilcoxon\n",
    "import itertools\n",
    "\n",
    "# Excel-Export\n",
    "try:\n",
    "    import openpyxl\n",
    "    EXCEL_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Warnung: openpyxl nicht installiert. Excel-Export wird deaktiviert.\")\n",
    "    EXCEL_AVAILABLE = False\n",
    "\n",
    "# SSL-Fehler beim Download verhindern\n",
    "import ssl\n",
    "try:\n",
    "    ssl._create_default_https_context = ssl._create_unverified_context\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# Reproduzierbarkeit\n",
    "# -------------------------------------------------------------------------------\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# Logging konfigurieren mit UTF-8 Encoding\n",
    "# -------------------------------------------------------------------------------\n",
    "# Erstelle Log-Verzeichnis\n",
    "log_dir = \"logs\"\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "# Logging Setup mit UTF-8 Encoding für Windows\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(\n",
    "            os.path.join(log_dir, f\"dachmaterial_active_learning_{time.strftime('%Y%m%d_%H%M%S')}.log\"),\n",
    "            encoding='utf-8'\n",
    "        ),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set encoding for stdout to handle Unicode - nur wenn möglich\n",
    "if sys.platform == 'win32':\n",
    "    # Prüfe ob wir in einer Jupyter/IPython Umgebung sind\n",
    "    try:\n",
    "        get_ipython()\n",
    "        # In Jupyter/IPython - keine Änderung nötig\n",
    "        logger.info(\"Jupyter/IPython Umgebung erkannt - UTF-8 Handling bereits aktiv\")\n",
    "    except NameError:\n",
    "        # Normales Python - versuche UTF-8 zu setzen\n",
    "        try:\n",
    "            import io\n",
    "            if hasattr(sys.stdout, 'buffer'):\n",
    "                sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')\n",
    "                sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Konnte UTF-8 Encoding nicht setzen: {e}\")\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# Konfiguration\n",
    "# -------------------------------------------------------------------------------\n",
    "BUDGET_PERCENTAGES = [0.2, 0.4, 0.6, 0.8, 1.0]  # 20%, 40%, 60%, 80%, 100%\n",
    "BATCH_SIZE = 500  # Größere Batches für effizienteres Training\n",
    "N_RUNS = 5  # Erhöht von 3 auf 5 für bessere statistische Aussagekraft\n",
    "INITIAL_PERCENTAGE = 0.01  # 1% initial labeling\n",
    "SIGNIFICANCE_LEVEL = 0.05  # Für statistische Tests\n",
    "MIN_SAMPLES_PER_CLASS = 20  # Mindestanzahl Samples pro Klasse\n",
    "\n",
    "# Dachmaterial Klassen (wird dynamisch geladen)\n",
    "DACHMATERIAL_CLASSES = []\n",
    "\n",
    "# Erstelle Output-Verzeichnisse\n",
    "output_dirs = [\"plots\", \"results\", \"reports\"]\n",
    "for dir_name in output_dirs:\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "        logger.info(f\"Erstellt Verzeichnis: {dir_name}\")\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# 1) Dachmaterial-Daten laden - VOLLSTÄNDIGER DATENSATZ\n",
    "# -------------------------------------------------------------------------------\n",
    "def load_dachmaterial_data(filepath='umrisse_with_all_data_and_shape_and_patch_and_normal.csv'):\n",
    "    \"\"\"\n",
    "    Lädt den VOLLSTÄNDIGEN Dachmaterial-Datensatz.\n",
    "    Filtert Klassen mit zu wenigen Samples aus.\n",
    "    Gibt auch den Preprocessor zurück für konsistente Transformation.\n",
    "    \"\"\"\n",
    "    logger.info(\"Lade vollständigen Dachmaterial-Datensatz...\")\n",
    "    \n",
    "    try:\n",
    "        # Daten laden\n",
    "        df = pd.read_csv(filepath)\n",
    "        logger.info(f\"[ok] Datensatz geladen: {len(df):,} Zeilen, {len(df.columns)} Spalten\")\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"Datei '{filepath}' nicht gefunden!\")\n",
    "        logger.error(\"Bitte stellen Sie sicher, dass die CSV-Datei im aktuellen Verzeichnis liegt.\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Fehler beim Laden der Daten: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Zielvariable und Features definieren\n",
    "    target_col = 'mat_qgis'\n",
    "    feature_cols = ['area', 'area_type', 'Shape', 'ezg']\n",
    "    \n",
    "    # Nur Zeilen mit gültiger Zielvariable behalten\n",
    "    df = df[df[target_col].notna()].copy()\n",
    "    \n",
    "    # Klassen-Verteilung anzeigen\n",
    "    class_dist = df[target_col].value_counts()\n",
    "    logger.info(f\"Ursprüngliche Klassen-Verteilung:\\n{class_dist}\")\n",
    "    \n",
    "    # Filtere Klassen mit zu wenigen Samples\n",
    "    valid_classes = class_dist[class_dist >= MIN_SAMPLES_PER_CLASS].index.tolist()\n",
    "    removed_classes = class_dist[class_dist < MIN_SAMPLES_PER_CLASS].index.tolist()\n",
    "    \n",
    "    if removed_classes:\n",
    "        logger.warning(f\"Entferne Klassen mit weniger als {MIN_SAMPLES_PER_CLASS} Samples:\")\n",
    "        for cls in removed_classes:\n",
    "            logger.warning(f\"  - {cls}: {class_dist[cls]} Samples\")\n",
    "    \n",
    "    # Behalte nur Samples der gültigen Klassen\n",
    "    df = df[df[target_col].isin(valid_classes)].copy()\n",
    "    \n",
    "    # Aktualisierte Klassen-Verteilung\n",
    "    class_dist_filtered = df[target_col].value_counts()\n",
    "    logger.info(f\"Gefilterte Klassen-Verteilung:\\n{class_dist_filtered}\")\n",
    "    \n",
    "    # Features und Target trennen\n",
    "    X = df[feature_cols].copy()\n",
    "    y = df[target_col].copy()\n",
    "    \n",
    "    # Label Encoding für Target\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    \n",
    "    # Dachmaterial-Klassen speichern\n",
    "    global DACHMATERIAL_CLASSES\n",
    "    DACHMATERIAL_CLASSES = list(label_encoder.classes_)\n",
    "    \n",
    "    logger.info(f\"[ok] Dachmaterial-Datensatz vorbereitet: {len(X):,} Samples\")\n",
    "    logger.info(f\"  Klassen: {len(DACHMATERIAL_CLASSES)} - {', '.join(DACHMATERIAL_CLASSES)}\")\n",
    "    \n",
    "    # Feature-Typen analysieren\n",
    "    numeric_features = ['area']\n",
    "    categorical_features = ['area_type', 'Shape', 'ezg']\n",
    "    \n",
    "    # Preprocessing Pipeline erstellen\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('onehot', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'))\n",
    "    ])\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ])\n",
    "    \n",
    "    # Robuster Train/Test Split\n",
    "    # Verwende StratifiedShuffleSplit für bessere Kontrolle\n",
    "    splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=SEED)\n",
    "    train_idx, test_idx = next(splitter.split(X, y_encoded))\n",
    "    \n",
    "    X_train = X.iloc[train_idx]\n",
    "    X_test = X.iloc[test_idx]\n",
    "    y_train = y_encoded[train_idx]\n",
    "    y_test = y_encoded[test_idx]\n",
    "    \n",
    "    # Preprocessing\n",
    "    X_train_processed = preprocessor.fit_transform(X_train).astype(np.float32)\n",
    "    X_test_processed = preprocessor.transform(X_test).astype(np.float32)\n",
    "    \n",
    "    # Validierung der Daten - mit verbesserter Prüfung\n",
    "    train_classes = set(np.unique(y_train))\n",
    "    test_classes = set(np.unique(y_test))\n",
    "    all_classes = set(range(len(DACHMATERIAL_CLASSES)))\n",
    "    \n",
    "    logger.info(f\"Klassen im Trainingsset: {len(train_classes)}\")\n",
    "    logger.info(f\"Klassen im Testset: {len(test_classes)}\")\n",
    "    \n",
    "    # Warnung wenn nicht alle Klassen im Test-Set sind\n",
    "    missing_in_test = all_classes - test_classes\n",
    "    if missing_in_test:\n",
    "        logger.warning(f\"Folgende Klassen fehlen im Test-Set: {[DACHMATERIAL_CLASSES[i] for i in missing_in_test]}\")\n",
    "        logger.warning(\"Dies kann bei sehr unbalancierten Datensätzen vorkommen.\")\n",
    "    \n",
    "    # Sicherstellen dass mindestens die Mehrheit der Klassen vertreten ist\n",
    "    if len(test_classes) < len(all_classes) * 0.7:\n",
    "        logger.error(\"Zu wenige Klassen im Test-Set! Versuche anderen Random State.\")\n",
    "        # Versuche mit anderem Random State\n",
    "        for attempt in range(5):\n",
    "            new_seed = SEED + attempt + 1\n",
    "            splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=new_seed)\n",
    "            train_idx, test_idx = next(splitter.split(X, y_encoded))\n",
    "            \n",
    "            y_train_temp = y_encoded[train_idx]\n",
    "            y_test_temp = y_encoded[test_idx]\n",
    "            \n",
    "            test_classes_temp = set(np.unique(y_test_temp))\n",
    "            if len(test_classes_temp) >= len(all_classes) * 0.8:\n",
    "                logger.info(f\"Besserer Split gefunden mit Seed {new_seed}\")\n",
    "                X_train = X.iloc[train_idx]\n",
    "                X_test = X.iloc[test_idx]\n",
    "                y_train = y_train_temp\n",
    "                y_test = y_test_temp\n",
    "                \n",
    "                # Re-fit preprocessing\n",
    "                X_train_processed = preprocessor.fit_transform(X_train).astype(np.float32)\n",
    "                X_test_processed = preprocessor.transform(X_test).astype(np.float32)\n",
    "                break\n",
    "    \n",
    "    logger.info(f\"[ok] Daten vorbereitet: {len(X_train):,} Trainingssamples, {len(X_test):,} Testsamples\")\n",
    "    logger.info(f\"  Feature-Dimension nach Preprocessing: {X_train_processed.shape[1]}\")\n",
    "    logger.info(f\"  Klassen: {len(np.unique(y_train))} im Training, {len(np.unique(y_test))} im Test\")\n",
    "    logger.info(f\"  Speicherbedarf: {(X_train_processed.nbytes + X_test_processed.nbytes) / 1024**2:.1f} MB\")\n",
    "    \n",
    "    return X_train_processed, y_train, X_test_processed, y_test, label_encoder, preprocessor\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# 2) TabularNN Modell für Dachmaterial\n",
    "# -------------------------------------------------------------------------------\n",
    "class OptimizedTabularNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Optimierte NN-Architektur für tabellarische Dachmaterial-Daten.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, num_classes=11):\n",
    "        super(OptimizedTabularNN, self).__init__()\n",
    "        \n",
    "        # Architektur für tabellarische Daten\n",
    "        self.features = nn.Sequential(\n",
    "            # Layer 1\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.4),\n",
    "            \n",
    "            # Layer 2\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            # Layer 3\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Linear(64, num_classes)\n",
    "        \n",
    "        # Device handling mit Fehlerbehandlung\n",
    "        if torch.cuda.is_available():\n",
    "            try:\n",
    "                self.device = torch.device('cuda')\n",
    "                # Test ob CUDA wirklich funktioniert\n",
    "                test_tensor = torch.zeros(1).cuda()\n",
    "                del test_tensor\n",
    "            except:\n",
    "                logger.warning(\"CUDA verfügbar aber nicht nutzbar. Verwende CPU.\")\n",
    "                self.device = torch.device('cpu')\n",
    "        else:\n",
    "            self.device = torch.device('cpu')\n",
    "        \n",
    "        self.to(self.device)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize network weights.\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def fit(self, X_np, y_np, epochs=10, lr=1e-3, batch_size=256, verbose=False):\n",
    "        \"\"\"\n",
    "        Trainiert das TabularNN mit optimierten Hyperparametern.\n",
    "        \"\"\"\n",
    "        self.train()\n",
    "        \n",
    "        # Hyperparameter-Anpassung basierend auf Datensatzgröße\n",
    "        if len(X_np) < 1000:\n",
    "            batch_size = min(32, len(X_np))\n",
    "            lr = lr * 0.1\n",
    "        \n",
    "        optimizer = optim.AdamW(self.parameters(), lr=lr, weight_decay=1e-4)\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Create dataset\n",
    "        try:\n",
    "            dataset = TensorDataset(\n",
    "                torch.from_numpy(X_np).float(),\n",
    "                torch.from_numpy(y_np).long()\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Fehler beim Erstellen des Datasets: {e}\")\n",
    "            raise\n",
    "        \n",
    "        # DataLoader mit optimierten Settings\n",
    "        loader = DataLoader(\n",
    "            dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=True,\n",
    "            num_workers=0,  # Immer 0 für Kompatibilität\n",
    "            pin_memory=(self.device.type == 'cuda'),\n",
    "            drop_last=False\n",
    "        )\n",
    "        \n",
    "        # Training loop mit Fehlerbehandlung\n",
    "        try:\n",
    "            for epoch in range(epochs):\n",
    "                total_loss = 0.0\n",
    "                batch_count = 0\n",
    "                \n",
    "                for xb, yb in loader:\n",
    "                    xb, yb = xb.to(self.device), yb.to(self.device)\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = self(xb)\n",
    "                    loss = loss_fn(outputs, yb)\n",
    "                    \n",
    "                    # Gradient clipping zur Stabilität\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=1.0)\n",
    "                    \n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    total_loss += loss.item()\n",
    "                    batch_count += 1\n",
    "                \n",
    "                scheduler.step()\n",
    "                \n",
    "                if verbose and (epoch + 1) % 2 == 0:\n",
    "                    avg_loss = total_loss / max(batch_count, 1)\n",
    "                    logger.info(f\"    Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Fehler während des Trainings: {e}\")\n",
    "            raise\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X_np, batch_size=1024):\n",
    "        \"\"\"\n",
    "        Gibt Wahrscheinlichkeiten für große Datenmengen zurück.\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        probs = []\n",
    "        \n",
    "        # Anpassung der Batch-Größe bei wenig Speicher\n",
    "        if self.device.type == 'cuda':\n",
    "            try:\n",
    "                free_memory = torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated()\n",
    "                if free_memory < 1024**3:  # Weniger als 1GB frei\n",
    "                    batch_size = 256\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                for i in range(0, len(X_np), batch_size):\n",
    "                    batch = torch.from_numpy(X_np[i:i+batch_size]).float().to(self.device)\n",
    "                    logits = self(batch)\n",
    "                    batch_probs = F.softmax(logits, dim=1)\n",
    "                    probs.append(batch_probs.cpu().numpy())\n",
    "                    \n",
    "                    # Speicher freigeben\n",
    "                    del batch, logits, batch_probs\n",
    "                    if self.device.type == 'cuda':\n",
    "                        torch.cuda.empty_cache()\n",
    "                        \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Fehler bei predict_proba: {e}\")\n",
    "                raise\n",
    "        \n",
    "        return np.vstack(probs) if probs else np.array([])\n",
    "\n",
    "    def predict(self, X_np, batch_size=1024):\n",
    "        \"\"\"\n",
    "        Gibt Vorhersagen zurück.\n",
    "        \"\"\"\n",
    "        probs = self.predict_proba(X_np, batch_size)\n",
    "        return np.argmax(probs, axis=1) if len(probs) > 0 else np.array([])\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# 3) Wrapper für sklearn-Klassifikatoren\n",
    "# -------------------------------------------------------------------------------\n",
    "class SklearnWrapper:\n",
    "    \"\"\"\n",
    "    Wrapper-Klasse für sklearn-Klassifikatoren mit NN-ähnlicher API.\n",
    "    \"\"\"\n",
    "    def __init__(self, classifier, scaler=None):\n",
    "        self.classifier = classifier\n",
    "        self.scaler = scaler\n",
    "        self.is_fitted = False\n",
    "        \n",
    "    def fit(self, X_np, y_np, **kwargs):\n",
    "        \"\"\"Trainiert den Klassifikator mit Fehlerbehandlung.\"\"\"\n",
    "        try:\n",
    "            if self.scaler is not None:\n",
    "                X_scaled = self.scaler.fit_transform(X_np)\n",
    "            else:\n",
    "                X_scaled = X_np\n",
    "            \n",
    "            self.classifier.fit(X_scaled, y_np)\n",
    "            self.is_fitted = True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Fehler beim Training des sklearn-Modells: {e}\")\n",
    "            raise\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X_np):\n",
    "        \"\"\"Gibt Wahrscheinlichkeiten zurueck mit Fehlerbehandlung.\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise RuntimeError(\"Modell wurde noch nicht trainiert!\")\n",
    "        \n",
    "        try:\n",
    "            if self.scaler is not None:\n",
    "                X_scaled = self.scaler.transform(X_np)\n",
    "            else:\n",
    "                X_scaled = X_np\n",
    "                \n",
    "            if hasattr(self.classifier, 'predict_proba'):\n",
    "                return self.classifier.predict_proba(X_scaled)\n",
    "            else:\n",
    "                # Für SVM mit probability=False oder andere Klassifikatoren\n",
    "                if hasattr(self.classifier, 'decision_function'):\n",
    "                    decision = self.classifier.decision_function(X_scaled)\n",
    "                    \n",
    "                    # Multi-class Fall\n",
    "                    if len(decision.shape) == 2:\n",
    "                        # Softmax auf decision values\n",
    "                        exp_decision = np.exp(decision - np.max(decision, axis=1, keepdims=True))\n",
    "                        probs = exp_decision / np.sum(exp_decision, axis=1, keepdims=True)\n",
    "                    else:\n",
    "                        # Binary Fall - konvertiere zu 2-Klassen-Wahrscheinlichkeiten\n",
    "                        probs = np.zeros((len(decision), 2))\n",
    "                        probs[:, 1] = 1 / (1 + np.exp(-decision))\n",
    "                        probs[:, 0] = 1 - probs[:, 1]\n",
    "                    return probs\n",
    "                else:\n",
    "                    # Fallback: One-hot encoding der Vorhersagen\n",
    "                    predictions = self.classifier.predict(X_scaled)\n",
    "                    n_classes = len(np.unique(predictions))\n",
    "                    probs = np.zeros((len(predictions), n_classes))\n",
    "                    for i, pred in enumerate(predictions):\n",
    "                        probs[i, int(pred)] = 1.0\n",
    "                    return probs\n",
    "                    \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Fehler bei predict_proba: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def predict(self, X_np):\n",
    "        \"\"\"Gibt Vorhersagen zurück mit Fehlerbehandlung.\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise RuntimeError(\"Modell wurde noch nicht trainiert!\")\n",
    "            \n",
    "        try:\n",
    "            if self.scaler is not None:\n",
    "                X_scaled = self.scaler.transform(X_np)\n",
    "            else:\n",
    "                X_scaled = X_np\n",
    "            return self.classifier.predict(X_scaled)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Fehler bei predict: {e}\")\n",
    "            raise\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# 4) Klassifikator-Factory\n",
    "# -------------------------------------------------------------------------------\n",
    "def create_classifier(classifier_name, input_dim=None, n_classes=None):\n",
    "    \"\"\"\n",
    "    Erstellt einen Klassifikator basierend auf dem Namen.\n",
    "    \n",
    "    Args:\n",
    "        classifier_name: Name des Klassifikators\n",
    "        input_dim: Input-Dimension für Neural Network\n",
    "        n_classes: Anzahl der Klassen\n",
    "    \n",
    "    Returns:\n",
    "        Klassifikator-Objekt mit einheitlicher API\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if classifier_name == 'Neural Network':\n",
    "            if input_dim is None or n_classes is None:\n",
    "                raise ValueError(\"input_dim und n_classes müssen für Neural Network angegeben werden\")\n",
    "            return OptimizedTabularNN(input_dim=input_dim, num_classes=n_classes)\n",
    "        \n",
    "        elif classifier_name == 'Naive Bayes':\n",
    "            return SklearnWrapper(GaussianNB())\n",
    "        \n",
    "        elif classifier_name == 'Random Forest':\n",
    "            # Angepasste Parameter für bessere Performance\n",
    "            n_jobs = min(os.cpu_count() - 1, -1) if os.cpu_count() else -1\n",
    "            return SklearnWrapper(\n",
    "                RandomForestClassifier(\n",
    "                    n_estimators=100,\n",
    "                    max_depth=None,\n",
    "                    min_samples_split=2,\n",
    "                    min_samples_leaf=1,\n",
    "                    n_jobs=n_jobs,\n",
    "                    random_state=SEED,\n",
    "                    verbose=0\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        elif classifier_name == 'Logistic Regression':\n",
    "            return SklearnWrapper(\n",
    "                LogisticRegression(\n",
    "                    max_iter=1000,\n",
    "                    solver='saga',\n",
    "                    multi_class='multinomial',\n",
    "                    n_jobs=-1,\n",
    "                    random_state=SEED,\n",
    "                    verbose=0\n",
    "                ),\n",
    "                scaler=StandardScaler()\n",
    "            )\n",
    "        \n",
    "        elif classifier_name == 'SVM':\n",
    "            return SklearnWrapper(\n",
    "                SVC(\n",
    "                    kernel='rbf',\n",
    "                    gamma='scale',\n",
    "                    decision_function_shape='ovr',\n",
    "                    probability=True,\n",
    "                    cache_size=500,  # Mehr Cache für bessere Performance\n",
    "                    random_state=SEED,\n",
    "                    verbose=False\n",
    "                ),\n",
    "                scaler=StandardScaler()\n",
    "            )\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unbekannter Klassifikator: {classifier_name}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Fehler beim Erstellen des Klassifikators {classifier_name}: {e}\")\n",
    "        raise\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# 5) Query-Strategien mit Fehlerbehandlung\n",
    "# -------------------------------------------------------------------------------\n",
    "def entropy_sampling(model, X_pool, n_instances=1):\n",
    "    \"\"\"\n",
    "    Waehlt Samples mit hoechster Entropie aus.\n",
    "    H(x) = -Σ p(y|x) * log(p(y|x))\n",
    "    \"\"\"\n",
    "    try:\n",
    "        probs = model.predict_proba(X_pool)\n",
    "        \n",
    "        # Kleine Konstante hinzufügen um log(0) zu vermeiden\n",
    "        epsilon = 1e-10\n",
    "        probs = np.clip(probs, epsilon, 1.0 - epsilon)\n",
    "        \n",
    "        # Entropie berechnen\n",
    "        entropies = -np.sum(probs * np.log(probs), axis=1)\n",
    "        \n",
    "        # Sicherstellen, dass wir nicht mehr Samples anfordern als verfügbar\n",
    "        n_instances = min(n_instances, len(X_pool))\n",
    "        \n",
    "        # Indizes mit höchster Entropie\n",
    "        return np.argsort(entropies)[-n_instances:]\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Fehler bei Entropy Sampling: {e}\")\n",
    "        # Fallback zu Random Sampling\n",
    "        return random_sampling(model, X_pool, n_instances)\n",
    "\n",
    "def margin_sampling(model, X_pool, n_instances=1):\n",
    "    \"\"\"\n",
    "    Wählt Samples mit kleinstem Margin zwischen Top-2 Klassen.\n",
    "    margin = P(y1|x) - P(y2|x)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        probs = model.predict_proba(X_pool)\n",
    "        \n",
    "        # Sortiere Wahrscheinlichkeiten\n",
    "        sorted_probs = np.sort(probs, axis=1)\n",
    "        \n",
    "        # Berechne Margin\n",
    "        if sorted_probs.shape[1] >= 2:\n",
    "            margins = sorted_probs[:, -1] - sorted_probs[:, -2]\n",
    "        else:\n",
    "            # Falls nur eine Klasse, verwende 1 - max_prob als Margin\n",
    "            margins = 1.0 - sorted_probs[:, -1]\n",
    "        \n",
    "        # Sicherstellen, dass wir nicht mehr Samples anfordern als verfügbar\n",
    "        n_instances = min(n_instances, len(X_pool))\n",
    "        \n",
    "        # Indizes mit kleinstem Margin\n",
    "        return np.argsort(margins)[:n_instances]\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Fehler bei Margin Sampling: {e}\")\n",
    "        # Fallback zu Random Sampling\n",
    "        return random_sampling(model, X_pool, n_instances)\n",
    "\n",
    "def least_confidence_sampling(model, X_pool, n_instances=1):\n",
    "    \"\"\"\n",
    "    Wählt Samples mit geringster Konfidenz.\n",
    "    confidence = max P(y|x)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        probs = model.predict_proba(X_pool)\n",
    "        \n",
    "        # Maximum-Wahrscheinlichkeit als Konfidenz\n",
    "        confidences = np.max(probs, axis=1)\n",
    "        \n",
    "        # Sicherstellen, dass wir nicht mehr Samples anfordern als verfügbar\n",
    "        n_instances = min(n_instances, len(X_pool))\n",
    "        \n",
    "        # Indizes mit geringster Konfidenz\n",
    "        return np.argsort(confidences)[:n_instances]\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Fehler bei Least Confidence Sampling: {e}\")\n",
    "        # Fallback zu Random Sampling\n",
    "        return random_sampling(model, X_pool, n_instances)\n",
    "\n",
    "def random_sampling(model, X_pool, n_instances=1):\n",
    "    \"\"\"\n",
    "    Zufällige Auswahl (Baseline).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Sicherstellen, dass wir nicht mehr Samples anfordern als verfügbar\n",
    "        n_instances = min(n_instances, len(X_pool))\n",
    "        \n",
    "        if n_instances <= 0:\n",
    "            return np.array([], dtype=int)\n",
    "            \n",
    "        return np.random.choice(len(X_pool), size=n_instances, replace=False)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Fehler bei Random Sampling: {e}\")\n",
    "        # Notfall-Fallback\n",
    "        return np.arange(min(n_instances, len(X_pool)))\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# 6) Statistische Analyse Funktionen mit Fehlerbehandlung\n",
    "# -------------------------------------------------------------------------------\n",
    "def cliffs_delta(x, y):\n",
    "    \"\"\"\n",
    "    Berechnet Cliff's Delta als nicht-parametrisches Effektstaerkemaß.\n",
    "    \n",
    "    Interpretation:\n",
    "    |d| < 0.147 \"negligible\"\n",
    "    |d| < 0.33  \"small\" \n",
    "    |d| < 0.474 \"medium\"\n",
    "    |d| >= 0.474 \"large\"\n",
    "    \"\"\"\n",
    "    try:\n",
    "        nx = len(x)\n",
    "        ny = len(y)\n",
    "        \n",
    "        if nx == 0 or ny == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        # Konvertiere zu numpy arrays falls nötig\n",
    "        x = np.asarray(x)\n",
    "        y = np.asarray(y)\n",
    "        \n",
    "        # Berechne die Anzahl der Paare, wo x[i] > y[j]\n",
    "        greater = 0\n",
    "        less = 0\n",
    "        \n",
    "        # Vektorisierte Berechnung für bessere Performance\n",
    "        for xi in x:\n",
    "            greater += np.sum(xi > y)\n",
    "            less += np.sum(xi < y)\n",
    "        \n",
    "        # Cliff's Delta\n",
    "        d = (greater - less) / (nx * ny)\n",
    "        \n",
    "        # Sicherstellen, dass d im Bereich [-1, 1] liegt\n",
    "        d = np.clip(d, -1.0, 1.0)\n",
    "        \n",
    "        return d\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Fehler bei Cliff's Delta Berechnung: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "def interpret_cliffs_delta(d):\n",
    "    \"\"\"\n",
    "    Interpretiert die Effektstärke nach Cliff's Delta.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        abs_d = abs(float(d))\n",
    "        if abs_d < 0.147:\n",
    "            return \"negligible\"\n",
    "        elif abs_d < 0.33:\n",
    "            return \"small\"\n",
    "        elif abs_d < 0.474:\n",
    "            return \"medium\"\n",
    "        else:\n",
    "            return \"large\"\n",
    "    except:\n",
    "        return \"unknown\"\n",
    "\n",
    "def perform_statistical_analysis(results_df, metric='accuracy'):\n",
    "    \"\"\"\n",
    "    Fuehrt statistische Analyse durch: Wilcoxon Signed-Rank Test mit Bonferroni-Korrektur.\n",
    "    \"\"\"\n",
    "    statistical_results = []\n",
    "    \n",
    "    try:\n",
    "        classifiers = results_df['classifier'].unique()\n",
    "        strategies = results_df['strategy'].unique()\n",
    "        budget_levels = results_df['budget_pct'].unique()\n",
    "        \n",
    "        for classifier in classifiers:\n",
    "            for budget_pct in budget_levels:\n",
    "                # Hole Random Sampling als Baseline\n",
    "                baseline_data = results_df[\n",
    "                    (results_df['classifier'] == classifier) & \n",
    "                    (results_df['strategy'] == 'Random Sampling') & \n",
    "                    (results_df['budget_pct'] == budget_pct)\n",
    "                ][metric].values\n",
    "                \n",
    "                # Vergleiche mit anderen Strategien\n",
    "                for strategy in strategies:\n",
    "                    if strategy == 'Random Sampling':\n",
    "                        continue\n",
    "                        \n",
    "                    strategy_data = results_df[\n",
    "                        (results_df['classifier'] == classifier) & \n",
    "                        (results_df['strategy'] == strategy) & \n",
    "                        (results_df['budget_pct'] == budget_pct)\n",
    "                    ][metric].values\n",
    "                    \n",
    "                    # Mindestens N_RUNS Datenpunkte für statistische Tests\n",
    "                    if len(baseline_data) >= N_RUNS and len(strategy_data) >= N_RUNS:\n",
    "                        # Wilcoxon Signed-Rank Test\n",
    "                        try:\n",
    "                            # Prüfe ob alle Werte gleich sind\n",
    "                            if np.allclose(strategy_data, baseline_data):\n",
    "                                statistic, p_value = 0.0, 1.0\n",
    "                            else:\n",
    "                                statistic, p_value = wilcoxon(\n",
    "                                    strategy_data, baseline_data, \n",
    "                                    alternative='greater',\n",
    "                                    zero_method='zsplit'\n",
    "                                )\n",
    "                        except Exception as e:\n",
    "                            logger.warning(f\"Wilcoxon Test fehlgeschlagen für {classifier}-{strategy}: {e}\")\n",
    "                            statistic, p_value = 0.0, 1.0\n",
    "                        \n",
    "                        # Effektstärke\n",
    "                        effect_size = cliffs_delta(strategy_data, baseline_data)\n",
    "                        effect_interpretation = interpret_cliffs_delta(effect_size)\n",
    "                        \n",
    "                        # Mittelwerte und Standardabweichungen\n",
    "                        baseline_mean = np.mean(baseline_data) if len(baseline_data) > 0 else 0\n",
    "                        baseline_std = np.std(baseline_data) if len(baseline_data) > 0 else 0\n",
    "                        strategy_mean = np.mean(strategy_data) if len(strategy_data) > 0 else 0\n",
    "                        strategy_std = np.std(strategy_data) if len(strategy_data) > 0 else 0\n",
    "                        \n",
    "                        # Verbesserung berechnen mit Division-by-Zero-Schutz\n",
    "                        improvement = strategy_mean - baseline_mean\n",
    "                        improvement_pct = ((improvement / baseline_mean) * 100) if baseline_mean > 0 else 0\n",
    "                        \n",
    "                        statistical_results.append({\n",
    "                            'classifier': classifier,\n",
    "                            'budget_pct': budget_pct,\n",
    "                            'strategy': strategy,\n",
    "                            'baseline_mean': baseline_mean,\n",
    "                            'baseline_std': baseline_std,\n",
    "                            'strategy_mean': strategy_mean,\n",
    "                            'strategy_std': strategy_std,\n",
    "                            'improvement': improvement,\n",
    "                            'improvement_pct': improvement_pct,\n",
    "                            'wilcoxon_statistic': float(statistic),\n",
    "                            'p_value': float(p_value),\n",
    "                            'cliffs_delta': float(effect_size),\n",
    "                            'effect_size': effect_interpretation,\n",
    "                            'n_samples': len(strategy_data)\n",
    "                        })\n",
    "        \n",
    "        # Konvertiere zu DataFrame\n",
    "        stat_df = pd.DataFrame(statistical_results)\n",
    "        \n",
    "        if len(stat_df) > 0:\n",
    "            # Bonferroni-Korrektur für multiple Vergleiche\n",
    "            n_comparisons = len(stat_df)\n",
    "            stat_df['p_value_corrected'] = np.minimum(stat_df['p_value'] * n_comparisons, 1.0)\n",
    "            stat_df['significant'] = stat_df['p_value_corrected'] < SIGNIFICANCE_LEVEL\n",
    "        else:\n",
    "            logger.warning(\"Keine statistischen Ergebnisse generiert!\")\n",
    "            \n",
    "        return stat_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Fehler bei der statistischen Analyse: {e}\")\n",
    "        return pd.DataFrame()  # Leerer DataFrame als Fallback\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# 7) Active Learning Hauptfunktion mit Fehlerbehandlung\n",
    "# -------------------------------------------------------------------------------\n",
    "def run_active_learning_experiment(X_train, y_train, X_test, y_test,\n",
    "                                 classifier_name, strategy_name, strategy_func,\n",
    "                                 budget_percentages, batch_size=500,\n",
    "                                 input_dim=None, n_classes=None):\n",
    "    \"\"\"\n",
    "    Führt ein Active Learning Experiment durch mit umfassender Fehlerbehandlung.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    n_total = len(y_train)\n",
    "    \n",
    "    # Input-Dimension und Anzahl Klassen ermitteln\n",
    "    if input_dim is None:\n",
    "        input_dim = X_train.shape[1]\n",
    "    if n_classes is None:\n",
    "        n_classes = len(np.unique(y_train))\n",
    "    \n",
    "    for budget_pct in budget_percentages:\n",
    "        n_budget = int(budget_pct * n_total)\n",
    "        \n",
    "        logger.info(f\"\\n{classifier_name} + {strategy_name} - Budget: {budget_pct:.0%} ({n_budget:,} Samples)\")\n",
    "        \n",
    "        for run in range(N_RUNS):\n",
    "            logger.info(f\"  Run {run+1}/{N_RUNS}\")\n",
    "            \n",
    "            try:\n",
    "                # Set seed for reproducibility\n",
    "                np.random.seed(SEED + run)\n",
    "                torch.manual_seed(SEED + run)\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.manual_seed(SEED + run)\n",
    "                \n",
    "                # Initialisierung\n",
    "                pool_indices = np.arange(n_total)\n",
    "                labeled_indices = []\n",
    "                \n",
    "                # Initiale zufällige Auswahl\n",
    "                n_initial = max(100, int(INITIAL_PERCENTAGE * n_total))\n",
    "                n_initial = min(n_initial, len(pool_indices))  # Sicherstellen dass genug Samples da sind\n",
    "                \n",
    "                initial_indices = np.random.choice(pool_indices, size=n_initial, replace=False)\n",
    "                labeled_indices = list(initial_indices)\n",
    "                pool_indices = np.setdiff1d(pool_indices, labeled_indices)\n",
    "                \n",
    "                # Tracking\n",
    "                accuracies = []\n",
    "                n_labeled_list = []\n",
    "                query_times = []\n",
    "                train_times = []\n",
    "                \n",
    "                while len(labeled_indices) < n_budget and len(pool_indices) > 0:\n",
    "                    start_time = time.time()\n",
    "                    \n",
    "                    # Modell erstellen und trainieren\n",
    "                    model = create_classifier(classifier_name, input_dim=input_dim, n_classes=n_classes)\n",
    "                    \n",
    "                    train_start = time.time()\n",
    "                    \n",
    "                    # Training mit Fehlerbehandlung\n",
    "                    try:\n",
    "                        model.fit(X_train[labeled_indices], y_train[labeled_indices])\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Fehler beim Training in Run {run+1}: {e}\")\n",
    "                        # Skip diesen Durchlauf\n",
    "                        break\n",
    "                    \n",
    "                    train_time = time.time() - train_start\n",
    "                    train_times.append(train_time)\n",
    "                    \n",
    "                    # Evaluation\n",
    "                    try:\n",
    "                        y_pred = model.predict(X_test)\n",
    "                        acc = accuracy_score(y_test, y_pred)\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Fehler bei der Evaluation in Run {run+1}: {e}\")\n",
    "                        acc = 0.0\n",
    "                    \n",
    "                    accuracies.append(acc)\n",
    "                    n_labeled_list.append(len(labeled_indices))\n",
    "                    \n",
    "                    # Nächste Batch auswählen\n",
    "                    n_query = min(batch_size, n_budget - len(labeled_indices), len(pool_indices))\n",
    "                    if n_query <= 0:\n",
    "                        break\n",
    "                    \n",
    "                    # Query mit Zeitmessung und Fehlerbehandlung\n",
    "                    query_start = time.time()\n",
    "                    try:\n",
    "                        query_indices = strategy_func(model, X_train[pool_indices], n_query)\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Fehler bei Query-Strategie {strategy_name}: {e}\")\n",
    "                        # Fallback zu Random Sampling\n",
    "                        query_indices = random_sampling(model, X_train[pool_indices], n_query)\n",
    "                    \n",
    "                    query_time = time.time() - query_start\n",
    "                    query_times.append(query_time)\n",
    "                    \n",
    "                    # Validierung der Query-Indizes\n",
    "                    query_indices = np.asarray(query_indices)\n",
    "                    query_indices = query_indices[query_indices < len(pool_indices)]  # Entferne ungültige Indizes\n",
    "                    \n",
    "                    if len(query_indices) == 0:\n",
    "                        logger.warning(f\"Keine gueltigen Query-Indizes in Run {run+1}\")\n",
    "                        break\n",
    "                    \n",
    "                    selected_indices = pool_indices[query_indices]\n",
    "                    \n",
    "                    # Update\n",
    "                    labeled_indices.extend(selected_indices)\n",
    "                    pool_indices = np.setdiff1d(pool_indices, selected_indices)\n",
    "                    \n",
    "                    # Progress logging - Unicode-Fix: Verwende -> statt →\n",
    "                    if len(labeled_indices) % 2000 == 0 or len(labeled_indices) == n_budget:\n",
    "                        logger.info(f\"    {len(labeled_indices):,} labeled -> Accuracy: {acc:.4f} \"\n",
    "                                  f\"(Train: {train_time:.1f}s, Query: {query_time:.2f}s)\")\n",
    "                    \n",
    "                    # Speicher freigeben bei NN\n",
    "                    if classifier_name == 'Neural Network' and hasattr(model, 'device') and model.device.type == 'cuda':\n",
    "                        torch.cuda.empty_cache()\n",
    "                \n",
    "                # Finale Evaluation mit mehr Training\n",
    "                if len(labeled_indices) > 0:\n",
    "                    try:\n",
    "                        model = create_classifier(classifier_name, input_dim=input_dim, n_classes=n_classes)\n",
    "                        \n",
    "                        # Mehr Epochs für finale Evaluation\n",
    "                        if classifier_name == 'Neural Network':\n",
    "                            model.fit(X_train[labeled_indices], y_train[labeled_indices], \n",
    "                                     epochs=20, verbose=False)\n",
    "                        else:\n",
    "                            model.fit(X_train[labeled_indices], y_train[labeled_indices])\n",
    "                        \n",
    "                        y_pred = model.predict(X_test)\n",
    "                        final_acc = accuracy_score(y_test, y_pred)\n",
    "                        final_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Fehler bei finaler Evaluation in Run {run+1}: {e}\")\n",
    "                        final_acc = acc if 'acc' in locals() else 0.0\n",
    "                        final_f1 = 0.0\n",
    "                    \n",
    "                    results.append({\n",
    "                        'classifier': classifier_name,\n",
    "                        'strategy': strategy_name,\n",
    "                        'budget_pct': budget_pct,\n",
    "                        'run': run,\n",
    "                        'n_labeled': len(labeled_indices),\n",
    "                        'accuracy': final_acc,\n",
    "                        'f1_score': final_f1,\n",
    "                        'accuracies': accuracies,\n",
    "                        'n_labeled_list': n_labeled_list,\n",
    "                        'avg_query_time': np.mean(query_times) if query_times else 0,\n",
    "                        'avg_train_time': np.mean(train_times) if train_times else 0\n",
    "                    })\n",
    "                    \n",
    "                    # Unicode-Fix: Verwende -> statt →\n",
    "                    logger.info(f\"    Final: {len(labeled_indices):,} labeled -> \"\n",
    "                              f\"Accuracy: {final_acc:.4f}, F1: {final_f1:.4f}\")\n",
    "                \n",
    "                # Speicher freigeben\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Unerwarteter Fehler in Run {run+1}: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "    \n",
    "    return results\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# 8) Visualisierung: Pro Klassifikator mit Signifikanzmarkierungen\n",
    "# -------------------------------------------------------------------------------\n",
    "def plot_per_classifier_with_significance(all_results, stat_results):\n",
    "    \"\"\"\n",
    "    Erstellt eine Visualisierung pro Klassifikator mit Signifikanzmarkierungen.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Style setzen mit Fallback\n",
    "        try:\n",
    "            plt.style.use('seaborn-v0_8-whitegrid')\n",
    "        except:\n",
    "            try:\n",
    "                plt.style.use('seaborn-whitegrid')\n",
    "            except:\n",
    "                plt.style.use('ggplot')\n",
    "        \n",
    "        # Farben für Strategien\n",
    "        strategy_colors = {\n",
    "            'Random Sampling': '#808080',\n",
    "            'Entropy Sampling': '#1f77b4',\n",
    "            'Margin Sampling': '#ff7f0e',\n",
    "            'Least Confidence': '#2ca02c'\n",
    "        }\n",
    "        \n",
    "        # Klassifikatoren extrahieren\n",
    "        classifiers = sorted(list(set(r['classifier'] for r in all_results)))\n",
    "        \n",
    "        # Eine Figure pro Klassifikator\n",
    "        for classifier in classifiers:\n",
    "            fig, axes = plt.subplots(1, len(BUDGET_PERCENTAGES), figsize=(20, 4))\n",
    "            \n",
    "            # Handle für einzelne Subplot\n",
    "            if len(BUDGET_PERCENTAGES) == 1:\n",
    "                axes = [axes]\n",
    "                \n",
    "            fig.suptitle(f'{classifier} - Active Learning Performance with Statistical Significance', \n",
    "                         fontsize=16, y=1.02)\n",
    "            \n",
    "            # Gesamtanzahl Samples\n",
    "            n_total_samples = max(r['n_labeled'] for r in all_results if r['budget_pct'] == 1.0)\n",
    "            \n",
    "            for budget_idx, budget_pct in enumerate(BUDGET_PERCENTAGES):\n",
    "                ax = axes[budget_idx]\n",
    "                \n",
    "                # Daten für diesen Klassifikator und Budget\n",
    "                for strategy, color in strategy_colors.items():\n",
    "                    strategy_results = [r for r in all_results \n",
    "                                      if r['classifier'] == classifier \n",
    "                                      and r['strategy'] == strategy \n",
    "                                      and r['budget_pct'] == budget_pct]\n",
    "                    \n",
    "                    if strategy_results:\n",
    "                        # Lernkurven aggregieren\n",
    "                        max_samples = int(budget_pct * n_total_samples)\n",
    "                        x_common = np.linspace(100, max_samples, 100)\n",
    "                        y_interpolated = []\n",
    "                        \n",
    "                        for r in strategy_results:\n",
    "                            if len(r['n_labeled_list']) > 1:\n",
    "                                try:\n",
    "                                    y_interp = np.interp(x_common, r['n_labeled_list'], r['accuracies'])\n",
    "                                    y_interpolated.append(y_interp)\n",
    "                                except:\n",
    "                                    logger.warning(f\"Interpolation fehlgeschlagen für {classifier}-{strategy}\")\n",
    "                        \n",
    "                        if y_interpolated:\n",
    "                            y_mean = np.mean(y_interpolated, axis=0)\n",
    "                            y_std = np.std(y_interpolated, axis=0)\n",
    "                            \n",
    "                            # Überprüfe Signifikanz\n",
    "                            is_significant = False\n",
    "                            effect_size = \"\"\n",
    "                            if strategy != 'Random Sampling' and not stat_results.empty:\n",
    "                                sig_data = stat_results[\n",
    "                                    (stat_results['classifier'] == classifier) & \n",
    "                                    (stat_results['strategy'] == strategy) & \n",
    "                                    (stat_results['budget_pct'] == budget_pct)\n",
    "                                ]\n",
    "                                if not sig_data.empty:\n",
    "                                    is_significant = sig_data.iloc[0]['significant']\n",
    "                                    effect_size = sig_data.iloc[0]['effect_size']\n",
    "                            \n",
    "                            # Label mit Signifikanz\n",
    "                            label = strategy\n",
    "                            if is_significant:\n",
    "                                label += f\" *({effect_size})\"\n",
    "                            \n",
    "                            # Plot mit Konfidenzintervall\n",
    "                            ax.plot(x_common, y_mean, \n",
    "                                   label=label, \n",
    "                                   color=color, \n",
    "                                   linewidth=2.5,\n",
    "                                   linestyle='-' if not is_significant or strategy == 'Random Sampling' else '--',\n",
    "                                   marker='o' if strategy == 'Random Sampling' else None,\n",
    "                                   markevery=10 if strategy == 'Random Sampling' else None,\n",
    "                                   markersize=4)\n",
    "                            \n",
    "                            ax.fill_between(x_common, \n",
    "                                          y_mean - y_std, \n",
    "                                          y_mean + y_std, \n",
    "                                          color=color, \n",
    "                                          alpha=0.2)\n",
    "                \n",
    "                # Achsenbeschriftung\n",
    "                ax.set_xlabel('Number of Labeled Samples', fontsize=11)\n",
    "                ax.set_ylabel('Test Accuracy', fontsize=11)\n",
    "                ax.set_title(f'Budget: {int(budget_pct*100)}%', fontsize=12)\n",
    "                \n",
    "                # Grid und Limits\n",
    "                ax.grid(True, alpha=0.3)\n",
    "                ax.set_ylim([0.0, 1.0])\n",
    "                \n",
    "                # X-Achse formatieren\n",
    "                ax.ticklabel_format(style='plain', axis='x')\n",
    "                ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{int(x/1000)}k' if x >= 1000 else str(int(x))))\n",
    "                \n",
    "                # Legende nur beim ersten Plot\n",
    "                if budget_idx == 0:\n",
    "                    ax.legend(loc='lower right', fontsize=9, framealpha=0.9)\n",
    "            \n",
    "            # Signifikanz-Erklärung\n",
    "            fig.text(0.5, -0.05, \n",
    "                    '* = statistically significant (p < 0.05 with Bonferroni correction); ' +\n",
    "                    'Effect size in parentheses (negligible/small/medium/large)',\n",
    "                    ha='center', fontsize=10, style='italic')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Speichern mit Fehlerbehandlung\n",
    "            filename = f'plots/dachmaterial_{classifier.lower().replace(\" \", \"_\")}_with_significance.png'\n",
    "            try:\n",
    "                plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "                logger.info(f\"[ok] Visualisierung mit Signifikanz für {classifier} erstellt: {filename}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Fehler beim Speichern der Visualisierung für {classifier}: {e}\")\n",
    "                \n",
    "            plt.close()\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Fehler bei plot_per_classifier_with_significance: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# 9) Visualisierung: Statistische Zusammenfassung\n",
    "# -------------------------------------------------------------------------------\n",
    "def plot_statistical_summary(stat_results):\n",
    "    \"\"\"\n",
    "    Erstellt eine Visualisierung der statistischen Ergebnisse.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Style setzen\n",
    "        try:\n",
    "            plt.style.use('seaborn-v0_8-whitegrid')\n",
    "        except:\n",
    "            try:\n",
    "                plt.style.use('seaborn-whitegrid')\n",
    "            except:\n",
    "                plt.style.use('ggplot')\n",
    "        \n",
    "        if stat_results.empty:\n",
    "            logger.warning(\"Keine statistischen Ergebnisse zum Visualisieren!\")\n",
    "            return\n",
    "            \n",
    "        # Filtere nur signifikante Ergebnisse\n",
    "        sig_results = stat_results[stat_results['significant']].copy() if 'significant' in stat_results.columns else pd.DataFrame()\n",
    "        \n",
    "        if sig_results.empty:\n",
    "            logger.warning(\"Keine signifikanten Ergebnisse gefunden!\")\n",
    "            # Erstelle trotzdem eine Visualisierung mit allen Ergebnissen\n",
    "            sig_results = stat_results.copy()\n",
    "        \n",
    "        # Figure mit Subplots\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        fig.suptitle('Statistical Analysis Summary - Dachmaterial Dataset', fontsize=16)\n",
    "        \n",
    "        # 1. Heatmap der p-Werte\n",
    "        try:\n",
    "            ax1 = axes[0, 0]\n",
    "            pivot_p = stat_results.pivot_table(\n",
    "                values='p_value_corrected',\n",
    "                index=['classifier', 'strategy'],\n",
    "                columns='budget_pct',\n",
    "                fill_value=1.0\n",
    "            )\n",
    "            \n",
    "            if sns is not None and not pivot_p.empty:\n",
    "                sns.heatmap(pivot_p, \n",
    "                            annot=True, \n",
    "                            fmt='.3f', \n",
    "                            cmap='RdYlGn_r',\n",
    "                            vmin=0, \n",
    "                            vmax=0.1,\n",
    "                            cbar_kws={'label': 'Corrected p-value'},\n",
    "                            ax=ax1)\n",
    "            else:\n",
    "                ax1.text(0.5, 0.5, 'Heatmap nicht verfügbar', ha='center', va='center')\n",
    "                \n",
    "            ax1.set_title('Corrected p-values (Wilcoxon Signed-Rank Test)')\n",
    "            ax1.set_xlabel('Budget (%)')\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Fehler bei p-Wert Heatmap: {e}\")\n",
    "            axes[0, 0].text(0.5, 0.5, 'Fehler bei der Erstellung', ha='center', va='center')\n",
    "        \n",
    "        # 2. Heatmap der Effektstärken\n",
    "        try:\n",
    "            ax2 = axes[0, 1]\n",
    "            pivot_effect = stat_results.pivot_table(\n",
    "                values='cliffs_delta',\n",
    "                index=['classifier', 'strategy'],\n",
    "                columns='budget_pct',\n",
    "                fill_value=0.0\n",
    "            )\n",
    "            \n",
    "            if sns is not None and not pivot_effect.empty:\n",
    "                sns.heatmap(pivot_effect, \n",
    "                            annot=True, \n",
    "                            fmt='.3f', \n",
    "                            cmap='coolwarm',\n",
    "                            center=0,\n",
    "                            vmin=-1, \n",
    "                            vmax=1,\n",
    "                            cbar_kws={'label': \"Cliff's Delta\"},\n",
    "                            ax=ax2)\n",
    "            else:\n",
    "                ax2.text(0.5, 0.5, 'Heatmap nicht verfügbar', ha='center', va='center')\n",
    "                \n",
    "            ax2.set_title(\"Effect Size (Cliff's Delta)\")\n",
    "            ax2.set_xlabel('Budget (%)')\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Fehler bei Effektstärken Heatmap: {e}\")\n",
    "            axes[0, 1].text(0.5, 0.5, 'Fehler bei der Erstellung', ha='center', va='center')\n",
    "        \n",
    "        # 3. Anzahl signifikanter Verbesserungen pro Strategie\n",
    "        try:\n",
    "            ax3 = axes[1, 0]\n",
    "            if not sig_results.empty and 'strategy' in sig_results.columns:\n",
    "                sig_counts = sig_results.groupby('strategy').size().sort_values(ascending=False)\n",
    "                sig_counts.plot(kind='bar', ax=ax3, color='steelblue')\n",
    "                ax3.set_title('Number of Significant Improvements per Strategy')\n",
    "                ax3.set_xlabel('Strategy')\n",
    "                ax3.set_ylabel('Count')\n",
    "                ax3.grid(axis='y', alpha=0.3)\n",
    "            else:\n",
    "                ax3.text(0.5, 0.5, 'Keine signifikanten Ergebnisse', ha='center', va='center')\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Fehler bei Signifikanz-Barplot: {e}\")\n",
    "            axes[1, 0].text(0.5, 0.5, 'Fehler bei der Erstellung', ha='center', va='center')\n",
    "        \n",
    "        # 4. Durchschnittliche Effektstärke pro Klassifikator\n",
    "        try:\n",
    "            ax4 = axes[1, 1]\n",
    "            if 'classifier' in stat_results.columns and 'cliffs_delta' in stat_results.columns:\n",
    "                avg_effect = stat_results.groupby('classifier')['cliffs_delta'].agg(['mean', 'std'])\n",
    "                avg_effect['mean'].plot(kind='bar', ax=ax4, yerr=avg_effect['std'], \n",
    "                                       capsize=5, color='darkorange')\n",
    "                ax4.set_title(\"Average Effect Size per Classifier\")\n",
    "                ax4.set_xlabel('Classifier')\n",
    "                ax4.set_ylabel(\"Mean Cliff's Delta\")\n",
    "                ax4.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "                ax4.grid(axis='y', alpha=0.3)\n",
    "            else:\n",
    "                ax4.text(0.5, 0.5, 'Daten nicht verfügbar', ha='center', va='center')\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Fehler bei Effektstärken-Barplot: {e}\")\n",
    "            axes[1, 1].text(0.5, 0.5, 'Fehler bei der Erstellung', ha='center', va='center')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Speichern\n",
    "        filename = 'plots/dachmaterial_statistical_summary.png'\n",
    "        try:\n",
    "            plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "            logger.info(f\"[ok] Statistische Zusammenfassung erstellt: {filename}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Fehler beim Speichern der statistischen Zusammenfassung: {e}\")\n",
    "            \n",
    "        plt.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Fehler bei plot_statistical_summary: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# 10) Detaillierte statistische Tabelle\n",
    "# -------------------------------------------------------------------------------\n",
    "def create_statistical_report(stat_results):\n",
    "    \"\"\"\n",
    "    Erstellt einen detaillierten statistischen Bericht.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Sortiere nach Effektstärke\n",
    "        if not stat_results.empty and 'cliffs_delta' in stat_results.columns:\n",
    "            stat_results_sorted = stat_results.sort_values('cliffs_delta', ascending=False)\n",
    "        else:\n",
    "            stat_results_sorted = stat_results\n",
    "        \n",
    "        # Erstelle formatierten Bericht\n",
    "        report = []\n",
    "        report.append(\"\\n\" + \"=\"*100)\n",
    "        report.append(\"DETAILLIERTER STATISTISCHER BERICHT - DACHMATERIAL\")\n",
    "        report.append(\"=\"*100)\n",
    "        report.append(f\"Signifikanzniveau: {SIGNIFICANCE_LEVEL} (mit Bonferroni-Korrektur)\")\n",
    "        report.append(f\"Anzahl Runs pro Experiment: {N_RUNS}\")\n",
    "        report.append(f\"Statistischer Test: Wilcoxon Signed-Rank Test\")\n",
    "        report.append(f\"Effektstärkemaß: Cliff's Delta\")\n",
    "        report.append(\"\\n\")\n",
    "        \n",
    "        # Signifikante Ergebnisse\n",
    "        if 'significant' in stat_results_sorted.columns:\n",
    "            sig_results = stat_results_sorted[stat_results_sorted['significant']]\n",
    "        else:\n",
    "            sig_results = pd.DataFrame()\n",
    "        \n",
    "        if not sig_results.empty:\n",
    "            report.append(\"SIGNIFIKANTE VERBESSERUNGEN GEGENÜBER RANDOM SAMPLING:\")\n",
    "            report.append(\"-\"*100)\n",
    "            report.append(f\"{'Klassifikator':<20} {'Strategie':<20} {'Budget':<10} {'Verbesserung':<15} \"\n",
    "                         f\"{'p-Wert':<12} {'Effekt':<15} {'Interpretation':<15}\")\n",
    "            report.append(\"-\"*100)\n",
    "            \n",
    "            for _, row in sig_results.iterrows():\n",
    "                report.append(f\"{row['classifier']:<20} {row['strategy']:<20} \"\n",
    "                             f\"{int(row['budget_pct']*100):>8}% \"\n",
    "                             f\"{row['improvement_pct']:>13.2f}% \"\n",
    "                             f\"{row['p_value_corrected']:>11.4f} \"\n",
    "                             f\"{row['cliffs_delta']:>14.3f} \"\n",
    "                             f\"{row['effect_size']:<15}\")\n",
    "        else:\n",
    "            report.append(\"Keine signifikanten Verbesserungen gefunden!\")\n",
    "        \n",
    "        # Zusammenfassung nach Strategie\n",
    "        report.append(\"\\n\\nZUSAMMENFASSUNG NACH STRATEGIE:\")\n",
    "        report.append(\"-\"*100)\n",
    "        \n",
    "        for strategy in ['Entropy Sampling', 'Margin Sampling', 'Least Confidence']:\n",
    "            if 'strategy' in stat_results.columns:\n",
    "                strategy_data = stat_results[stat_results['strategy'] == strategy]\n",
    "                if not strategy_data.empty:\n",
    "                    sig_count = strategy_data['significant'].sum() if 'significant' in strategy_data.columns else 0\n",
    "                    avg_improvement = strategy_data['improvement_pct'].mean() if 'improvement_pct' in strategy_data.columns else 0\n",
    "                    avg_effect = strategy_data['cliffs_delta'].mean() if 'cliffs_delta' in strategy_data.columns else 0\n",
    "                    \n",
    "                    report.append(f\"\\n{strategy}:\")\n",
    "                    report.append(f\"  - Signifikante Verbesserungen: {sig_count}/{len(strategy_data)} \"\n",
    "                                 f\"({sig_count/len(strategy_data)*100:.1f}%)\")\n",
    "                    report.append(f\"  - Durchschnittliche Verbesserung: {avg_improvement:.2f}%\")\n",
    "                    report.append(f\"  - Durchschnittliche Effektstärke: {avg_effect:.3f}\")\n",
    "        \n",
    "        # Zusammenfassung nach Klassifikator\n",
    "        report.append(\"\\n\\nZUSAMMENFASSUNG NACH KLASSIFIKATOR:\")\n",
    "        report.append(\"-\"*100)\n",
    "        \n",
    "        if 'classifier' in stat_results.columns:\n",
    "            for classifier in stat_results['classifier'].unique():\n",
    "                classifier_data = stat_results[stat_results['classifier'] == classifier]\n",
    "                sig_count = classifier_data['significant'].sum() if 'significant' in classifier_data.columns else 0\n",
    "                \n",
    "                report.append(f\"\\n{classifier}:\")\n",
    "                report.append(f\"  - Signifikante Verbesserungen: {sig_count}/{len(classifier_data)} \"\n",
    "                             f\"({sig_count/len(classifier_data)*100:.1f}%)\")\n",
    "                \n",
    "                if sig_count > 0 and 'cliffs_delta' in classifier_data.columns:\n",
    "                    best_strategy = classifier_data.loc[classifier_data['cliffs_delta'].idxmax()]\n",
    "                    report.append(f\"  - Beste Strategie: {best_strategy['strategy']} \"\n",
    "                                 f\"bei {int(best_strategy['budget_pct']*100)}% Budget \"\n",
    "                                 f\"(Effekt: {best_strategy['cliffs_delta']:.3f})\")\n",
    "        \n",
    "        report.append(\"\\n\" + \"=\"*100)\n",
    "        \n",
    "        # Ausgabe\n",
    "        report_text = \"\\n\".join(report)\n",
    "        print(report_text)\n",
    "        \n",
    "        # Speichern\n",
    "        report_filename = 'reports/dachmaterial_statistical_report.txt'\n",
    "        try:\n",
    "            with open(report_filename, 'w', encoding='utf-8') as f:\n",
    "                f.write(report_text)\n",
    "            logger.info(f\"[ok] Statistischer Bericht gespeichert: {report_filename}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Fehler beim Speichern des Berichts: {e}\")\n",
    "        \n",
    "        return report_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Fehler bei create_statistical_report: {e}\")\n",
    "        return \"Fehler bei der Berichterstellung\"\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# 11) Visualisierung: Finale Vergleichsmatrix\n",
    "# -------------------------------------------------------------------------------\n",
    "def plot_final_comparison(all_results):\n",
    "    \"\"\"\n",
    "    Erstellt eine Visualisierung, die zeigt, welche Kombination aus Klassifikator\n",
    "    und Query-Strategie bei 100% Budget am besten abgeschnitten hat.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Style setzen\n",
    "        try:\n",
    "            plt.style.use('seaborn-v0_8-whitegrid')\n",
    "        except:\n",
    "            try:\n",
    "                plt.style.use('seaborn-whitegrid')\n",
    "            except:\n",
    "                plt.style.use('ggplot')\n",
    "        \n",
    "        # Nur Ergebnisse mit 100% Budget\n",
    "        final_results = [r for r in all_results if r['budget_pct'] == 1.0]\n",
    "        \n",
    "        if not final_results:\n",
    "            logger.warning(\"Keine Ergebnisse mit 100% Budget gefunden!\")\n",
    "            return\n",
    "        \n",
    "        # Aggregiere Ergebnisse\n",
    "        summary_data = []\n",
    "        classifiers = sorted(list(set(r['classifier'] for r in final_results)))\n",
    "        strategies = ['Random Sampling', 'Entropy Sampling', 'Margin Sampling', 'Least Confidence']\n",
    "        \n",
    "        for classifier in classifiers:\n",
    "            for strategy in strategies:\n",
    "                results = [r for r in final_results \n",
    "                          if r['classifier'] == classifier and r['strategy'] == strategy]\n",
    "                \n",
    "                if results:\n",
    "                    mean_acc = np.mean([r['accuracy'] for r in results])\n",
    "                    std_acc = np.std([r['accuracy'] for r in results])\n",
    "                    mean_f1 = np.mean([r['f1_score'] for r in results])\n",
    "                    \n",
    "                    summary_data.append({\n",
    "                        'Classifier': classifier,\n",
    "                        'Strategy': strategy,\n",
    "                        'Accuracy': mean_acc,\n",
    "                        'Accuracy_std': std_acc,\n",
    "                        'F1-Score': mean_f1\n",
    "                    })\n",
    "        \n",
    "        if not summary_data:\n",
    "            logger.warning(\"Keine Daten für finale Vergleichsmatrix!\")\n",
    "            return\n",
    "            \n",
    "        # DataFrame erstellen\n",
    "        df = pd.DataFrame(summary_data)\n",
    "        \n",
    "        # Pivot für Heatmap\n",
    "        pivot_acc = df.pivot(index='Classifier', columns='Strategy', values='Accuracy')\n",
    "        pivot_f1 = df.pivot(index='Classifier', columns='Strategy', values='F1-Score')\n",
    "        \n",
    "        # Figure mit zwei Subplots\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        \n",
    "        # Heatmap für Accuracy\n",
    "        if sns is not None and not pivot_acc.empty:\n",
    "            sns.heatmap(pivot_acc, \n",
    "                        annot=True, \n",
    "                        fmt='.4f', \n",
    "                        cmap='RdYlGn', \n",
    "                        vmin=0.0, \n",
    "                        vmax=1.0,\n",
    "                        cbar_kws={'label': 'Test Accuracy'},\n",
    "                        ax=ax1)\n",
    "        else:\n",
    "            ax1.text(0.5, 0.5, 'Heatmap nicht verfügbar', ha='center', va='center')\n",
    "            \n",
    "        ax1.set_title('Test Accuracy at 100% Budget', fontsize=14)\n",
    "        ax1.set_xlabel('Query Strategy', fontsize=12)\n",
    "        ax1.set_ylabel('Classifier', fontsize=12)\n",
    "        \n",
    "        # Heatmap für F1-Score\n",
    "        if sns is not None and not pivot_f1.empty:\n",
    "            sns.heatmap(pivot_f1, \n",
    "                        annot=True, \n",
    "                        fmt='.4f', \n",
    "                        cmap='RdYlGn', \n",
    "                        vmin=0.0, \n",
    "                        vmax=1.0,\n",
    "                        cbar_kws={'label': 'F1-Score (Macro)'},\n",
    "                        ax=ax2)\n",
    "        else:\n",
    "            ax2.text(0.5, 0.5, 'Heatmap nicht verfügbar', ha='center', va='center')\n",
    "            \n",
    "        ax2.set_title('F1-Score at 100% Budget', fontsize=14)\n",
    "        ax2.set_xlabel('Query Strategy', fontsize=12)\n",
    "        ax2.set_ylabel('Classifier', fontsize=12)\n",
    "        \n",
    "        plt.suptitle('Final Performance Comparison - Full Dachmaterial Dataset (100% Budget)', fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Speichern\n",
    "        filename = 'plots/dachmaterial_final_comparison.png'\n",
    "        try:\n",
    "            plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "            logger.info(f\"[ok] Finale Vergleichsvisualisierung erstellt: {filename}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Fehler beim Speichern der finalen Vergleichsmatrix: {e}\")\n",
    "            \n",
    "        plt.close()\n",
    "        \n",
    "        # Beste Kombination finden\n",
    "        if not df.empty:\n",
    "            best_idx = df['Accuracy'].idxmax()\n",
    "            best_result = df.iloc[best_idx]\n",
    "            \n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"BESTE KOMBINATION BEI 100% BUDGET:\")\n",
    "            print(\"=\"*60)\n",
    "            print(f\"Klassifikator: {best_result['Classifier']}\")\n",
    "            print(f\"Query-Strategie: {best_result['Strategy']}\")\n",
    "            print(f\"Test Accuracy: {best_result['Accuracy']:.4f} (±{best_result['Accuracy_std']:.4f})\")\n",
    "            print(f\"F1-Score: {best_result['F1-Score']:.4f}\")\n",
    "            print(\"=\"*60)\n",
    "            \n",
    "            # Top 5 Kombinationen\n",
    "            print(\"\\nTOP 5 KOMBINATIONEN:\")\n",
    "            print(\"-\"*60)\n",
    "            top5 = df.nlargest(5, 'Accuracy')[['Classifier', 'Strategy', 'Accuracy', 'F1-Score']]\n",
    "            for idx, (_, row) in enumerate(top5.iterrows()):\n",
    "                print(f\"{idx+1}. {row['Classifier']} + {row['Strategy']}: \"\n",
    "                      f\"Acc={row['Accuracy']:.4f}, F1={row['F1-Score']:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Fehler bei plot_final_comparison: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# 12) Zusätzliche Analyse: Improvement over Random\n",
    "# -------------------------------------------------------------------------------\n",
    "def plot_improvement_analysis(all_results):\n",
    "    \"\"\"\n",
    "    Zeigt die Verbesserung der Active Learning Strategien gegenüber Random Sampling.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Style setzen\n",
    "        try:\n",
    "            plt.style.use('seaborn-v0_8-whitegrid')\n",
    "        except:\n",
    "            try:\n",
    "                plt.style.use('seaborn-whitegrid')\n",
    "            except:\n",
    "                plt.style.use('ggplot')\n",
    "        \n",
    "        # Berechne Verbesserungen\n",
    "        improvements = []\n",
    "        classifiers = sorted(list(set(r['classifier'] for r in all_results)))\n",
    "        strategies = ['Entropy Sampling', 'Margin Sampling', 'Least Confidence']\n",
    "        \n",
    "        for classifier in classifiers:\n",
    "            for strategy in strategies:\n",
    "                for budget_pct in BUDGET_PERCENTAGES:\n",
    "                    # Random Sampling Baseline\n",
    "                    random_results = [r for r in all_results \n",
    "                                    if r['classifier'] == classifier \n",
    "                                    and r['strategy'] == 'Random Sampling' \n",
    "                                    and r['budget_pct'] == budget_pct]\n",
    "                    \n",
    "                    # Active Learning Strategy\n",
    "                    strategy_results = [r for r in all_results \n",
    "                                      if r['classifier'] == classifier \n",
    "                                      and r['strategy'] == strategy \n",
    "                                      and r['budget_pct'] == budget_pct]\n",
    "                    \n",
    "                    if random_results and strategy_results:\n",
    "                        random_acc = np.mean([r['accuracy'] for r in random_results])\n",
    "                        strategy_acc = np.mean([r['accuracy'] for r in strategy_results])\n",
    "                        \n",
    "                        # Prozentuale Verbesserung\n",
    "                        improvement = ((strategy_acc - random_acc) / random_acc) * 100 if random_acc > 0 else 0\n",
    "                        \n",
    "                        improvements.append({\n",
    "                            'Classifier': classifier,\n",
    "                            'Strategy': strategy,\n",
    "                            'Budget': int(budget_pct * 100),\n",
    "                            'Improvement (%)': improvement\n",
    "                        })\n",
    "        \n",
    "        if not improvements:\n",
    "            logger.warning(\"Keine Verbesserungsdaten gefunden!\")\n",
    "            return\n",
    "            \n",
    "        # DataFrame erstellen\n",
    "        imp_df = pd.DataFrame(improvements)\n",
    "        \n",
    "        # Figure\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "        \n",
    "        # Gruppierter Barplot\n",
    "        x = np.arange(len(BUDGET_PERCENTAGES))\n",
    "        width = 0.15\n",
    "        \n",
    "        # Erstelle eine eindeutige Farbe für jede Klassifikator-Strategie-Kombination\n",
    "        colors = plt.cm.tab20(np.linspace(0, 1, len(classifiers) * len(strategies)))\n",
    "        color_idx = 0\n",
    "        \n",
    "        for i, classifier in enumerate(classifiers):\n",
    "            for j, strategy in enumerate(strategies):\n",
    "                data = imp_df[(imp_df['Classifier'] == classifier) & \n",
    "                             (imp_df['Strategy'] == strategy)]\n",
    "                \n",
    "                if not data.empty:\n",
    "                    values = []\n",
    "                    for b in BUDGET_PERCENTAGES:\n",
    "                        budget_data = data[data['Budget'] == int(b*100)]\n",
    "                        if not budget_data.empty:\n",
    "                            values.append(budget_data['Improvement (%)'].values[0])\n",
    "                        else:\n",
    "                            values.append(0)\n",
    "                    \n",
    "                    offset = (i * len(strategies) + j - len(classifiers) * len(strategies) / 2) * width\n",
    "                    bars = ax.bar(x + offset, values, width, \n",
    "                                 label=f'{classifier} - {strategy}',\n",
    "                                 color=colors[color_idx])\n",
    "                    color_idx += 1\n",
    "        \n",
    "        # Achsenbeschriftung\n",
    "        ax.set_xlabel('Budget (%)', fontsize=12)\n",
    "        ax.set_ylabel('Improvement over Random Sampling (%)', fontsize=12)\n",
    "        ax.set_title('Active Learning Improvement Analysis - Dachmaterial Dataset', fontsize=14)\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels([f'{int(b*100)}%' for b in BUDGET_PERCENTAGES])\n",
    "        \n",
    "        # Nulllinie\n",
    "        ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "        \n",
    "        # Legende\n",
    "        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)\n",
    "        \n",
    "        # Grid\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Speichern\n",
    "        filename = 'plots/dachmaterial_improvement_analysis.png'\n",
    "        try:\n",
    "            plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "            logger.info(f\"[ok] Improvement-Analyse erstellt: {filename}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Fehler beim Speichern der Improvement-Analyse: {e}\")\n",
    "            \n",
    "        plt.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Fehler bei plot_improvement_analysis: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# 13) Label-Einsparungs-Analyse Funktionen\n",
    "# -------------------------------------------------------------------------------\n",
    "def calculate_label_savings(all_results, target_performance_percentages=[0.90, 0.95, 0.98]):\n",
    "    \"\"\"\n",
    "    Berechnet die Label-Einsparung für Active Learning Strategien.\n",
    "    \n",
    "    Args:\n",
    "        all_results: Liste aller Experiment-Ergebnisse\n",
    "        target_performance_percentages: Prozentsätze der Random Sampling 100% Performance\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame mit Label-Einsparungen\n",
    "    \"\"\"\n",
    "    savings_results = []\n",
    "    \n",
    "    # Gruppiere nach Klassifikator\n",
    "    classifiers = sorted(list(set(r['classifier'] for r in all_results)))\n",
    "    \n",
    "    # Gesamtanzahl Samples\n",
    "    n_total_samples = max(r['n_labeled'] for r in all_results if r['budget_pct'] == 1.0)\n",
    "    \n",
    "    for classifier in classifiers:\n",
    "        # Hole Random Sampling Performance bei 100% Budget als Referenz\n",
    "        random_100_results = [r for r in all_results \n",
    "                            if r['classifier'] == classifier \n",
    "                            and r['strategy'] == 'Random Sampling' \n",
    "                            and r['budget_pct'] == 1.0]\n",
    "        \n",
    "        if not random_100_results:\n",
    "            continue\n",
    "            \n",
    "        # Durchschnittliche Performance bei 100% Budget\n",
    "        random_100_acc = np.mean([r['accuracy'] for r in random_100_results])\n",
    "        \n",
    "        # Für verschiedene Ziel-Performance-Level\n",
    "        for target_pct in target_performance_percentages:\n",
    "            target_accuracy = random_100_acc * target_pct\n",
    "            \n",
    "            # Für jede Strategie\n",
    "            for strategy in ['Random Sampling', 'Entropy Sampling', 'Margin Sampling', 'Least Confidence']:\n",
    "                strategy_results = [r for r in all_results \n",
    "                                  if r['classifier'] == classifier \n",
    "                                  and r['strategy'] == strategy]\n",
    "                \n",
    "                if not strategy_results:\n",
    "                    continue\n",
    "                \n",
    "                # Aggregiere Lernkurven über alle Runs\n",
    "                all_curves = []\n",
    "                for r in strategy_results:\n",
    "                    if 'n_labeled_list' in r and 'accuracies' in r:\n",
    "                        all_curves.append((r['n_labeled_list'], r['accuracies']))\n",
    "                \n",
    "                if not all_curves:\n",
    "                    continue\n",
    "                \n",
    "                # Finde minimale Label-Anzahl um Ziel-Accuracy zu erreichen\n",
    "                labels_needed = []\n",
    "                \n",
    "                for n_labeled_list, accuracies in all_curves:\n",
    "                    # Interpoliere um den Punkt zu finden, wo target_accuracy erreicht wird\n",
    "                    if len(accuracies) > 0 and max(accuracies) >= target_accuracy:\n",
    "                        # Finde ersten Punkt, der target_accuracy überschreitet\n",
    "                        for i, acc in enumerate(accuracies):\n",
    "                            if acc >= target_accuracy:\n",
    "                                labels_needed.append(n_labeled_list[i])\n",
    "                                break\n",
    "                    else:\n",
    "                        # Ziel nicht erreicht - verwende Maximum\n",
    "                        labels_needed.append(n_total_samples)\n",
    "                \n",
    "                if labels_needed:\n",
    "                    avg_labels_needed = np.mean(labels_needed)\n",
    "                    std_labels_needed = np.std(labels_needed)\n",
    "                    \n",
    "                    # Berechne Einsparung gegenüber 100%\n",
    "                    savings_pct = ((n_total_samples - avg_labels_needed) / n_total_samples) * 100\n",
    "                    \n",
    "                    # Berechne Einsparung gegenüber Random Sampling\n",
    "                    if strategy != 'Random Sampling':\n",
    "                        random_labels = next((s['avg_labels_needed'] for s in savings_results \n",
    "                                            if s['classifier'] == classifier \n",
    "                                            and s['strategy'] == 'Random Sampling' \n",
    "                                            and s['target_performance'] == int(target_pct*100)), n_total_samples)\n",
    "                        relative_savings_pct = ((random_labels - avg_labels_needed) / random_labels) * 100 if random_labels > 0 else 0\n",
    "                    else:\n",
    "                        relative_savings_pct = 0\n",
    "                    \n",
    "                    savings_results.append({\n",
    "                        'classifier': classifier,\n",
    "                        'strategy': strategy,\n",
    "                        'target_performance': int(target_pct * 100),\n",
    "                        'target_accuracy': target_accuracy,\n",
    "                        'avg_labels_needed': avg_labels_needed,\n",
    "                        'std_labels_needed': std_labels_needed,\n",
    "                        'savings_pct': savings_pct,\n",
    "                        'relative_savings_pct': relative_savings_pct,\n",
    "                        'random_100_acc': random_100_acc\n",
    "                    })\n",
    "    \n",
    "    return pd.DataFrame(savings_results)\n",
    "\n",
    "def plot_label_savings(savings_df, dataset_name=\"Dachmaterial\"):\n",
    "    \"\"\"\n",
    "    Visualisiert die Label-Einsparungen.\n",
    "    \"\"\"\n",
    "    # Style setzen\n",
    "    try:\n",
    "        plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    except:\n",
    "        plt.style.use('ggplot')\n",
    "    \n",
    "    # Figure mit Subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle(f'{dataset_name} - Label Savings Analysis', fontsize=16)\n",
    "    \n",
    "    # 1. Labels benötigt für verschiedene Performance-Level\n",
    "    ax1 = axes[0, 0]\n",
    "    \n",
    "    # Gruppiere nach Klassifikator und Target Performance\n",
    "    for classifier in savings_df['classifier'].unique():\n",
    "        for target in savings_df['target_performance'].unique():\n",
    "            data = savings_df[(savings_df['classifier'] == classifier) & \n",
    "                            (savings_df['target_performance'] == target)]\n",
    "            \n",
    "            if not data.empty:\n",
    "                strategies = data['strategy'].values\n",
    "                labels_needed = data['avg_labels_needed'].values\n",
    "                errors = data['std_labels_needed'].values\n",
    "                \n",
    "                x = np.arange(len(strategies))\n",
    "                width = 0.2\n",
    "                offset = (target - 95) * width / 5  # Offset basierend auf target\n",
    "                \n",
    "                ax1.bar(x + offset, labels_needed, width, \n",
    "                       yerr=errors, capsize=5,\n",
    "                       label=f'{classifier} - {target}% of baseline',\n",
    "                       alpha=0.7)\n",
    "    \n",
    "    ax1.set_xlabel('Strategy')\n",
    "    ax1.set_ylabel('Labels Needed')\n",
    "    ax1.set_title('Labels Required to Reach Target Performance')\n",
    "    ax1.set_xticks(np.arange(len(strategies)))\n",
    "    ax1.set_xticklabels(strategies, rotation=45, ha='right')\n",
    "    ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 2. Relative Einsparung gegenüber Random Sampling\n",
    "    ax2 = axes[0, 1]\n",
    "    \n",
    "    # Pivot für Heatmap\n",
    "    pivot_savings = savings_df[savings_df['strategy'] != 'Random Sampling'].pivot_table(\n",
    "        values='relative_savings_pct',\n",
    "        index=['classifier', 'strategy'],\n",
    "        columns='target_performance',\n",
    "        fill_value=0\n",
    "    )\n",
    "    \n",
    "    if not pivot_savings.empty and sns is not None:\n",
    "        sns.heatmap(pivot_savings, \n",
    "                    annot=True, \n",
    "                    fmt='.1f', \n",
    "                    cmap='RdYlGn',\n",
    "                    center=0,\n",
    "                    cbar_kws={'label': 'Label Savings (%)'},\n",
    "                    ax=ax2)\n",
    "        ax2.set_title('Label Savings vs Random Sampling (%)')\n",
    "        ax2.set_xlabel('Target Performance (% of baseline)')\n",
    "    \n",
    "    # 3. Absolute Label-Anzahl pro Klassifikator bei 95% Performance\n",
    "    ax3 = axes[1, 0]\n",
    "    \n",
    "    data_95 = savings_df[savings_df['target_performance'] == 95]\n",
    "    \n",
    "    classifiers = data_95['classifier'].unique()\n",
    "    strategies = ['Random Sampling', 'Entropy Sampling', 'Margin Sampling', 'Least Confidence']\n",
    "    \n",
    "    x = np.arange(len(classifiers))\n",
    "    width = 0.2\n",
    "    \n",
    "    for i, strategy in enumerate(strategies):\n",
    "        values = []\n",
    "        errors = []\n",
    "        for classifier in classifiers:\n",
    "            row = data_95[(data_95['classifier'] == classifier) & \n",
    "                         (data_95['strategy'] == strategy)]\n",
    "            if not row.empty:\n",
    "                values.append(row['avg_labels_needed'].values[0])\n",
    "                errors.append(row['std_labels_needed'].values[0])\n",
    "            else:\n",
    "                values.append(0)\n",
    "                errors.append(0)\n",
    "        \n",
    "        ax3.bar(x + i*width - 1.5*width, values, width, \n",
    "               yerr=errors, capsize=5,\n",
    "               label=strategy, alpha=0.8)\n",
    "    \n",
    "    ax3.set_xlabel('Classifier')\n",
    "    ax3.set_ylabel('Labels Needed')\n",
    "    ax3.set_title('Labels Needed to Reach 95% of Baseline Performance')\n",
    "    ax3.set_xticks(x)\n",
    "    ax3.set_xticklabels(classifiers, rotation=45, ha='right')\n",
    "    ax3.legend()\n",
    "    ax3.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Referenzlinie bei 100% der Daten\n",
    "    n_total_samples = savings_df['avg_labels_needed'].max() * 1.1  # Schätzung\n",
    "    ax3.axhline(y=n_total_samples, color='red', linestyle='--', alpha=0.5, label='Full Dataset')\n",
    "    \n",
    "    # 4. Zusammenfassungstabelle\n",
    "    ax4 = axes[1, 1]\n",
    "    ax4.axis('tight')\n",
    "    ax4.axis('off')\n",
    "    \n",
    "    # Erstelle Zusammenfassungstabelle für 95% Performance\n",
    "    summary_data = []\n",
    "    for classifier in classifiers:\n",
    "        row_data = [classifier]\n",
    "        for strategy in strategies:\n",
    "            data = data_95[(data_95['classifier'] == classifier) & \n",
    "                          (data_95['strategy'] == strategy)]\n",
    "            if not data.empty:\n",
    "                labels = data['avg_labels_needed'].values[0]\n",
    "                savings = data['savings_pct'].values[0]\n",
    "                row_data.append(f'{int(labels):,}\\n({savings:.1f}% saved)')\n",
    "            else:\n",
    "                row_data.append('N/A')\n",
    "        summary_data.append(row_data)\n",
    "    \n",
    "    table = ax4.table(cellText=summary_data,\n",
    "                     colLabels=['Classifier'] + strategies,\n",
    "                     cellLoc='center',\n",
    "                     loc='center')\n",
    "    \n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(9)\n",
    "    table.scale(1.2, 2)\n",
    "    \n",
    "    # Style the header row\n",
    "    for i in range(len(strategies) + 1):\n",
    "        table[(0, i)].set_facecolor('#40466e')\n",
    "        table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "    \n",
    "    ax4.set_title('Summary: Labels Needed for 95% Performance', pad=20)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Speichern\n",
    "    filename = f'plots/{dataset_name.lower()}_label_savings_analysis.png'\n",
    "    plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "    logger.info(f\"[ok] Label-Einsparungs-Analyse erstellt: {filename}\")\n",
    "    \n",
    "    plt.close()\n",
    "    \n",
    "    return savings_df\n",
    "\n",
    "def create_label_savings_report(savings_df, dataset_name=\"Dachmaterial\"):\n",
    "    \"\"\"\n",
    "    Erstellt einen detaillierten Bericht über Label-Einsparungen.\n",
    "    \"\"\"\n",
    "    report = []\n",
    "    report.append(\"\\n\" + \"=\"*80)\n",
    "    report.append(f\"LABEL-EINSPARUNGS-BERICHT - {dataset_name}\")\n",
    "    report.append(\"=\"*80)\n",
    "    \n",
    "    # Für jedes Performance-Level\n",
    "    for target_perf in sorted(savings_df['target_performance'].unique()):\n",
    "        report.append(f\"\\nZIEL: {target_perf}% der Baseline-Performance\")\n",
    "        report.append(\"-\"*60)\n",
    "        \n",
    "        target_data = savings_df[savings_df['target_performance'] == target_perf]\n",
    "        \n",
    "        # Nach Klassifikator gruppieren\n",
    "        for classifier in sorted(target_data['classifier'].unique()):\n",
    "            classifier_data = target_data[target_data['classifier'] == classifier]\n",
    "            baseline_acc = classifier_data['random_100_acc'].iloc[0]\n",
    "            target_acc = classifier_data['target_accuracy'].iloc[0]\n",
    "            \n",
    "            report.append(f\"\\n{classifier}:\")\n",
    "            report.append(f\"  Baseline (Random 100%): {baseline_acc:.4f}\")\n",
    "            report.append(f\"  Ziel-Accuracy: {target_acc:.4f}\")\n",
    "            report.append(f\"  Labels benötigt:\")\n",
    "            \n",
    "            # Sortiere nach Labels benötigt\n",
    "            sorted_data = classifier_data.sort_values('avg_labels_needed')\n",
    "            \n",
    "            for _, row in sorted_data.iterrows():\n",
    "                strategy = row['strategy']\n",
    "                labels = row['avg_labels_needed']\n",
    "                std = row['std_labels_needed']\n",
    "                savings = row['savings_pct']\n",
    "                rel_savings = row['relative_savings_pct']\n",
    "                \n",
    "                report.append(f\"    - {strategy:<20}: {int(labels):>6,} ± {int(std):>4} \"\n",
    "                            f\"({savings:>5.1f}% gespart)\")\n",
    "                \n",
    "                if strategy != 'Random Sampling' and rel_savings > 0:\n",
    "                    report.append(f\"      -> {rel_savings:.1f}% weniger Labels als Random Sampling\")\n",
    "    \n",
    "    # Beste Strategien\n",
    "    report.append(\"\\n\\nBESTE STRATEGIEN (bei 95% Performance):\")\n",
    "    report.append(\"-\"*60)\n",
    "    \n",
    "    data_95 = savings_df[savings_df['target_performance'] == 95]\n",
    "    \n",
    "    for classifier in sorted(data_95['classifier'].unique()):\n",
    "        classifier_data = data_95[data_95['classifier'] == classifier]\n",
    "        best_row = classifier_data.loc[classifier_data['avg_labels_needed'].idxmin()]\n",
    "        \n",
    "        report.append(f\"{classifier}: {best_row['strategy']} \"\n",
    "                     f\"(nur {int(best_row['avg_labels_needed']):,} Labels = \"\n",
    "                     f\"{best_row['savings_pct']:.1f}% Einsparung)\")\n",
    "    \n",
    "    # Durchschnittliche Einsparungen\n",
    "    report.append(\"\\n\\nDURCHSCHNITTLICHE EINSPARUNGEN ÜBER ALLE KLASSIFIKATOREN:\")\n",
    "    report.append(\"-\"*60)\n",
    "    \n",
    "    avg_savings = data_95.groupby('strategy')['relative_savings_pct'].mean()\n",
    "    for strategy, savings in avg_savings.items():\n",
    "        if strategy != 'Random Sampling':\n",
    "            report.append(f\"{strategy}: {savings:.1f}% weniger Labels als Random Sampling\")\n",
    "    \n",
    "    report.append(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    # Ausgabe und Speichern\n",
    "    report_text = \"\\n\".join(report)\n",
    "    print(report_text)\n",
    "    \n",
    "    filename = f'reports/{dataset_name.lower()}_label_savings_report.txt'\n",
    "    \n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        f.write(report_text)\n",
    "    \n",
    "    logger.info(f\"[ok] Label-Einsparungs-Bericht gespeichert: {filename}\")\n",
    "    \n",
    "    return report_text\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# Hauptprogramm\n",
    "# -------------------------------------------------------------------------------\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Haupteinstiegspunkt für das erweiterte Active Learning Experiment für Dachmaterial.\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"ACTIVE LEARNING AUF DACHMATERIAL - ROBUSTE VERSION MIT STATISTISCHER ANALYSE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # System Info\n",
    "    print(f\"Python Version: {sys.version.split()[0]}\")\n",
    "    print(f\"PyTorch Version: {torch.__version__}\")\n",
    "    print(f\"NumPy Version: {np.__version__}\")\n",
    "    print(f\"Pandas Version: {pd.__version__}\")\n",
    "    print(f\"Scikit-learn Version: {sklearn.__version__}\")\n",
    "    print(f\"SciPy Version: {scipy.__version__}\")\n",
    "    \n",
    "    # Device Info\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    if device.type == 'cuda':\n",
    "        print(f\"Verwende GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    else:\n",
    "        print(\"Verwende CPU (keine GPU gefunden)\")\n",
    "    \n",
    "    print(f\"\\nExperiment-Konfiguration:\")\n",
    "    print(f\"- Anzahl Runs: {N_RUNS}\")\n",
    "    print(f\"- Budget-Stufen: {[f'{int(b*100)}%' for b in BUDGET_PERCENTAGES]}\")\n",
    "    print(f\"- Batch-Größe: {BATCH_SIZE}\")\n",
    "    print(f\"- Signifikanzniveau: {SIGNIFICANCE_LEVEL}\")\n",
    "    print(f\"- Mindest-Samples pro Klasse: {MIN_SAMPLES_PER_CLASS}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Daten laden\n",
    "    try:\n",
    "        X_train, y_train, X_test, y_test, label_encoder, preprocessor = load_dachmaterial_data()\n",
    "        \n",
    "        # Speichere wichtige Variablen für spätere Verwendung\n",
    "        input_dim = X_train.shape[1]\n",
    "        n_classes = len(np.unique(y_train))\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Kritischer Fehler beim Laden der Daten: {e}\")\n",
    "        return 1\n",
    "    \n",
    "    # Klassifikatoren und Strategien definieren\n",
    "    classifiers = ['Neural Network', 'Naive Bayes', 'Random Forest', 'Logistic Regression', 'SVM']\n",
    "    strategies = [\n",
    "        ('Random Sampling', random_sampling),\n",
    "        ('Entropy Sampling', entropy_sampling),\n",
    "        ('Margin Sampling', margin_sampling),\n",
    "        ('Least Confidence', least_confidence_sampling)\n",
    "    ]\n",
    "    \n",
    "    # Experimente durchführen\n",
    "    all_results = []\n",
    "    total_start_time = time.time()\n",
    "    \n",
    "    total_experiments = len(classifiers) * len(strategies)\n",
    "    current_experiment = 0\n",
    "    \n",
    "    for classifier_name in classifiers:\n",
    "        for strategy_name, strategy_func in strategies:\n",
    "            current_experiment += 1\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Experiment {current_experiment}/{total_experiments}: {classifier_name} + {strategy_name}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            experiment_start_time = time.time()\n",
    "            \n",
    "            try:\n",
    "                results = run_active_learning_experiment(\n",
    "                    X_train, y_train, X_test, y_test,\n",
    "                    classifier_name, strategy_name, strategy_func,\n",
    "                    BUDGET_PERCENTAGES, BATCH_SIZE,\n",
    "                    input_dim=input_dim, n_classes=n_classes\n",
    "                )\n",
    "                all_results.extend(results)\n",
    "                \n",
    "                experiment_time = time.time() - experiment_start_time\n",
    "                print(f\"\\n[ok] {classifier_name} + {strategy_name} abgeschlossen in {experiment_time/60:.1f} Minuten\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Kritischer Fehler bei {classifier_name} + {strategy_name}: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "    \n",
    "    # Gesamtzeit\n",
    "    total_time = time.time() - total_start_time\n",
    "    print(f\"\\n[ok] Alle Experimente abgeschlossen in {total_time/60:.1f} Minuten\")\n",
    "    \n",
    "    # Überprüfe ob Ergebnisse vorhanden sind\n",
    "    if not all_results:\n",
    "        logger.error(\"Keine Experimenteergebnisse vorhanden!\")\n",
    "        return 1\n",
    "    \n",
    "    # Ergebnisse in DataFrame konvertieren für statistische Analyse\n",
    "    try:\n",
    "        results_df = pd.DataFrame([{\n",
    "            'classifier': r['classifier'],\n",
    "            'strategy': r['strategy'],\n",
    "            'budget_pct': r['budget_pct'],\n",
    "            'run': r['run'],\n",
    "            'n_labeled': r['n_labeled'],\n",
    "            'accuracy': r['accuracy'],\n",
    "            'f1_score': r['f1_score'],\n",
    "            'avg_query_time': r.get('avg_query_time', 0),\n",
    "            'avg_train_time': r.get('avg_train_time', 0)\n",
    "        } for r in all_results])\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Fehler beim Erstellen des Results DataFrame: {e}\")\n",
    "        return 1\n",
    "    \n",
    "    # Statistische Analyse durchführen\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Führe statistische Analyse durch...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        stat_results = perform_statistical_analysis(results_df)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Fehler bei der statistischen Analyse: {e}\")\n",
    "        stat_results = pd.DataFrame()\n",
    "    \n",
    "    # Statistischen Bericht erstellen\n",
    "    try:\n",
    "        statistical_report = create_statistical_report(stat_results)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Fehler beim Erstellen des statistischen Berichts: {e}\")\n",
    "    \n",
    "    # Label-Einsparungs-Analyse\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Berechne Label-Einsparungen...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        # Berechne Label-Einsparungen für verschiedene Performance-Level\n",
    "        savings_df = calculate_label_savings(all_results, target_performance_percentages=[0.90, 0.95, 0.98])\n",
    "        \n",
    "        # Visualisiere Label-Einsparungen\n",
    "        plot_label_savings(savings_df, dataset_name=\"Dachmaterial\")\n",
    "        \n",
    "        # Erstelle detaillierten Bericht\n",
    "        label_savings_report = create_label_savings_report(savings_df, dataset_name=\"Dachmaterial\")\n",
    "        \n",
    "        # Speichere als CSV\n",
    "        savings_csv = 'results/dachmaterial_label_savings.csv'\n",
    "        savings_df.to_csv(savings_csv, index=False)\n",
    "        print(f\"[ok] Label-Einsparungen gespeichert: {savings_csv}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Fehler bei der Label-Einsparungs-Analyse: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    # Ergebnisse visualisieren\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Erstelle Visualisierungen...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        # Pro Klassifikator mit Signifikanz\n",
    "        plot_per_classifier_with_significance(all_results, stat_results)\n",
    "        \n",
    "        # Statistische Zusammenfassung\n",
    "        plot_statistical_summary(stat_results)\n",
    "        \n",
    "        # Finale Vergleichsmatrix\n",
    "        plot_final_comparison(all_results)\n",
    "        \n",
    "        # Improvement Analyse\n",
    "        plot_improvement_analysis(all_results)\n",
    "        \n",
    "        print(\"[ok] Alle Visualisierungen erstellt\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Fehler bei der Visualisierung: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    # Ergebnisse speichern\n",
    "    try:\n",
    "        # Detaillierte Ergebnisse\n",
    "        csv_filename = 'results/dachmaterial_active_learning_results.csv'\n",
    "        results_df.to_csv(csv_filename, index=False)\n",
    "        print(f\"\\n[ok] Ergebnisse gespeichert in: {csv_filename}\")\n",
    "        \n",
    "        # Statistische Ergebnisse\n",
    "        if not stat_results.empty:\n",
    "            stat_csv_filename = 'results/dachmaterial_statistical_analysis.csv'\n",
    "            stat_results.to_csv(stat_csv_filename, index=False)\n",
    "            print(f\"[ok] Statistische Analyse gespeichert in: {stat_csv_filename}\")\n",
    "        \n",
    "        # Zusammenfassung als Excel (wenn verfügbar)\n",
    "        if EXCEL_AVAILABLE:\n",
    "            excel_filename = 'results/dachmaterial_active_learning_summary.xlsx'\n",
    "            try:\n",
    "                with pd.ExcelWriter(excel_filename, engine='openpyxl') as writer:\n",
    "                    # Raw results\n",
    "                    results_df.to_excel(writer, sheet_name='Raw Results', index=False)\n",
    "                    \n",
    "                    # Statistical analysis\n",
    "                    if not stat_results.empty:\n",
    "                        stat_results.to_excel(writer, sheet_name='Statistical Analysis', index=False)\n",
    "                    \n",
    "                    # Summary by classifier and strategy\n",
    "                    summary = results_df.groupby(['classifier', 'strategy', 'budget_pct'])[['accuracy', 'f1_score']].agg(['mean', 'std'])\n",
    "                    summary.to_excel(writer, sheet_name='Summary Statistics')\n",
    "                    \n",
    "                    # Best combinations at 100% budget\n",
    "                    final_results = results_df[results_df['budget_pct'] == 1.0]\n",
    "                    if not final_results.empty:\n",
    "                        best_combinations = final_results.groupby(['classifier', 'strategy'])[['accuracy', 'f1_score']].mean()\n",
    "                        best_combinations = best_combinations.sort_values('accuracy', ascending=False)\n",
    "                        best_combinations.to_excel(writer, sheet_name='Best Combinations')\n",
    "                    \n",
    "                    # Significant improvements\n",
    "                    if not stat_results.empty and 'significant' in stat_results.columns:\n",
    "                        sig_improvements = stat_results[stat_results['significant']].sort_values('cliffs_delta', ascending=False)\n",
    "                        if not sig_improvements.empty:\n",
    "                            sig_improvements.to_excel(writer, sheet_name='Significant Improvements', index=False)\n",
    "                    \n",
    "                    # Label savings\n",
    "                    if 'savings_df' in locals() and not savings_df.empty:\n",
    "                        savings_df.to_excel(writer, sheet_name='Label Savings', index=False)\n",
    "                \n",
    "                print(f\"[ok] Zusammenfassung gespeichert in: {excel_filename}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Fehler beim Excel-Export: {e}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Fehler beim Speichern der Ergebnisse: {e}\")\n",
    "    \n",
    "    # Abschlusszusammenfassung\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EXPERIMENT ERFOLGREICH ABGESCHLOSSEN\")\n",
    "    print(f\"Gesamtanzahl Experimente: {len(all_results)}\")\n",
    "    print(f\"Datensatzgröße: {len(y_train):,} Trainingssamples\")\n",
    "    print(f\"Klassifikatoren: {len(classifiers)}\")\n",
    "    print(f\"Query-Strategien: {len(strategies)}\")\n",
    "    print(f\"Budget-Stufen: {len(BUDGET_PERCENTAGES)}\")\n",
    "    print(f\"Wiederholungen pro Experiment: {N_RUNS}\")\n",
    "    \n",
    "    # Statistische Zusammenfassung\n",
    "    if not stat_results.empty:\n",
    "        total_comparisons = len(stat_results)\n",
    "        significant_count = stat_results['significant'].sum() if 'significant' in stat_results.columns else 0\n",
    "        print(f\"\\nStatistische Analyse:\")\n",
    "        print(f\"- Anzahl Vergleiche: {total_comparisons}\")\n",
    "        print(f\"- Signifikante Ergebnisse: {significant_count} ({significant_count/total_comparisons*100:.1f}%)\")\n",
    "        print(f\"- Verwendeter Test: Wilcoxon Signed-Rank Test\")\n",
    "        print(f\"- Effektstärkemaß: Cliff's Delta\")\n",
    "        print(f\"- Multiple Vergleiche: Bonferroni-Korrektur\")\n",
    "    \n",
    "    print(\"\\nLabel-Einsparungs-Analyse durchgeführt!\")\n",
    "    print(\"- Visualisierung: plots/dachmaterial_label_savings_analysis.png\")\n",
    "    print(\"- Bericht: reports/dachmaterial_label_savings_report.txt\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return 0\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        exit_code = main()\n",
    "        sys.exit(exit_code)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unerwarteter Fehler im Hauptprogramm: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "015d4e4d-faa8-47be-9c0b-498d35361f2d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mSystemExit\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2343\u001b[39m\n\u001b[32m   2341\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   2342\u001b[39m     exit_code = main()\n\u001b[32m-> \u001b[39m\u001b[32m2343\u001b[39m     \u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexit_code\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2344\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2345\u001b[39m     logger.error(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnerwarteter Fehler im Hauptprogramm: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mSystemExit\u001b[39m: 0"
     ]
    }
   ],
   "source": [
    "%tb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1d26da-8855-40e3-8334-de1e34596965",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
